{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKoTq9xW-PdW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/logo.png\" width=150>\n",
    "\n",
    "When working with sequential data (time-series, sentences, etc.) the order of the inputs is crucial for the task at hand. Recurrent neural networks (RNNs) process sequential data by accounting for the current input and also what has been learned from previous inputs. In this notebook, we'll learn how to create and train RNNs on sequential data.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/rnn.png\" width=550>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Objective:**  Process sequential data by accounting for the currend input and also what has been learned from previous inputs.\n",
    "* **Advantages:** \n",
    "    * Account for order and previous inputs in a meaningful way.\n",
    "    * Conditioned generation for generating sequences.\n",
    "* **Disadvantages:** \n",
    "    * Each time step's prediction depends on the previous prediction so it's difficult to parallelize RNN operations. \n",
    "    * Processing long sequences can yield memory and computation issues.\n",
    "    * Interpretability is difficult but there are few [techniques](https://arxiv.org/abs/1506.02078) that use the activations from RNNs to see what parts of the inputs are processed. \n",
    "* **Miscellaneous:** \n",
    "    * Architectural tweaks to make RNNs faster and interpretable is an ongoing area of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/rnn2.png\" width=650>\n",
    "\n",
    "RNN forward pass for a single time step $X_t$:\n",
    "\n",
    "$h_t = tanh(W_{hh}h_{t-1} + W_{xh}X_t+b_h)$\n",
    "\n",
    "$y_t = W_{hy}h_t + b_y $\n",
    "\n",
    "$ P(y) = softmax(y_t) = \\frac{e^y}{\\sum e^y} $\n",
    "\n",
    "*where*:\n",
    "* $X_t$ = input at time step t | $\\in \\mathbb{R}^{NXE}$ ($N$ is the batch size, $E$ is the embedding dim)\n",
    "* $W_{hh}$ = hidden units weights| $\\in \\mathbb{R}^{HXH}$ ($H$ is the hidden dim)\n",
    "* $h_{t-1}$ = previous timestep's hidden state $\\in \\mathbb{R}^{NXH}$\n",
    "* $W_{xh}$ = input weights| $\\in \\mathbb{R}^{EXH}$\n",
    "* $b_h$ = hidden units bias $\\in \\mathbb{R}^{HX1}$\n",
    "* $W_{hy}$ = output weights| $\\in \\mathbb{R}^{HXC}$ ($C$ is the number of classes)\n",
    "* $b_y$ = output bias $\\in \\mathbb{R}^{CX1}$\n",
    "\n",
    "You repeat this for every time step's input ($X_{t+1}, X_{t+2}, ..., X_{N})$ to the get the predicted outputs at each time step.\n",
    "\n",
    "**Note**: At the first time step, the previous hidden state $h_{t-1}$ can either be a zero vector (unconditioned) or initialize (conditioned). If we are conditioning the RNN, the first hidden state $h_0$ can belong to a specific condition or we can concat the specific condition to the randomly initialized hidden vectors at each time step. More on this in the subsequent notebooks on RNNs.\n",
    "\n",
    "\n",
    "Let's see what the forward pass looks like with an RNN for a synthetic task such as processing reviews (a sequence of words) to predict the sentiment at the end of processing the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_size = 10 # max length per input (masking will be used for sequences that aren't this max length)\n",
    "x_lengths = [8, 5, 4, 10, 5] # lengths of each input sequence\n",
    "embedding_dim = 100\n",
    "rnn_hidden_dim = 256\n",
    "output_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "# Initialize synthetic inputs\n",
    "x_in = torch.randn(batch_size, seq_size, embedding_dim)\n",
    "x_lengths = torch.tensor(x_lengths)\n",
    "print (x_in.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "# Initialize hidden state\n",
    "hidden_t = torch.zeros((batch_size, rnn_hidden_dim))\n",
    "print (hidden_t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNCell(100, 256)\n"
     ]
    }
   ],
   "source": [
    "# Initialize RNN cell\n",
    "rnn_cell = nn.RNNCell(embedding_dim, rnn_hidden_dim)\n",
    "print (rnn_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass through RNN\n",
    "x_in = x_in.permute(1, 0, 2) # RNN needs batch_size to be at dim 1\n",
    "\n",
    "# Loop through the inputs time steps\n",
    "hiddens = []\n",
    "for t in range(seq_size):\n",
    "    hidden_t = rnn_cell(x_in[t], hidden_t)\n",
    "    hiddens.append(hidden_t)\n",
    "hiddens = torch.stack(hiddens)\n",
    "hiddens = hiddens.permute(1, 0, 2) # bring batch_size back to dim 0\n",
    "print (hiddens.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  torch.Size([5, 10, 256])\n",
      "h_n:  torch.Size([1, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "# We also could've used a more abstracted layer\n",
    "x_in = torch.randn(batch_size, seq_size, embedding_dim)\n",
    "rnn = nn.RNN(embedding_dim, rnn_hidden_dim, batch_first=True)\n",
    "out, h_n = rnn(x_in) #h_n is the last hidden state\n",
    "print (\"out: \", out.size())\n",
    "print (\"h_n: \", h_n.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_last_relevant_hidden(hiddens, x_lengths):\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(hiddens[batch_index, column_index])\n",
    "    return torch.stack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "# Gather the last relevant hidden state\n",
    "z = gather_last_relevant_hidden(hiddens, x_lengths)\n",
    "print (z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "tensor([[0.2263, 0.2619, 0.2048, 0.3070],\n",
      "        [0.2010, 0.3693, 0.2140, 0.2156],\n",
      "        [0.2148, 0.3652, 0.2394, 0.1807],\n",
      "        [0.2711, 0.2492, 0.2606, 0.2190],\n",
      "        [0.2911, 0.2015, 0.3370, 0.1704]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass through FC layer\n",
    "fc1 = nn.Linear(rnn_hidden_dim, output_dim)\n",
    "y_pred = fc1(z)\n",
    "y_pred = F.softmax(y_pred, dim=1)\n",
    "print (y_pred.size())\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hb4_VaBIMVFD"
   },
   "source": [
    "### Sequential data\n",
    "\n",
    "There are a variety of different sequential tasks that RNNs can help with.\n",
    "\n",
    "1. **One to one**: there is one input and produces one output. \n",
    "    * Ex. Given a word predict it's class (verb, noun, etc.).\n",
    "2. **One to many**: one input generates many outputs.\n",
    "    * Ex. Given a sentiment (positive, negative, etc.) generate a review.\n",
    "3. **Many to one**: Many inputs are sequentially processed to generate one output.\n",
    "    * Ex. Process the words in a review to predict the sentiment.\n",
    "4. **Many to many**: Many inputs are sequentially processed to generate many outputs.\n",
    "    * Ex. Given a sentence in French, processes the entire sentence and then generate the English translation.\n",
    "    * Ex. Given a sequence of time-series data, predict the probability of an event (risk of disease) at each time step.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/seq2seq.jpeg\" width=700>\n",
    "\n",
    "*From [(The Unreasonable Effectiveness of Recurrent Neural Networks)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "The first example is the usual full mesh network. Each following demonstrates the processing of a certain sequence of arbitrary length (red rectangles) and the generation of the output sequence, also of arbitrary length (blue rectangles).\n",
    "\n",
    "In this case, the green rectangles in each figure are the same weights. So, on the one hand, we are training a very, very deep network (if you look at it upside down), and on the other, a strictly limited number of parameters.\n",
    "\n",
    "---\n",
    "### Write a simple RNN right away!\n",
    "\n",
    "Let me remind you, she does something like this:\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" width=\"70%\">\n",
    "\n",
    "\n",
    "Generally speaking, you can come up with many variations on this implementation. In our case, the processing will be as follows:\n",
    "\n",
    "$$ h_t = tanh (W_h [h_ {t-1}; x_t] + b_h) $$\n",
    "\n",
    "$ h_ {t-1} $ is the hidden state obtained in the previous step, $ x_t $ is the input vector. $ [h_ {t-1}; x_t] $ is a simple concatenation of vectors. Just like in the picture!\n",
    "\n",
    "Let's check our network on a very simple task: make it say the index of the first element in the sequence.\n",
    "\n",
    "Those. for the sequence `[1, 2, 1, 3]` the network must predict `1`.\n",
    "\n",
    "Let's start with the generation of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yOI4JGgHT-z3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[9, 8, 5, 2, 0, 5, 0, 7, 1, 1, 4, 3, 9, 7, 8, 1, 3, 7, 8, 4, 1, 4, 5, 4,\n",
       "          9, 2, 7, 6, 9, 9, 6, 5, 4, 7, 3, 3, 5, 7, 5, 2, 2, 4, 1, 7, 0, 9, 4, 4,\n",
       "          9, 4, 0, 9, 3, 6, 1, 5, 1, 2, 6, 2, 4, 1, 5, 4, 2, 6, 1, 0, 2, 6, 0, 5,\n",
       "          8, 7, 2, 5, 0, 4, 5, 7, 0, 7, 4, 5, 8, 4, 9, 2, 1, 2, 5, 3, 1, 1, 4, 6,\n",
       "          3, 5, 1, 8, 7, 6, 6, 8, 0, 1, 3, 4, 8, 6, 3, 6, 5, 5, 5, 5, 6, 9, 4, 1,\n",
       "          2, 0, 9, 6, 6, 8, 2, 8],\n",
       "         [4, 3, 0, 8, 9, 5, 7, 6, 1, 6, 2, 0, 8, 5, 7, 6, 4, 8, 9, 0, 6, 0, 6, 0,\n",
       "          9, 0, 8, 9, 5, 1, 3, 8, 1, 3, 9, 6, 7, 8, 7, 4, 0, 7, 4, 3, 5, 9, 5, 7,\n",
       "          8, 6, 9, 7, 7, 5, 6, 0, 5, 6, 2, 6, 7, 3, 5, 8, 2, 1, 7, 4, 0, 7, 5, 9,\n",
       "          0, 7, 9, 3, 8, 6, 3, 8, 8, 4, 2, 7, 0, 6, 7, 6, 1, 0, 4, 1, 1, 1, 4, 0,\n",
       "          5, 8, 0, 9, 7, 8, 7, 9, 3, 6, 3, 8, 7, 4, 0, 8, 4, 5, 1, 8, 7, 6, 6, 3,\n",
       "          4, 8, 4, 1, 2, 7, 1, 4],\n",
       "         [2, 1, 5, 4, 3, 6, 7, 0, 0, 8, 7, 7, 0, 5, 5, 1, 1, 2, 8, 5, 4, 6, 4, 7,\n",
       "          7, 9, 5, 2, 4, 9, 6, 2, 5, 3, 7, 1, 0, 6, 5, 8, 2, 7, 8, 9, 1, 4, 8, 6,\n",
       "          2, 9, 6, 0, 3, 8, 9, 4, 1, 8, 3, 4, 7, 6, 7, 0, 3, 7, 5, 2, 2, 4, 7, 7,\n",
       "          6, 4, 0, 8, 7, 8, 3, 5, 9, 5, 3, 7, 1, 2, 3, 2, 4, 2, 9, 4, 0, 1, 6, 6,\n",
       "          0, 8, 2, 8, 3, 5, 4, 1, 2, 7, 8, 1, 4, 9, 4, 8, 8, 8, 6, 1, 9, 4, 6, 0,\n",
       "          9, 2, 1, 0, 5, 9, 2, 4],\n",
       "         [1, 5, 3, 9, 8, 3, 5, 0, 2, 7, 5, 1, 9, 5, 9, 0, 6, 0, 1, 6, 4, 8, 4, 2,\n",
       "          9, 4, 6, 6, 5, 1, 4, 5, 2, 6, 0, 0, 0, 0, 5, 2, 6, 0, 5, 6, 5, 3, 0, 4,\n",
       "          3, 6, 9, 8, 4, 5, 3, 0, 7, 3, 7, 7, 2, 8, 4, 8, 1, 5, 8, 5, 1, 7, 0, 5,\n",
       "          0, 2, 1, 5, 0, 2, 3, 4, 5, 3, 9, 6, 7, 5, 7, 2, 5, 9, 9, 8, 9, 0, 2, 9,\n",
       "          6, 7, 8, 3, 8, 4, 4, 9, 9, 5, 4, 7, 4, 7, 2, 7, 4, 2, 9, 1, 4, 4, 5, 8,\n",
       "          5, 3, 0, 1, 5, 9, 8, 7],\n",
       "         [5, 9, 3, 3, 6, 5, 1, 6, 6, 4, 3, 8, 5, 6, 1, 2, 6, 1, 1, 0, 7, 8, 8, 4,\n",
       "          4, 4, 8, 4, 3, 8, 9, 7, 3, 3, 9, 6, 8, 8, 8, 7, 7, 6, 0, 7, 8, 4, 1, 6,\n",
       "          4, 2, 2, 7, 0, 2, 0, 0, 6, 4, 1, 8, 6, 1, 1, 0, 7, 8, 8, 3, 7, 6, 2, 2,\n",
       "          9, 2, 6, 9, 5, 0, 2, 9, 4, 7, 1, 4, 4, 1, 0, 0, 5, 8, 2, 7, 8, 1, 9, 7,\n",
       "          0, 6, 8, 3, 8, 2, 2, 9, 3, 7, 8, 4, 8, 8, 9, 4, 2, 7, 5, 8, 8, 1, 7, 9,\n",
       "          0, 0, 7, 1, 0, 9, 7, 6]]),\n",
       " tensor([9, 8, 5, 2, 0, 5, 0, 7, 1, 1, 4, 3, 9, 7, 8, 1, 3, 7, 8, 4, 1, 4, 5, 4,\n",
       "         9, 2, 7, 6, 9, 9, 6, 5, 4, 7, 3, 3, 5, 7, 5, 2, 2, 4, 1, 7, 0, 9, 4, 4,\n",
       "         9, 4, 0, 9, 3, 6, 1, 5, 1, 2, 6, 2, 4, 1, 5, 4, 2, 6, 1, 0, 2, 6, 0, 5,\n",
       "         8, 7, 2, 5, 0, 4, 5, 7, 0, 7, 4, 5, 8, 4, 9, 2, 1, 2, 5, 3, 1, 1, 4, 6,\n",
       "         3, 5, 1, 8, 7, 6, 6, 8, 0, 1, 3, 4, 8, 6, 3, 6, 5, 5, 5, 5, 6, 9, 4, 1,\n",
       "         2, 0, 9, 6, 6, 8, 2, 8]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_data(batch_size=128, seq_len=5):\n",
    "    data = torch.randint(0, 10, size=(seq_len, batch_size), dtype=torch.long)\n",
    "    return data, data[0]\n",
    "\n",
    "X_val, y_val = generate_data()\n",
    "X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQ0Gsr4SFNtB"
   },
   "source": [
    "Please note that the batch has the dimension `(sequence_length, batch_size, input_size)`. All `RNN` in pytorch work with this default format.\n",
    "\n",
    "This is done for performance reasons, but you can change this behavior with the help of the `batch_first` argument if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PqS7HPRhZSBC"
   },
   "source": [
    "**Task** Implement the `SimpleRNN` class, which performs the calculation using the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ed1b2TUvZRs0"
   },
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self._hidden_size = hidden_size\n",
    "        <create Linear layer>\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        seq_len, batch_size = inputs.shape[:2]\n",
    "        if hidden is None:\n",
    "            hidden = inputs.new_zeros((batch_size, self._hidden_size))\n",
    "         \n",
    "        for i in range(seq_len):\n",
    "            <apply linear layer to concatenation of current input (inputs[i]) and hidden>\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KS2xw2YIZ_EU"
   },
   "source": [
    "\n",
    "It should be clear why it is useful to have the first dimension seq_len - you need to be able to take `inputs [i]` - the subbatch related to this timestamp. If the data were located differently, this operation would be much more expensive.\n",
    "\n",
    "** Task ** Implement the `MemorizerModel` class, with the sequence` Embedding -> SimpleRNN -> Linear `. You can use `nn.Sequential`\n",
    "\n",
    "To make embeddings, you can use `nn.Embedding.from_pretrained`. For simplicity, we will do a one-hot-encoding representation — to do this, we simply need to initialize the network with the unit matrix `torch.eye (N)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEUr4Xa9Z81I"
   },
   "outputs": [],
   "source": [
    "# u can use nn.Sequential too\n",
    "class MemorizerModel(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        <create layers>\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        <apply 'em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiDRoQWDawaW"
   },
   "source": [
    "Run the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IbVk7zUjUQ_v"
   },
   "outputs": [],
   "source": [
    "rnn = MemorizerModel(hidden_size=32)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters())\n",
    "\n",
    "total_loss = 0\n",
    "epochs_count = 1000\n",
    "for epoch_ind in range(epochs_count):\n",
    "    X_train, y_train = generate_data(seq_len=25)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    rnn.train()\n",
    "    \n",
    "    logits = rnn(X_train)\n",
    "\n",
    "    loss = criterion(logits, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if (epoch_ind + 1) % 100 == 0:\n",
    "        rnn.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = rnn(X_val)\n",
    "            val_loss = criterion(logits, y_val)\n",
    "            print('[{}/{}] Train: {:.3f} Val: {:.3f}'.format(epoch_ind + 1, epochs_count, \n",
    "                                                             total_loss / 100, val_loss.item()))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQJg3FROIq4v"
   },
   "source": [
    "**Task** Look at how sequence length affects network performance.\n",
    "\n",
    "First, look at how long the network is able to learn. Secondly, try to train a network with a short sequence length, and then apply it to longer ones.\n",
    "\n",
    "**Assignment** It is stated that `relu` fits RNN better. Try it too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSHNuT5b61Ky"
   },
   "source": [
    "## Training RNN\n",
    "\n",
    "\n",
    "<img src=\"https://image.ibb.co/cEYkw9/rnn_bptt_with_gradients.png\">\n",
    "\n",
    "*From [Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)*\n",
    "\n",
    "\n",
    "If everything went according to plan, we had to look at how RNN's were forgotten.\n",
    "\n",
    "To understand the reason, it is worth remembering exactly how the RNN learning takes place, for example, here: [Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/) или здесь - [Vanishing Gradients & LSTMs](http://harinisuresh.com/2016/10/09/lstms/).\n",
    "\n",
    "In short, one of the problems of learning recurrent networks is * explosion of gradients *. It manifests itself when the matrix of weights is such that it increases the norm of the gradient vector during the reverse pass. As a result, the rate of the gradient grows exponentially and it \"explodes.\"\n",
    "\n",
    "This problem can be solved using clipping gradients: `nn.utils.clip_grad_norm_ (rnn.parameters (), 1.)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues with vanilla RNNs\n",
    "\n",
    "There are several issues with the vanilla RNN that we've seen so far. \n",
    "\n",
    "1. When we have an input sequence that has many time steps, it becomes difficult for the model to retain information seen earlier as we process more and more of the downstream timesteps. The goals of the model is to retain the useful components in the previously seen time steps but this becomes cumbersome when we have so many time steps to process. \n",
    "\n",
    "2. During backpropagation, the gradient from the loss has to travel all the way back towards the first time step. If our gradient is larger than 1 (${1.01}^{1000} = 20959$) or less than 1 (${0.99}^{1000} = 4.31e-5$) and we have lot's of time steps, this can quickly spiral out of control.\n",
    "\n",
    "To address both these issues, the concept of gating was introduced to RNNs. Gating allows RNNs to control the information flow between each time step to optimize on the task. Selectively allowing information to pass through allows the model to process inputs with many time steps. The most common RNN gated varients are the long short term memory ([LSTM](https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM)) units and gated recurrent units ([GRUs](https://pytorch.org/docs/stable/nn.html#torch.nn.GRU)). You can read more about how these units work [here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/gates.png\" width=900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU in PyTorch\n",
    "gru = nn.GRU(input_size=embedding_dim, hidden_size=rnn_hidden_dim, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "# Initialize synthetic input\n",
    "x_in = torch.randn(batch_size, seq_size, embedding_dim)\n",
    "print (x_in.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: torch.Size([5, 10, 256])\n",
      "h_n: torch.Size([1, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "out, h_n = gru(x_in)\n",
    "print (\"out:\", out.size())\n",
    "print (\"h_n:\", h_n.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Choosing whether to use GRU or LSTM really depends on the data and empirical performance. GRUs offer comparable performance with reduce number of parameters while LSTMs are more efficient and may make the difference in performance for your particular task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13x5erUgTjDC"
   },
   "source": [
    "## LSTM vs GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAjZh9YkYAMH"
   },
   "source": [
    "Another problem is *attenuation of gradients*. It is connected the opposite - with the exponential decay of gradients. And now it is solved in more complicated ways.\n",
    "\n",
    "Namely - use gate'ovye architecture.\n",
    "\n",
    "The idea of gate is simple, but important, they are used not only in recurrent networks.\n",
    "\n",
    "If you look at how our SimpleRNN works, you will notice that each time the memory (ie, $ h_t $) is overwritten. I want to be able to make this rewrite controlled: do not discard any important information from the vector.\n",
    "\n",
    "Let's get for this the vector $g \\in \\{0,1 \\}^n $, which will say which $h_{t-1}$ cells are good, and instead of which ones it is worth substituting new values:\n",
    "\n",
    "$$ h_t = g \\odot f (x_t, h_{t-1}) + (1 - g) \\odot h_{t-1}. $$\n",
    "\n",
    "For example:\n",
    "$$\n",
    " \\begin{bmatrix}\n",
    "  8 \\\\\n",
    "  11 \\\\\n",
    "  3 \\\\\n",
    "  7\n",
    " \\end{bmatrix} =\n",
    " \\begin{bmatrix}\n",
    "  0 \\\\\n",
    "  1 \\\\\n",
    "  0 \\\\\n",
    "  0\n",
    " \\end{bmatrix}\n",
    " \\odot\n",
    "  \\begin{bmatrix}\n",
    "  7 \\\\\n",
    "  11 \\\\\n",
    "  6 \\\\\n",
    "  5\n",
    " \\end{bmatrix}\n",
    " +\n",
    "  \\begin{bmatrix}\n",
    "  1 \\\\\n",
    "  0 \\\\\n",
    "  1 \\\\\n",
    "  1\n",
    " \\end{bmatrix}\n",
    " \\odot\n",
    "  \\begin{bmatrix}\n",
    "  8 \\\\\n",
    "  5 \\\\\n",
    "  3 \\\\\n",
    "  7\n",
    " \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To achieve differentiability, we use sigmoid: $ \\sigma(f (x_t, h_ {t-1})) $.\n",
    "\n",
    "As a result, the network itself will, looking at the inputs, decide which cells of its memory and how much it costs to rewrite.\n",
    "\n",
    "### LSTM\n",
    "\n",
    "It seems that the first architecture that applied this mechanism was LSTM (Long Short-Term Memory).\n",
    "\n",
    "In it, we also add $ c_ {t-1} $ to $ h_ {t-1} $: $ h_ {t-1} $ is all the same hidden states obtained in the previous step, and $ c_ {t -1} $ is a memory vector.\n",
    "\n",
    "Schematically - something like this:\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" width=\"50%\">\n",
    "\n",
    "*From [(Understanding LSTM Networks)](http://colah.github.io/posts/2015-08-Understanding-LSTMs)*\n",
    "\n",
    "\n",
    "For a start, we can in the same way, as before, calculate a new hidden state (we denote it by $ \\tilde c_{t} $):\n",
    "\n",
    "$$ \\tilde c_{t} = tanh(W_h [h_ {t-1}; x_t] + b_h) $$\n",
    "\n",
    "In normal RNNs, we would simply overwrite the value of the latent state with this value. And now we want to understand how much information we need from $ c_ {t-1} $ and from $ \\tilde c_ {t} $.\n",
    "\n",
    "Rate it sigmoid:\n",
    "$$f = \\sigma(W_f [h_{t-1}; x_t] + b_f),$$\n",
    "$$i = \\sigma(W_i [h_{t-1}; x_t] + b_i).$$\n",
    "\n",
    "The first is about how much you want to forget the old information. The second is how interesting is the new one. Then\n",
    "\n",
    "$$ c_t = f \\odot c_ {t-1} + i \\odot \\tilde c_t. $$\n",
    "\n",
    "We will also weigh the new hidden state:\n",
    "\n",
    "$$ o = \\sigma (W_o [h_ {t-1}; x_t] + b_o), $$\n",
    "$$ h_t = o \\odot tanh (c_t). $$\n",
    "\n",
    "Another picture:\n",
    "\n",
    "<img src=\"https://image.ibb.co/e6HQUU/details.png\">\n",
    " \n",
    "*From [Vanishing Gradients & LSTMs](http://harinisuresh.com/2016/10/09/lstms/)*\n",
    "\n",
    "Why is the problem of damped gradients solved? Because look at the derivative $ \\frac {\\partial c_t} {\\partial c_ {t-1}} $. It is proportional to the $ f $ gate. If $ f = 1 $ - gradients flow unchanged. Otherwise - well, the network itself learns when it wants to forget something.\n",
    "\n",
    "It is highly recommended to read the article: [Understanding LSTM Networks] (http://colah.github.io/posts/2015-08-Understanding-LSTMs/) for more information and fun pictures.\n",
    "\n",
    "Why did I write these formulas? The main thing is to show how much more parameters you need to learn in LSTM compared to a regular RNN. Four times more!\n",
    "\n",
    "For those who fell asleep - [video, as forgets RNN (bottom)](https://www.youtube.com/watch?v=mLxsbWAYIpw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional RNNs\n",
    "There have been many advancements with RNNs ([attention](https://www.oreilly.com/ideas/interpretability-via-attentional-and-memory-based-interfaces-using-tensorflow), Quasi RNNs, etc.) that we will cover in later lessons but one of the basic and widely used ones are bidirectional RNNs (Bi-RNNs). The motivation behind bidirectional RNNs is to process an input sequence by both directions. Accounting for context from both sides can aid in performance when the entire input sequence is known at time of inference. A common application of Bi-RNNs is in translation where it's advantageous to look at an entire sentence from both sides when translating to another language (ie. Japanese → English).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/birnn.png\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiGRU in PyTorch\n",
    "bi_gru = nn.GRU(input_size=embedding_dim, hidden_size=rnn_hidden_dim, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: torch.Size([5, 10, 512])\n",
      "h_n: torch.Size([2, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "out, h_n = bi_gru(x_in)\n",
    "print (\"out:\", out.size()) # collection of all hidden states from the RNN for each time step\n",
    "print (\"h_n:\", h_n.size()) # last hidden state from the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_9KWgbwQMatn"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7-fQPwJUKtV"
   },
   "outputs": [],
   "source": [
    "symbols = set(symb for word in data_train for symb in word)\n",
    "char2ind = {symb: ind + 1 for ind, symb in enumerate(symbols)}\n",
    "char2ind[''] = 0\n",
    "\n",
    "lang2ind = {lang: ind for ind, lang in enumerate(set(labels_train))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dcFgEy7YeFw0"
   },
   "source": [
    "Convert dataset.\n",
    "\n",
    "**Task** Write a batch generator that will select a random set of words on the fly and convert them into matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWWClVsTRVuA"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, labels, char2ind, lang2ind, batch_size):\n",
    "    # let's do the conversion part first\n",
    "    labels = np.array([lang2ind[label] for label in labels])\n",
    "    data = [[char2ind.get(symb, 0) for symb in word] for word in data]\n",
    "    \n",
    "    indices = np.arange(len(data))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, len(data), batch_size):\n",
    "        end = min(start + batch_size, len(data))\n",
    "        \n",
    "        batch_indices = indices[start: end]\n",
    "        \n",
    "        max_word_len = max(len(data[ind]) for ind in batch_indices)\n",
    "        X = np.zeros((max_word_len, len(batch_indices)))\n",
    "        <fill X>\n",
    "            \n",
    "        yield X, labels[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfeR4B_hbH9P"
   },
   "source": [
    "Лень передавать `char2ind, lang2ind`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uA-_jRNdaCM3"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "iterate_batches = partial(iterate_batches, char2ind=char2ind, lang2ind=lang2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HD5i7WmTVlGk"
   },
   "outputs": [],
   "source": [
    "next(iterate_batches(data, labels, batch_size=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnkAUetgs6Tr"
   },
   "source": [
    "**Задание** Реализуйте простую модель на `SimpleRNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zk3OSidVS_px"
   },
   "outputs": [],
   "source": [
    "class SurnamesClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_hidden_dim, classes_count):\n",
    "        super().__init__()\n",
    "        \n",
    "        <set layers>\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        'embed(inputs) -> prediction'\n",
    "        <implement it>\n",
    "    \n",
    "    def embed(self, inputs):\n",
    "        'inputs -> word embedding'\n",
    "        <and it> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vXN-QIrZs95"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None):  \n",
    "    epoch_loss = 0.\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    model.train(is_train)\n",
    "    \n",
    "    data, labels = data\n",
    "    batchs_count = math.ceil(len(data) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (X_batch, y_batch) in enumerate(iterate_batches(data, labels, batch_size=batch_size)):\n",
    "            X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                optimizer.step()\n",
    "\n",
    "            print('\\r[{} / {}]: Loss = {:.4f}'.format(i, batchs_count, loss.item()), end='')\n",
    "                \n",
    "    return epoch_loss / batchs_count\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss = do_epoch(model, criterion, train_data, batch_size, optimizer)\n",
    "        \n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}'\n",
    "        if not val_data is None:\n",
    "            val_loss = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, val_loss))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9vBDF2gbypR"
   },
   "outputs": [],
   "source": [
    "model = SurnamesClassifier(vocab_size=len(char2ind), emb_dim=16, lstm_hidden_dim=64, classes_count=len(lang2ind)).cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=128, train_data=(data_train, labels_train),\n",
    "    val_data=(data_test, labels_test), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jC76XyGjigFx"
   },
   "source": [
    "**Задание** Напишите функцию для тестирования полученной сети: пусть она принимает слово и говорит, в каком языке с какой вероятностью это может быть фамилией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsGNbpBVJ3xO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aWqOPgdbIYcl"
   },
   "source": [
    "**Задание** Оцените качество модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dT2QE6IycXo9"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "y_test, y_pred = [], []\n",
    "<calc 'em>\n",
    "\n",
    "print('Accuracy = {:.2%}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test, y_pred, \n",
    "                            target_names=[lang for lang, _ in sorted(lang2ind.items(), key=lambda x: x[1])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B_FZ9x0NInft"
   },
   "source": [
    "## Визуализация эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJkyBV2bAK05"
   },
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.colors import RGB\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    output_notebook()\n",
    "    \n",
    "    if isinstance(color, str): \n",
    "        color = [color] * len(x)\n",
    "    if isinstance(color, np.ndarray):\n",
    "        color = [RGB(*x[:3]) for x in color]\n",
    "    print(color)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: \n",
    "        pl.show(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def get_tsne_projection(word_vectors):\n",
    "    tsne = TSNE(n_components=2, verbose=100)\n",
    "    return scale(tsne.fit_transform(word_vectors))\n",
    "    \n",
    "    \n",
    "def visualize_embeddings(embeddings, token, colors):\n",
    "    tsne = get_tsne_projection(embeddings)\n",
    "    draw_vectors(tsne[:, 0], tsne[:, 1], color=colors, token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1u-74cv7IrH9"
   },
   "source": [
    "Мы опять получили эмбеддинги - символьного уровня теперь.\n",
    "\n",
    "Хочется на них посмотреть\n",
    "\n",
    "**Задание** Посчитайте векторы для случайных слов и выведите их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jt6LsI0NAPAm"
   },
   "outputs": [],
   "source": [
    "word_indices = np.random.choice(np.arange(len(data_test)), 1000, replace=False)\n",
    "words = [data_test[ind] for ind in word_indices]\n",
    "word_labels = [labels_test[ind] for ind in word_indices]\n",
    "\n",
    "model.eval()\n",
    "X_batch, y_batch = next(iterate_batches(words, word_labels, batch_size=1000))\n",
    "embeddings = <calc me>\n",
    "\n",
    "colors = plt.cm.tab20(y_batch) * 255\n",
    "\n",
    "visualize_embeddings(embeddings, words, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnQMDFTgKCY0"
   },
   "source": [
    "## Network visualization\n",
    "\n",
    "At each step, RNN produces some vector. The full layer applies only to the last output. But you can also look at intermediate states - how the network’s opinion changed about what this word refers to.\n",
    "\n",
    "**Task** Write your visualizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86lffPwLKmqt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DickfCNwJZFT"
   },
   "source": [
    "## Network improvement\n",
    "\n",
    "**Task** Replace SimpleRNN with LSTM. Compare quality.\n",
    "\n",
    "**Task** Add Dropout to LSTM (or later). A value of about 0.3 will be adequate.\n",
    "\n",
    "**Task** An important RNN is the Bidirectional RNN. In fact, these are two RNNs, one bypassing the sequence from left to right, the second - vice versa.\n",
    "\n",
    "As a result, for each point in time we have the vector $ h_t = [f_t; b_t] $ is the concatenation (or some other function of $ f_t $ and $ b_t $) of the states $ f_t $ and $ b_t $ of the forward and backward passage of the sequence. In sum, they cover the entire context.\n",
    "\n",
    "\n",
    "In our task, the Bidirectional option can help with the fact that the network will forget less about how the sequence began. That is, we will need to take $ f_N $ and $ b_N $ states: the first is the last state in the passage from left to right, i.e. output from the last character. The second is the last state at the back pass, i.e. output for the first character.\n",
    "\n",
    "Implement the Bidirectional Classifier. To do this, `LSTM` has a` bidirectional` option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ukwXJppHrDwS"
   },
   "source": [
    "# Referrence\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks, Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)  \n",
    "[Understanding LSTM Networks, Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)  \n",
    "[Recurrent Neural Networks Tutorial, Denny Britz](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)  \n",
    "[Vanishing Gradients & LSTMs, Harini Suresh](http://harinisuresh.com/2016/10/09/lstms/)\n",
    "[Non-Zero Initial States for Recurrent Neural Networks](https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html)\n",
    "[Explaining and illustrating orthogonal initialization for recurrent neural networks, Stephen Merity](http://smerity.com/articles/2016/orthogonal_init.html)\n",
    "[Comparative Study of CNN and RNN for Natural Language Processing, Yin, 2017](https://arxiv.org/abs/1702.01923)\n",
    "[cs224n \"Lecture 8: Recurrent Neural Networks and Language Models\"](https://www.youtube.com/watch?v=Keqep_PKrY8)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 05 - RNNs Intro.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
