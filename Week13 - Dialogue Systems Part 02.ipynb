{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4WlMyJVRkzQ"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch==0.4.1\n",
    "!pip -qq install torchtext==0.3.1\n",
    "!pip -qq install spacy==2.0.16\n",
    "!pip install -qq gensim==3.6.0\n",
    "!python -m spacy download en\n",
    "!wget -O squad.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1h8dplcVzRkbrSYaTAbXYEAjcbApMxYQL\"\n",
    "!unzip squad.zip\n",
    "!wget -O opensubs.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1x1mNHweP95IeGFbDJPAI7zffgxrbqb7b\"\n",
    "!unzip opensubs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvJKy3mtVOpw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKCD9Pt4Wupj"
   },
   "source": [
    "# General Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C3fbABnaXFyk"
   },
   "source": [
    "Today we are analyzing how the talker is arranged.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://meduza.io/image/attachments/images/002/547/612/large/RLnxN4VdUmWFcBp8GjxUmA.jpg\" width=\"20%\">\n",
    "</center>\n",
    "\n",
    "In general, we have already discussed the Seq2Seq models that can be used to implement chatters - however, they have a drawback: there is a high probability of generating something ungrammatical. Well, like those pies.\n",
    "\n",
    "Therefore, almost always go the other way - instead of generating apply ranking. You need to make a large base of answers in advance and simply choose the most appropriate to the context each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3mOxTaTjD0-p"
   },
   "source": [
    "## DSSM\n",
    "\n",
    "To do this, use DSSM (Deep Structured Semantic Models):\n",
    "\n",
    "<center>\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-b90431ff9b4c60c5d69069d7bc048ff0\" width=\"20%\">\n",
    "</center>\n",
    "    \n",
    "*From [What are Siamese neural networks, what applications are they good for, and why?](https://www.quora.com/What-are-Siamese-neural-networks-what-applications-are-they-good-for-and-why)*\n",
    "\n",
    "This network consists of (usually) a pair of towers: the left one encodes the request, the right one the answer. The task is to learn to count the proximity between request and response.\n",
    "\n",
    "Then they gather a large body of request-response pairs (a request can be either one question or a context — the last few questions / answers).\n",
    "\n",
    "For answers, their vectors are pre-calculated, each new request is encoded with the help of the right tower, and the nearest one is found among the pre-calculated vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JSz9ALe3w1v1"
   },
   "source": [
    "## Data\n",
    "\n",
    "We will use to start [Stanford Question Answering Dataset (SQuAD)] (https://rajpurkar.github.io/SQuAD-explorer/). In general, the task there is to find the answer to the question in the text. But we will simply choose among the text sentences that are closest to the question.\n",
    "\n",
    "* This part of the laptop is heavily based on [shadovsky laptop](https://github.com/yandexdataschool/nlp_course/blob/master/week10_dialogue/seminar.ipynb)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_6OqXUJxjnX4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_json('train.json')\n",
    "test_data = pd.read_json('test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0DPvn5djkha"
   },
   "outputs": [],
   "source": [
    "row = train_data.iloc[40]\n",
    "print('QUESTION:', row.question, '\\n')\n",
    "for i, cand in enumerate(row.options):\n",
    "    print('[ ]' if i not in row.correct_indices else '[v]', cand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KIeJIQ7ex4nB"
   },
   "source": [
    "Токенизируем предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udkrpaSHq7mg"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy = spacy.load('en')\n",
    "\n",
    "train_data.question = train_data.question.apply(lambda text: [tok.text.lower() for tok in spacy.tokenizer(text)])\n",
    "train_data.options = train_data.options.apply(lambda options: [[tok.text.lower() for tok in spacy.tokenizer(text)] for text in options])\n",
    "\n",
    "test_data.question = test_data.question.apply(lambda text: [tok.text.lower() for tok in spacy.tokenizer(text)])\n",
    "test_data.options = test_data.options.apply(lambda options: [[tok.text.lower() for tok in spacy.tokenizer(text)] for text in options])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tU5jRHpax7Ot"
   },
   "source": [
    "У нас не так-то много данных, чтобы учить всё с нуля, поэтому будем сразу использовать предобученные эмбеддинги:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mQjmbvs-jFQ"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dC8Db3DKyDSz"
   },
   "source": [
    "**Задание** Постройте матрицу предобученных эмбеддингов для самых частотных слов в выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AinFu7nb6DSf"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_word_embeddings(data, w2v_model, min_freq=5):\n",
    "    words = Counter()\n",
    "    \n",
    "    for text in data.question:\n",
    "        for word in text:\n",
    "            words[word] += 1\n",
    "            \n",
    "    for options in data.options:\n",
    "        for text in options:\n",
    "            for word in text:\n",
    "                words[word] += 1\n",
    "                \n",
    "    word2ind = {\n",
    "        '<pad>': 0,\n",
    "        '<unk>': 1\n",
    "    }\n",
    "    \n",
    "    embeddings = [\n",
    "        np.zeros(w2v_model.vectors.shape[1]),\n",
    "        np.zeros(w2v_model.vectors.shape[1])\n",
    "    ]\n",
    "    \n",
    "    <build embeddings>\n",
    "\n",
    "    return word2ind, np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKJhqyV__jhl"
   },
   "outputs": [],
   "source": [
    "word2ind, embeddings = build_word_embeddings(train_data, w2v_model, min_freq=8)\n",
    "print('Vocab size =', len(word2ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-_xdNG9yYka"
   },
   "source": [
    "Для генерации батчей будем использовать такой класс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4IoQrJ5rTSSN"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "def to_matrix(lines, word2ind):\n",
    "    max_sent_len = max(len(line) for line in lines)\n",
    "    matrix = np.zeros((len(lines), max_sent_len))\n",
    "\n",
    "    for batch_ind, line in enumerate(lines):\n",
    "        matrix[batch_ind, :len(line)] = [word2ind.get(word, 1) for word in line]\n",
    "\n",
    "    return LongTensor(matrix)\n",
    "\n",
    "\n",
    "class BatchIterator():\n",
    "    def __init__(self, data, batch_size, word2ind, shuffle=True):\n",
    "        self._data = data\n",
    "        self._num_samples = len(data)\n",
    "        self._batch_size = batch_size\n",
    "        self._word2ind = word2ind\n",
    "        self._shuffle = shuffle\n",
    "        self._batches_count = int(math.ceil(len(data) / batch_size))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._batches_count\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self._iterate_batches()\n",
    "\n",
    "    def _iterate_batches(self):\n",
    "        indices = np.arange(self._num_samples)\n",
    "        if self._shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for start in range(0, self._num_samples, self._batch_size):\n",
    "            end = min(start + self._batch_size, self._num_samples)\n",
    "\n",
    "            batch_indices = indices[start: end]\n",
    "\n",
    "            batch = self._data.iloc[batch_indices]\n",
    "            questions = batch['question'].values\n",
    "            correct_answers = np.array([\n",
    "                row['options'][random.choice(row['correct_indices'])]\n",
    "                for i, row in batch.iterrows()\n",
    "            ])\n",
    "            wrong_answers = np.array([\n",
    "                row['options'][random.choice(row['wrong_indices'])]\n",
    "                for i, row in batch.iterrows()\n",
    "            ])\n",
    "\n",
    "            yield {\n",
    "                'questions': to_matrix(questions, self._word2ind),\n",
    "                'correct_answers': to_matrix(correct_answers, self._word2ind),\n",
    "                'wrong_answers': to_matrix(wrong_answers, self._word2ind)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "as5kgtjLyRE6"
   },
   "outputs": [],
   "source": [
    "train_iter = BatchIterator(train_data, 64, word2ind)\n",
    "test_iter = BatchIterator(test_data, 128, word2ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27zeQlTJzU1x"
   },
   "source": [
    "Он просто сэмплирует последовательности из вопросов, правильных и неправильных ответов на них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-ohzmkXzO0e"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BSXFV1CTyfGo"
   },
   "source": [
    "## Model\n",
    "\n",
    "** Task ** Implement an encoder model for texts - DSSM model towers.\n",
    "\n",
    "* It doesn’t have to be a complex model, a convolutional model will do, which will learn much faster. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ve7WZ4-dvbuw"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_dim=128, output_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        <build some model>\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        <apply it>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFBAUGGfzFwF"
   },
   "source": [
    "### Triplet Loss\n",
    "\n",
    "We do not want just to teach the encoder to build embeddings for proposals. We want to attract vectors of correct answers to questions and push away the wrong ones. For this use, for example, * Triplet Loss *:\n",
    "\n",
    "$$ L = \\frac 1N \\underset {q, a^+, a^-} \\sum max(0, \\space \\delta - sim[V_q(q), V_a(a^+)] + sim[V_q(q), V_a(a^-)] ),$$\n",
    "\n",
    "Where\n",
    "* $ sim [a, b] $ similarity function (for example, dot product or cosine similarity)\n",
    "* $ \\delta $ - model hyperparameter. If $ sim [a, b] $ is linear in $ b $, then all $ \\delta> 0 $ are equivalent.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/margin.png\" width=\"20%\">\n",
    "</center>\n",
    "\n",
    "** Assignment ** Implement triplet loss, as well as counting the recall - the percentage of cases where the correct answer was closer to the wrong one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GdHJK51dz0CB"
   },
   "outputs": [],
   "source": [
    "class DSSM(nn.Module):\n",
    "    def __init__(self, question_encoder, answer_encoder):\n",
    "        super().__init__()\n",
    "        self.question_encoder = question_encoder\n",
    "        self.answer_encoder = answer_encoder\n",
    "        \n",
    "    def forward(self, questions, correct_answers, wrong_answers):\n",
    "        <perform forward pass>\n",
    "\n",
    "    def calc_triplet_loss(self, question_embeddings, correct_answer_embeddings, wrong_answer_embeddings, delta=1.0):\n",
    "        \"\"\"Returns the triplet loss based on the equation above\"\"\"\n",
    "        <do it>\n",
    "        \n",
    "    def calc_recall_at_1(self, question_embeddings, correct_answer_embeddings, wrong_answer_embeddings):\n",
    "        \"\"\"Returns the number of cases when the correct answer were more similar than incorrect one\"\"\"\n",
    "        <and it>\n",
    "        \n",
    "    @staticmethod\n",
    "    def similarity(question_embeddings, answer_embeddings):\n",
    "        \"\"\"Returns sim[a, b]\"\"\"\n",
    "        <and it too>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWvlMTWpxTvJ"
   },
   "outputs": [],
   "source": [
    "class ModelTrainer():\n",
    "    def __init__(self, model, optimizer):\n",
    "        self._model = model\n",
    "        self._optimizer = optimizer\n",
    "        \n",
    "    def on_epoch_begin(self, is_train, name, batches_count):\n",
    "        \"\"\"\n",
    "        Initializes metrics\n",
    "        \"\"\"\n",
    "        self._epoch_loss = 0\n",
    "        self._correct_count, self._total_count = 0, 0\n",
    "        self._is_train = is_train\n",
    "        self._name = name\n",
    "        self._batches_count = batches_count\n",
    "        \n",
    "        self._model.train(is_train)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Outputs final metrics\n",
    "        \"\"\"\n",
    "        return '{:>5s} Loss = {:.5f}, Recall@1 = {:.2%}'.format(\n",
    "            self._name, self._epoch_loss / self._batches_count, self._correct_count / self._total_count\n",
    "        )\n",
    "        \n",
    "    def on_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Performs forward and (if is_train) backward pass with optimization, updates metrics\n",
    "        \"\"\"\n",
    "        \n",
    "        question_embs, correct_answer_embs, wrong_answer_embs = self._model(\n",
    "            batch['questions'], batch['correct_answers'], batch['wrong_answers']\n",
    "        )\n",
    "        loss = self._model.calc_triplet_loss(question_embs, correct_answer_embs, wrong_answer_embs)\n",
    "        correct_count = self._model.calc_recall_at_1(question_embs, correct_answer_embs, wrong_answer_embs)\n",
    "        total_count = len(batch['questions'])\n",
    "        \n",
    "        self._correct_count += correct_count\n",
    "        self._total_count += total_count\n",
    "        self._epoch_loss += loss.item()\n",
    "        \n",
    "        if self._is_train:\n",
    "            self._optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self._model.parameters(), 1.)\n",
    "            self._optimizer.step()\n",
    "\n",
    "        return '{:>5s} Loss = {:.5f}, Recall@1 = {:.2%}'.format(\n",
    "            self._name, loss.item(), correct_count / total_count\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecI_vVBgzpVn"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.get_lock().locks = []\n",
    "\n",
    "\n",
    "def do_epoch(trainer, data_iter, is_train, name=None):\n",
    "    trainer.on_epoch_begin(is_train, name, batches_count=len(data_iter))\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=len(data_iter)) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                batch_progress = trainer.on_batch(batch)\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description(batch_progress)\n",
    "                \n",
    "            epoch_progress = trainer.on_epoch_end()\n",
    "            progress_bar.set_description(epoch_progress)\n",
    "            progress_bar.refresh()\n",
    "\n",
    "            \n",
    "def fit(trainer, train_iter, epochs_count=1, val_iter=None):\n",
    "    best_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        do_epoch(trainer, train_iter, is_train=True, name=name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            do_epoch(trainer, val_iter, is_train=False, name=name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1GvkkLh70S6"
   },
   "source": [
    "Запустим, наконец, учиться модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipXj9pOD2afY"
   },
   "outputs": [],
   "source": [
    "embeddings = FloatTensor(embeddings)\n",
    "\n",
    "model = DSSM(\n",
    "    Encoder(embeddings),\n",
    "    Encoder(embeddings)\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "trainer = ModelTrainer(model, optimizer)\n",
    "\n",
    "fit(trainer, train_iter, epochs_count=30, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_AHIoCB73XA"
   },
   "source": [
    "### Prediction accuracy\n",
    "\n",
    "Let us evaluate how well the model predicts the correct answer.\n",
    "\n",
    "** Task ** For each question, find the response index generated by the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QrA8N0zDJczj"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "<collect prediction indices>\n",
    "    \n",
    "accuracy = np.mean([\n",
    "    answer in correct_ind\n",
    "    for answer, correct_ind in zip(predictions, test_data['correct_indices'].values)\n",
    "])\n",
    "print(\"Accuracy: %0.5f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HZijVoOTLcCD"
   },
   "outputs": [],
   "source": [
    "def draw_results(question, possible_answers, predicted_index, correct_indices):\n",
    "    print(\"Q:\", ' '.join(question), end='\\n\\n')\n",
    "    for i, answer in enumerate(possible_answers):\n",
    "        print(\"#%i: %s %s\" % (i, '[*]' if i == predicted_index else '[ ]', ' '.join(answer)))\n",
    "    \n",
    "    print(\"\\nVerdict:\", \"CORRECT\" if predicted_index in correct_indices else \"INCORRECT\", \n",
    "          \"(ref: %s)\" % correct_indices, end='\\n' * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-_pYbNcsLfli"
   },
   "outputs": [],
   "source": [
    "for i in [1, 100, 1000, 2000, 3000, 4000, 5000]:\n",
    "    draw_results(test_data.iloc[i].question, test_data.iloc[i].options,\n",
    "                 predictions[i], test_data.iloc[i].correct_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5OC4EGt8HAo"
   },
   "source": [
    "## Hard-negatives mining\n",
    "\n",
    "In fact, in most cases we have negative examples.\n",
    "\n",
    "For example, there is a base of dialogues - and where to take negative examples to the answers?\n",
    "\n",
    "To do this, use * hard-negatives mining *. Take as a negative example the closest of the wrong examples in the batch:\n",
    "$$a^-_{hard} = \\underset {a^-} {argmax} \\space sim[V_q(q), V_a(a^-)]$$\n",
    "\n",
    "Wrong in this case - everything except the right :)\n",
    "\n",
    "It is implemented somehow like this:\n",
    "* Butch consists of the correct question-answer pairs.\n",
    "* For all questions and all answers, consider embeddings.\n",
    "* We have positive examples - it remains to find for each question the most similar answers that were intended for other questions.\n",
    "\n",
    "** Assignment ** Update `DSSM` to do hard-negatives mining inside it.\n",
    "\n",
    "* It may be necessary to normalize the vectors using `F.normalize` before calculating` similarity` *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tY_BebgAOY70"
   },
   "outputs": [],
   "source": [
    "class DSSM(nn.Module):\n",
    "    def __init__(self, question_encoder, answer_encoder):\n",
    "        super().__init__()\n",
    "        self.question_encoder = question_encoder\n",
    "        self.answer_encoder = answer_encoder\n",
    "        \n",
    "    def forward(self, questions, correct_answers, wrong_answers):\n",
    "        \"\"\"Ignore wrong_answers, they are here just for compatibility sake\"\"\"\n",
    "        <perform forward pass>\n",
    "\n",
    "    def calc_triplet_loss(self, question_embeddings, answer_embeddings, delta=1.0):\n",
    "        \"\"\"Returns the triplet loss based on the equation above\"\"\"\n",
    "        <calc triple loss with hard-negatives>\n",
    "        \n",
    "    def calc_recall_at_1(self, question_embeddings, correct_answer_embeddings, wrong_answer_embeddings):\n",
    "        \"\"\"Returns the number of cases when the correct answer were more similar than incorrect one\"\"\"\n",
    "        <calc recall>\n",
    "        \n",
    "    @staticmethod\n",
    "    def similarity(question_embeddings, answer_embeddings):\n",
    "        <calc it>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AIvf3Zvtabs"
   },
   "outputs": [],
   "source": [
    "model = DSSM(\n",
    "    question_encoder=Encoder(embeddings),\n",
    "    answer_encoder=Encoder(embeddings)\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "trainer = ModelTrainer(model, optimizer)\n",
    "\n",
    "fit(trainer, train_iter, epochs_count=30, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXaL1iRz-zEG"
   },
   "source": [
    "** Assignment ** There is also an option with semi-hard negatives - when the best example is taken among those whose similarity is less than the similarity of a question with a positive example. Try to implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edUVeXG0_lCa"
   },
   "source": [
    "# Chatty\n",
    "\n",
    "To implement a chat, you need a normal body with dialogs. For example, OpenSubtitles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s3YmDo9Z_xG-"
   },
   "outputs": [],
   "source": [
    "!head train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0R77ezW_1Wd"
   },
   "source": [
    "Ну, примерно нормальный.\n",
    "\n",
    "Считаем датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SwHYmb28HPUn"
   },
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "\n",
    "def read_dataset(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for line in tqdm(f):\n",
    "            query, response = line.strip().split('\\t')\n",
    "            data.append((\n",
    "                wordpunct_tokenize(query.strip()),\n",
    "                wordpunct_tokenize(response.strip())\n",
    "            ))\n",
    "    return data\n",
    "\n",
    "train_data = read_dataset('train.txt')\n",
    "val_data = read_dataset('valid.txt')\n",
    "test_data = read_dataset('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WTfkv17tHM3I"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, Example, Dataset, BucketIterator\n",
    "\n",
    "query_field = Field(lower=True)\n",
    "response_field = Field(lower=True)\n",
    "\n",
    "fields = [('query', query_field), ('response', response_field)]\n",
    "\n",
    "train_dataset = Dataset([Example.fromlist(example, fields) for example in train_data], fields)\n",
    "val_dataset = Dataset([Example.fromlist(example, fields) for example in val_data], fields)\n",
    "test_dataset = Dataset([Example.fromlist(example, fields) for example in test_data], fields)\n",
    "\n",
    "query_field.build_vocab(train_dataset, min_freq=5)\n",
    "response_field.build_vocab(train_dataset, min_freq=5)\n",
    "\n",
    "print('Query vocab size =', len(query_field.vocab))\n",
    "print('Response vocab size =', len(response_field.vocab))\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "    datasets=(train_dataset, val_dataset, test_dataset), batch_sizes=(512, 1024, 1024), \n",
    "    shuffle=True, device=DEVICE, sort=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-Ok7--NHUX0"
   },
   "source": [
    "**Задание** Реализовать болталку по аналогии с тем, что уже написали."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2x9-j4oz08p"
   },
   "source": [
    "# Referrence\n",
    "Learning Deep Structured Semantic Models for Web Search using Clickthrough Data, 2013 [[pdf]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf)  \n",
    "Deep Learning and Continuous Representations for Natural Language Processing, Microsoft tutorial [[pdf]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/NAACL-HLT-2015_tutorial.pdf)\n",
    "\n",
    "[Neural conversational models: как научить нейронную сеть светской беседе](https://habr.com/company/yandex/blog/333912/)  \n",
    "[Искусственный интеллект в поиске. Как Яндекс научился применять нейронные сети, чтобы искать по смыслу, а не по словам](https://habr.com/company/yandex/blog/314222/)  \n",
    "[Triplet loss, Olivier Moindrot](https://omoindrot.github.io/triplet-loss)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 13 - Dialogue Systems (Part 2).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
