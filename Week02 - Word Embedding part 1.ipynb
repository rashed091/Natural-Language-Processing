{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"text_cell_render border-box-sizing rendered_html\">\n",
    "<h1 id=\"Word2Vec\">Word2Vec<a class=\"anchor-link\" href=\"#Word2Vec\">&#182;</a></h1><p>We use the tf-idf to transform a document into a vector. This vector can be seen as the weighted sum of <strong>one-hot vectors</strong> of distinct words in the document. The one-hot vectors for words have some limitations. For example, we know that \"cat\" is more similar to \"dog\" than \"cow\" semantically. However, the one-hot vectors of these three words has the same distance: 0. Can we embed words into a vector space that preserves the semantic distance between words?</p>\n",
    "<p><strong>Word2vec</strong> is an NN-based word embedding method. It is able to represent words in a continuous, low dimensional vector space (i.e., the embedding space) where semantically similar words are mapped to nearby points. The main idea of word2vec is to uses an NN with only <strong>1 linear hidden layer</strong> (i.e., each hidden unit has the linear activation function $a_j^{(1)}=z_j^{(1)}$) to use a word to predict its neighbors, as shown below:</p>\n",
    "<p><img src=\"../images/fig-word2vec-sg.png\" width=\"350\"></p>\n",
    "<p>This is based on a key observation that <strong>semantically similar words are often used interchangeably in different contexts</strong>. For example, the words \"cat\" and \"dog\" may both appear in a context \"___ is my favorate pet.\" When feeding \"cat\" and \"dog\" into the NN to predict their nearby words, these two words will be likely to share the same/similar hidden representation ($\\boldsymbol{a}^{(1)}$ at layer 1) in order to predict the same/similar nearby words ($\\boldsymbol{a}^{(2)}$ at layer 2). After training the NN, we can use the weight matrix $\\boldsymbol{W}^{(1)}$ to encode a one-hot vector $\\boldsymbol{x}$ into a low dimensional dense vector $\\boldsymbol{h}$, by $\\boldsymbol{h}=\\boldsymbol{W}^{(1)\\top}\\boldsymbol{x}$.</p>\n",
    "<p>In the literature, the architecture of an word2vect NN comes in two flavors, the Continuous Bag-of-Words model (CBOW) and the skip-gram (SG) model.</p>\n",
    "<table>\n",
    "<thead><tr>\n",
    "<th style=\"text-align:center\"><img src=\"../images/fig-word2vec-cbow.png\" width=\"350\"></th>\n",
    "<th style=\"text-align:center\"><img src=\"../images/fig-word2vec-sg_1.png\" width=\"350\"></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td style=\"text-align:center\">CBOW</td>\n",
    "<td style=\"text-align:center\">Skip-gram</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<p>Algorithmically, these models are similar, except that CBOW predicts target words (e.g. \"cat\") from context words (\"is,\" \"my,\" \"favorite,\" and \"pet\"), while the skip-gram does the inverse and predicts context words from the target words. This inversion might seem like an arbitrary choice, but statistically it has the effect that CBOW smoothes over a lot of the distributional information (by treating an entire context as one example (C:[\"is,\" \"my,\" \"favorite,\" \"pet\"], T:\"cat\")). For the most part, this turns out to be a useful thing for smaller datasets. On the other hand, skip-gram treats each context-target pair (e.g., (T:\"cat\", C:\"pet\")) as a new observation and is shown to be able to capture the semantics better when we have a large dataset. We focus on the skip-gram model in the following.</p>\n",
    "<p>Note that the weights are shared across words to ensure that each word has a single embedding. This is called <strong>weight tying</strong>. Also, word2vec is a <strong>unsupervised learning</strong> task as it does not require explicit labels. An NN can be used for both supervised and unsupervised learning tasks.</p>\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"cell border-box-sizing text_cell rendered\">\n",
    "<div class=\"prompt input_prompt\">\n",
    "</div>\n",
    "<div class=\"inner_cell\">\n",
    "<div class=\"text_cell_render border-box-sizing rendered_html\">\n",
    "<h3 id=\"Cost-Function-and-Output-Layer\">Cost Function and Output Layer<a class=\"anchor-link\" href=\"#Cost-Function-and-Output-Layer\">&#182;</a></h3><p>As most NNs, a skip-gram word2vec model is trained using the maximum likelihood (ML) principle:</p>\n",
    "$$\\arg\\min_{\\Theta}\\sum_{i=1}^{N}{-\\log\\mathrm{P}(\\boldsymbol{y}^{(i)}\\,|\\,\\boldsymbol{x}^{(i)},\\Theta)}.$$<p>Unlike the binary classifier NN we have seen previously, it needs to output multiple classes (the vocabulary size $V$ in total). In a multiclass task where $y=1,\\cdots,V$, we usually assume</p>\n",
    "$$\\Pr(y\\,|\\,\\boldsymbol{x})\\sim\\mathrm{Categorical}(y\\,|\\,\\boldsymbol{x};\\boldsymbol{\\rho})=\\prod_{i=1}^{V}\\rho_{i}^{1(y;\\,y=i)}.$$<p>It is natural to use $V$ <strong>Softmax units</strong> in the output layer. That is, the activation function of each unit outputs one dimension of the softmax function, a generalization of the logistic sigmoid:</p>\n",
    "$$a_i^{(L)}=\\rho_i=\\mathrm{softmax}(\\boldsymbol{z}^{(L)})_{i}=\\frac{\\exp(z_{i}^{(L)})}{\\sum_{j=1}^{{\\color{red}V}}\\exp(z_{j}^{(L)})}.$$<p>The cost function then becomes:</p>\n",
    "$$\\arg\\min_{\\Theta}\\sum_{i}-\\log\\prod_{j}\\left(\\frac{\\exp(z_{j}^{(L)})}{\\sum_{k=1}^{{\\color{red}V}}\\exp(z_{k}^{(L)})}\\right)^{1(y^{(i)};y^{(i)}=j)}=\\arg\\min_{\\Theta}\\sum_{i}-z_{y^{(i)}}^{(L)}+\\log\\sum_{k=1}^{{\\color{red}V}}\\exp(z_{k}^{(L)})$$<p>Basically, we want to maximize $\\rho_j$ when seeing an example of class $j$. However, this objective introduces hight training cost when $V$ is large. Recall from the lecture that, at every training step in SGD, we need to compute the gradient of the cost function with respect to $\\boldsymbol{z}^{(L)}$. This gradient involves the $z_{i}^{(L)}$ of <strong>every unit</strong> at the output layer, which in turn leads to a lot of weight updates in $\\boldsymbol{W}^{(1)}$ and $\\boldsymbol{W}^{(2)}$ at every training step. The training will be very slow.</p>\n",
    "<h3 id=\"Negative-Sampling\">Negative Sampling<a class=\"anchor-link\" href=\"#Negative-Sampling\">&#182;</a></h3><p>To speed up the training process, we can instead replace the $V$ Softmax units at the output layer with $V$ Sigmoid units and use a new cost function:</p>\n",
    "$$\\arg\\min_{\\Theta}\\sum_{i=1}^{N}\\left[-\\log\\mathrm{P}(y^{(i)}=1\\,|\\,\\boldsymbol{x}^{(i)},\\Theta)-\\sum_{j\\in\\mathbb{N}^{(i)}}\\log\\mathrm{P}(y^{(j)}=0\\,|\\,\\boldsymbol{x}^{(j)},\\Theta)\\right],$$<p>which amounts to:</p>\n",
    "$$\\arg\\min_{\\Theta}\\sum_{i}\\left[-\\log\\rho_{y^{(i)}}-\\sum_{j\\in\\mathbb{N}^{(i)}}\\log(1-\\rho_{y^{(j)}})\\right],$$<p>where $\\rho_{y^{(i)}}=\\sigma(z_{y^{(i)}}^{(L)})$ and $\\mathbb{N}^{(i)}$, $\\vert\\mathbb{N}^{(i)}\\vert=U\\ll N$, is a set of indexes of the <strong>noise words</strong> that have never been in the same context as the target word. The cost function is minimized when the model assigns high $\\rho_{y^{(i)}}$ to the context words, and low $\\rho_{y^{(j)}}$'s to all its noise words. It can be shown that the new model approximates the original one (with the softmax output units) when training on infinite examples. But the new model is computationally advantageous because the cost function involves only $O(U)$ attributes in $\\boldsymbol{z}^{(L)}$ (thus $O(U)$ output units). This leads to much less weight updates at each training step and higher efficiency.</p>\n",
    "<p>The above trick for improving the training efficiency in a multiclass classification task is called the <strong>negative sampling</strong>. Note that each example now contains one context word and additionally $U$ noise words. We need some extra preprocessing steps.</p>\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"cell border-box-sizing text_cell rendered\">\n",
    "<div class=\"prompt input_prompt\">\n",
    "</div>\n",
    "<div class=\"inner_cell\">\n",
    "<div class=\"text_cell_render border-box-sizing rendered_html\">\n",
    "<h3 id=\"Preparing-Training-Examples\">Preparing Training Examples<a class=\"anchor-link\" href=\"#Preparing-Training-Examples\">&#182;</a></h3><p>With the negative sampling, each training example consists of one correct context word and $U$ noise words. To give an example, let's consider the corpus:</p>\n",
    "<pre>\n",
    "the quick brown fox jumped over the lazy dog ...\n",
    "</pre><p>Suppose the context window is 2 words (1 at left and 1 at right). We scan through the corpus by moving the window from left to right and get the context words (C) and target words (T):</p>\n",
    "<pre>\n",
    "(C:[the, brown], T:quick), \n",
    "(C:[quick, fox], T:brown), \n",
    "(C:[brown, jumped], T:fox), \n",
    "...\n",
    "</pre><p>Recall that the skip-gram model tries to predict <strong>each</strong> context word from its target word. So, we break a <code>(C:[context1, context2, ...], T:target)</code> pair into <code>(T:target, C:context1)</code>, <code>(T:target, C:context2)</code>, and so on. Our dataset becomes:</p>\n",
    "<pre>\n",
    "(T:quick, C:the), \n",
    "(T:quick, C:brown), \n",
    "(T:brown, C:quick), \n",
    "(T:brown, C:fox), \n",
    "...\n",
    "</pre><p>To support the negative sampling, we need to sample $U$ noise words (N) for each of the above pairs by following some distribution (typically the unigram distribution). For example, let $U=2$, our final dataset could be:</p>\n",
    "<pre>\n",
    "(T:quick, C:the, N:[over, lazy]), \n",
    "(T:quick, C:brown, N:[fox, over]), \n",
    "(T:brown, C:quick, N:[the, lazy]), \n",
    "(T:brown, C:fox, N:[jumped, dog]), \n",
    "...\n",
    "</pre><p>Given an example tuple, we regard the context word (C) as positive and noise words (N1 and N2) as negative when evaluating the cost function defined above. Specifically, the Sigmoid unit in the output layer outputs $\\rho_C$ for the context word and $\\rho_{N1}$ and $\\rho_{N2}$ for the noise words, and the loss function for this example is</p>\n",
    "$$-\\log\\rho_{C}-\\log(1-\\rho_{N1})-\\log(1-\\rho_{N2}).$$\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "<div class=\"cell border-box-sizing text_cell rendered\">\n",
    "<div class=\"prompt input_prompt\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZNq6ujzPtvd"
   },
   "source": [
    "## Word embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITLgcVz66AfV"
   },
   "source": [
    "Let us consider the high-level API - it has already implemented various classes of spare parts for learning neurons. We will solve the same problem as last time - learning word embeddings, only now we will teach them ourselves! First you need to prepare data for training.\n",
    "\n",
    "Let's collect and tokenize the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519], grad_fn=<SumBackward2>)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(hello_embed, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DF5mYpCsE9Uh"
   },
   "source": [
    "### Skip-Gram Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "om1IG5XEMGRa"
   },
   "source": [
    "Let's start with the skip-gram learning model word2vec.\n",
    "\n",
    "This is a simple model of just two layers. Her idea is to teach embeddingings so that they can predict the context of the corresponding words as best you can. That is, if we have learned well how to encode the words with which this occurs, then we know something about ourselves. For example, in a natural way it turns out that words that occur in the same contexts (say, `apple` and` orange`) will have close embeddingings vectors.\n",
    "\n",
    "<img src=\"https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/Word2vecExample.jpeg\" width=\"50%\">\n",
    "\n",
    "To do this, we model the probabilities $\\{P (w_ {c + j} | w_c): j = ck, ..., c + k, j \\neq c \\} $, where $ k $ is the size of the context window, $ c $ is the index of the central word.\n",
    "\n",
    "Let's put together such a model: we will teach a pair of matrices $ U $ - the embedding matrix, which we will later take for our own tasks, and $ V $ - the matrix of the output layer.\n",
    "\n",
    "\n",
    "Each word in the dictionary corresponds to a row in the $ U $ matrix and a $ V $ column.<img src=\"https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/SkipGram.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "What's going on here? The word is displayed in embedding - the string $ u_c $. Then this embedding is multiplied by the $ V $ matrix.\n",
    "\n",
    "As a result, we obtain a set of the number $ v_j ^ T u_c $ - the degree of similarity of the word with the number $ j $ and our word.\n",
    "\n",
    "We transform these numbers into something like probabilities â€” let's use the softmax function: $ P (i) = \\frac {e ^ {x_i}} {\\sum_j e ^ {x_j}} $.\n",
    "\n",
    "And then we will consider the cross-entropy loss:\n",
    "\n",
    "$$-\\sum_{-k \\leq j \\leq k, j \\neq 0} \\log \\frac{\\exp(v_{c+j}^T u_c)}{\\sum_{i=1}^{|V|} \\exp(v_i^T u_c)} \\to \\min_{U, V}.$$\n",
    "\n",
    "As a result, the vector $ u_c $ will approach the vector $ v_ {c_j} $ from its context.\n",
    "\n",
    "We realize this all to understand.\n",
    "\n",
    "#### Generation batches\n",
    "\n",
    "First you need to collect contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence):\n",
    "    sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n",
    "    \n",
    "    return [x.text for x in nlp.tokenizer(sentence) if x.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_data = pd.read_csv('.data/quora_train.csv')\n",
    "quora_data = quora_data[:3000]\n",
    "quora_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "quora_data.question1 = quora_data.question1.replace(np.nan, '', regex=True)\n",
    "quora_data.question2 = quora_data.question2.replace(np.nan, '', regex=True)\n",
    "\n",
    "texts = list(pd.concat([quora_data.question1, quora_data.question2]).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer(text.lower()) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 5\n",
    "\n",
    "words_counter = Counter(token for tokens in tokenized_texts for token in tokens)\n",
    "word2index = {\n",
    "    '<unk>': 0\n",
    "}\n",
    "\n",
    "for word, count in words_counter.most_common():\n",
    "    if count < MIN_COUNT:\n",
    "        break\n",
    "        \n",
    "    word2index[word] = len(word2index)\n",
    "    \n",
    "index2word = [word for word, _ in sorted(word2index.items(), key=lambda x: x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1628\n",
      "Tokens count: 73546\n",
      "Unknown tokens appeared: 10052\n",
      "Most freq words: ['?', 'the', 'what', 'is', 'i', 'how', 'to', 'in', 'a', 'do', 'of', 'are', 'and', 'can', 'for', 'you', 'why', 'my', 'it', 'best']\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size:', len(word2index))\n",
    "print('Tokens count:', sum(len(tokens) for tokens in tokenized_texts))\n",
    "print('Unknown tokens appeared:', sum(1 for tokens in tokenized_texts for token in tokens if token not in word2index))\n",
    "print('Most freq words:', index2word[1:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_contexts(tokenized_texts, window_size):\n",
    "    contexts = []\n",
    "    for tokens in tokenized_texts:\n",
    "        for i in range(len(tokens)):\n",
    "            central_word = tokens[i]\n",
    "            context = [tokens[i + delta] for delta in range(-window_size, window_size + 1) \n",
    "                       if delta != 0 and i + delta >= 0 and i + delta < len(tokens)]\n",
    "\n",
    "            contexts.append((central_word, context))\n",
    "            \n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', ['is', 'the']),\n",
       " ('is', ['what', 'the', 'step']),\n",
       " ('the', ['what', 'is', 'step', 'by']),\n",
       " ('step', ['is', 'the', 'by', 'step']),\n",
       " ('by', ['the', 'step', 'step', 'guide'])]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = build_contexts(tokenized_texts, window_size=2)\n",
    "contexts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, [4, 2]),\n",
       " (4, [3, 2, 511]),\n",
       " (2, [3, 4, 511, 62]),\n",
       " (511, [4, 2, 62, 511]),\n",
       " (62, [2, 511, 511, 0])]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = [(word2index.get(central_word, 0), [word2index.get(word, 0) for word in context]) \n",
    "            for central_word, context in contexts]\n",
    "contexts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_skip_gram_batchs_iter(contexts, window_size, num_skips, batch_size):\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * window_size\n",
    "    \n",
    "    central_words = [word for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
    "    contexts = [context for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
    "    \n",
    "    batch_size = int(batch_size / num_skips)\n",
    "    batchs_count = int(math.ceil(len(contexts) / batch_size))\n",
    "    \n",
    "    print('Initializing batchs generator with {} batchs per epoch'.format(batchs_count))\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(len(contexts))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(batchs_count):\n",
    "            batch_begin, batch_end = i * batch_size, min((i + 1) * batch_size, len(contexts))\n",
    "            batch_indices = indices[batch_begin: batch_end]\n",
    "\n",
    "            batch_data, batch_labels = [], []\n",
    "\n",
    "            for data_ind in batch_indices:\n",
    "                central_word, context = central_words[data_ind], contexts[data_ind]\n",
    "                \n",
    "                words_to_use = random.sample(context, num_skips)\n",
    "                batch_data.extend([central_word] * num_skips)\n",
    "                batch_labels.extend(words_to_use)\n",
    "            \n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing batchs generator with 2626 batchs per epoch\n"
     ]
    }
   ],
   "source": [
    "batch, labels = next(make_skip_gram_batchs_iter(contexts, window_size=2, num_skips=2, batch_size=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,    2, 1419, 1419, 1183, 1183,   41,   41,   11,   11,    8,    8,\n",
       "         661,  661,   28,   28,  380,  380,   99,   99,   61,   61,    1,    1,\n",
       "          53,   53,  463,  463,    1,    1,  488,  488])"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.LongTensor(batch)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 315,    3,    0,    9,  417,    0,    0,  426,  704,   11,  161,  541,\n",
       "           0,  456,   42,    9,    4,   31,    0,    2,  117,    0,    9, 1190,\n",
       "         136,   18,   13, 1561,   17, 1546,   73,  789])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.LongTensor(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 32\n",
    "VOCAB_SIZE = len(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6DXjZS3JyQZh"
   },
   "source": [
    "#### Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Embedding(VOCAB_SIZE, 32), nn.Linear(32, VOCAB_SIZE))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "    \n",
    "model = SkipGram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uMsuvEP90svi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing batchs generator with 1313 batchs per epoch\n",
      "Step = 1000, Avg Loss = 5.5083, Time = 8.91s\n",
      "Step = 2000, Avg Loss = 5.0394, Time = 8.38s\n",
      "Step = 3000, Avg Loss = 4.9165, Time = 7.70s\n",
      "Step = 4000, Avg Loss = 4.8583, Time = 8.71s\n",
      "Step = 5000, Avg Loss = 4.7689, Time = 8.51s\n",
      "Step = 6000, Avg Loss = 4.7246, Time = 8.85s\n",
      "Step = 7000, Avg Loss = 4.7054, Time = 11.46s\n",
      "Step = 8000, Avg Loss = 4.7047, Time = 14.48s\n",
      "Step = 9000, Avg Loss = 4.6684, Time = 12.16s\n",
      "Step = 10000, Avg Loss = 4.6527, Time = 9.94s\n"
     ]
    }
   ],
   "source": [
    "loss_every_nsteps = 1000\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "stop = 0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "for step, (batch, labels) in enumerate(make_skip_gram_batchs_iter(contexts, window_size=2, num_skips=4, batch_size=128)):\n",
    "    batch = torch.LongTensor(batch)\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(batch)\n",
    "    \n",
    "    loss = criterion(output, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if step != 0 and step % loss_every_nsteps == 0:\n",
    "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
    "                                                                    time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "        stop += 1\n",
    "    if stop == 10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pqq9kee41L4P"
   },
   "source": [
    "#### Analysis\n",
    "\n",
    "You can get embeddings by casting this spell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uWsYkNn-Hnl_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Embedding(1628, 32)\n",
      "  (1): Linear(in_features=32, out_features=1628, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for child in model.children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.weight : torch.Size([1628, 32])\n",
      "model.1.weight : torch.Size([1628, 32])\n",
      "model.1.bias : torch.Size([1628])\n"
     ]
    }
   ],
   "source": [
    "for name,parameters in model.named_parameters():\n",
    "    print(name,':',parameters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what a parameter looks like - \n",
      " Parameter containing:\n",
      "tensor([[-0.7005, -0.3949, -1.0316,  ...,  1.3106,  0.8832, -0.4425],\n",
      "        [ 0.1537, -0.6839,  0.0646,  ..., -0.1717, -0.2411,  0.0848],\n",
      "        [-0.6529,  0.5762, -0.3311,  ..., -0.2396, -0.1716, -0.1415],\n",
      "        ...,\n",
      "        [ 0.3938, -0.8552,  0.4637,  ..., -0.0057, -0.5812,  0.3205],\n",
      "        [-0.8397,  0.7625,  0.6288,  ...,  0.9212,  0.1511, -0.1192],\n",
      "        [-1.3223, -2.1495,  1.1332,  ...,  1.4895,  0.7985,  1.5277]],\n",
      "       requires_grad=True)\n",
      "This is what a parameter looks like - \n",
      " Parameter containing:\n",
      "tensor([[ 2.3062e-02, -1.3275e-01,  6.2402e-02,  ...,  2.8661e-02,\n",
      "         -9.2706e-03,  1.3411e-02],\n",
      "        [-8.6404e-04, -3.5879e-01,  4.5102e-01,  ..., -4.2325e-02,\n",
      "          3.7672e-02,  6.7543e-02],\n",
      "        [ 1.3683e-01, -3.8622e-01,  4.2439e-01,  ...,  1.0466e-01,\n",
      "         -1.9804e-02,  4.5757e-01],\n",
      "        ...,\n",
      "        [ 2.7370e-01,  6.5045e-01,  1.8759e-01,  ..., -9.5559e-01,\n",
      "          7.1552e-01, -1.8907e-01],\n",
      "        [ 1.5212e-01,  1.1585e-02,  5.4043e-01,  ..., -5.5408e-01,\n",
      "          1.9455e-01, -1.1954e+00],\n",
      "        [ 7.8534e-01,  7.2149e-02, -4.9281e-01,  ..., -1.2570e-01,\n",
      "          1.5372e+00, -1.0358e+00]], requires_grad=True)\n",
      "This is what a parameter looks like - \n",
      " Parameter containing:\n",
      "tensor([ 5.9312,  4.3697,  3.9160,  ..., -2.2772, -2.3076, -1.8473],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for child in model.children():\n",
    "    for param in child.parameters():\n",
    "        print(\"This is what a parameter looks like - \\n\",param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.model[0].weight.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZtxY2D01RB6"
   },
   "source": [
    "Check whether it turned out at least somehow adequately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bhDwuhDSHEDm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best',\n",
       " 'easiest',\n",
       " 'funniest',\n",
       " 'worst',\n",
       " 'review',\n",
       " 'downloading',\n",
       " 'most',\n",
       " 'fastest',\n",
       " 'hardest',\n",
       " 'favorite']"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def most_similar(embeddings, index2word, word2index, word):\n",
    "    word_emb = embeddings[word2index[word]]\n",
    "    \n",
    "    similarities = cosine_similarity([word_emb], embeddings)[0]\n",
    "    top10 = np.argsort(similarities)[-10:]\n",
    "    \n",
    "    return [index2word[index] for index in reversed(top10)]\n",
    "\n",
    "most_similar(embeddings, index2word, word2index, 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VS1x-mO1WKS"
   },
   "source": [
    "And visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuXv2HxsAecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 1000 samples in 0.007s...\n",
      "[t-SNE] Computed neighbors for 1000 samples in 0.100s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1000\n",
      "[t-SNE] Mean sigma: 1.360045\n",
      "[t-SNE] Computed conditional probabilities in 0.039s\n",
      "[t-SNE] Iteration 50: error = 83.9013748, gradient norm = 0.2899156 (50 iterations in 7.337s)\n",
      "[t-SNE] Iteration 100: error = 84.1195221, gradient norm = 0.3060217 (50 iterations in 7.885s)\n",
      "[t-SNE] Iteration 150: error = 84.7624664, gradient norm = 0.3165305 (50 iterations in 8.592s)\n",
      "[t-SNE] Iteration 200: error = 86.3966522, gradient norm = 0.2803719 (50 iterations in 9.326s)\n",
      "[t-SNE] Iteration 250: error = 86.0357513, gradient norm = 0.2777553 (50 iterations in 8.905s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 86.035751\n",
      "[t-SNE] Iteration 300: error = 2.5670052, gradient norm = 0.0045316 (50 iterations in 6.542s)\n",
      "[t-SNE] Iteration 350: error = 2.4186151, gradient norm = 0.0014197 (50 iterations in 5.201s)\n",
      "[t-SNE] Iteration 400: error = 2.3853025, gradient norm = 0.0004714 (50 iterations in 4.373s)\n",
      "[t-SNE] Iteration 450: error = 2.3710132, gradient norm = 0.0004297 (50 iterations in 4.597s)\n",
      "[t-SNE] Iteration 500: error = 2.3594692, gradient norm = 0.0003509 (50 iterations in 6.302s)\n",
      "[t-SNE] Iteration 550: error = 2.3538992, gradient norm = 0.0002175 (50 iterations in 5.476s)\n",
      "[t-SNE] Iteration 600: error = 2.3495955, gradient norm = 0.0001566 (50 iterations in 6.137s)\n",
      "[t-SNE] Iteration 650: error = 2.3464611, gradient norm = 0.0001740 (50 iterations in 5.808s)\n",
      "[t-SNE] Iteration 700: error = 2.3412797, gradient norm = 0.0001848 (50 iterations in 4.966s)\n",
      "[t-SNE] Iteration 750: error = 2.3384447, gradient norm = 0.0002185 (50 iterations in 4.985s)\n",
      "[t-SNE] Iteration 800: error = 2.3356655, gradient norm = 0.0001179 (50 iterations in 5.256s)\n",
      "[t-SNE] Iteration 850: error = 2.3342884, gradient norm = 0.0001362 (50 iterations in 4.906s)\n",
      "[t-SNE] Iteration 900: error = 2.3323917, gradient norm = 0.0001332 (50 iterations in 5.122s)\n",
      "[t-SNE] Iteration 950: error = 2.3301065, gradient norm = 0.0003217 (50 iterations in 4.863s)\n",
      "[t-SNE] Iteration 1000: error = 2.3291748, gradient norm = 0.0001343 (50 iterations in 5.006s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 2.329175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/newscred/.local/share/virtualenvs/NeuralNetwork-Db90wnYD/lib/python3.7/site-packages/sklearn/preprocessing/data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"07020903-ae7c-4af8-904e-1c4e3b9ec206\" data-root-id=\"1003\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"3a17b3b2-46ac-4137-a334-8d4d8cbb16be\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1012\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"1017\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"renderers\":[{\"id\":\"1012\",\"type\":\"LinearAxis\"},{\"id\":\"1016\",\"type\":\"Grid\"},{\"id\":\"1017\",\"type\":\"LinearAxis\"},{\"id\":\"1021\",\"type\":\"Grid\"},{\"id\":\"1030\",\"type\":\"BoxAnnotation\"},{\"id\":\"1040\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1044\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1028\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1004\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1008\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1006\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1010\",\"type\":\"LinearScale\"}},\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"overlay\":{\"id\":\"1030\",\"type\":\"BoxAnnotation\"}},\"id\":\"1024\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1025\",\"type\":\"SaveTool\"},{\"attributes\":{\"callback\":null,\"data\":{\"color\":[\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\"],\"token\":[\"?\",\"the\",\"what\",\"is\",\"i\",\"how\",\"to\",\"in\",\"a\",\"do\",\"of\",\"are\",\"and\",\"can\",\"for\",\"you\",\"why\",\"my\",\"it\",\"best\",\".\",\"on\",\"does\",\"be\",\"or\",\"if\",\"s\",\"have\",\"that\",\"with\",\"which\",\"should\",\"some\",\"get\",\"from\",\"an\",\"your\",\"india\",\"will\",\"when\",\"people\",\"good\",\"t\",\"there\",\"like\",\"would\",\"at\",\"who\",\"we\",\"one\",\"between\",\"as\",\"me\",\"not\",\"any\",\"quora\",\"about\",\"most\",\"did\",\"so\",\"after\",\"by\",\"life\",\"where\",\"they\",\"make\",\"time\",\"was\",\"way\",\"much\",\"think\",\"learn\",\"new\",\"am\",\"this\",\"has\",\"ever\",\"difference\",\"all\",\"use\",\"many\",\"work\",\"trump\",\"world\",\"find\",\"money\",\"know\",\"someone\",\"their\",\"start\",\"other\",\"up\",\"indian\",\"his\",\"more\",\"job\",\"but\",\"first\",\"better\",\"take\",\"year\",\"us\",\"m\",\"become\",\"without\",\"her\",\"phone\",\"don\",\"mean\",\"he\",\"than\",\"questions\",\"want\",\"online\",\"feel\",\"donald\",\"day\",\"being\",\"them\",\"black\",\"math\",\"english\",\"account\",\"war\",\"no\",\"stop\",\"possible\",\"buy\",\"old\",\"love\",\"computer\",\"could\",\"person\",\"into\",\"language\",\"question\",\"engineering\",\"girl\",\"google\",\"out\",\"different\",\"using\",\"company\",\"programming\",\"c\",\"notes\",\"prepare\",\"ways\",\"need\",\"still\",\"were\",\"before\",\"free\",\"ask\",\"long\",\"see\",\"things\",\"real\",\"improve\",\"number\",\"country\",\"now\",\"x\",\"years\",\"movies\",\"only\",\"weight\",\"go\",\"rs\",\"compare\",\"sex\",\"government\",\"under\",\"really\",\"over\",\"mobile\",\"help\",\"happen\",\"been\",\"thing\",\"just\",\"going\",\"used\",\"cost\",\"android\",\"college\",\"name\",\"books\",\"movie\",\"book\",\"lose\",\"iphone\",\"same\",\"through\",\"she\",\"app\",\"school\",\"increase\",\"friends\",\"during\",\"back\",\"data\",\"win\",\"student\",\"th\",\"our\",\"live\",\"exam\",\"car\",\"while\",\"girls\",\"white\",\"travel\",\"its\",\"come\",\"d\",\"watch\",\"important\",\"him\",\"change\",\"examples\",\"friend\",\"learning\",\"system\",\"death\",\"study\",\"having\",\"place\",\"ve\",\"made\",\"high\",\"laptop\",\"pakistan\",\"non\",\"business\",\"top\",\"had\",\"science\",\"card\",\"give\",\"month\",\"instagram\",\"right\",\"usa\",\"music\",\"hillary\",\"modi\",\"guy\",\"website\",\"next\",\"b\",\"password\",\"energy\",\"china\",\"then\",\"affect\",\"common\",\"bad\",\"video\",\"own\",\"look\",\"interview\",\"off\",\"class\",\"done\",\"election\",\"university\",\"facebook\",\"web\",\"email\",\"k\",\"causes\",\"cause\",\"pay\",\"two\",\"working\",\"chinese\",\"differences\",\"tell\",\"software\",\"president\",\"jee\",\"water\",\"say\",\"bank\",\"e\",\"future\",\"each\",\"clinton\",\"earn\",\"create\",\"interesting\",\"getting\",\"believe\",\"average\",\"home\",\"kind\",\"read\",\"process\",\"career\",\"java\",\"salary\",\"major\",\"men\",\"answer\",\"tv\",\"doesn\",\"visit\",\"big\",\"power\",\"purpose\",\"jobs\",\"history\",\"die\",\"word\",\"months\",\".com\",\"send\",\"value\",\"u\",\"internet\",\"very\",\"hack\",\"review\",\"state\",\"keep\",\"god\",\"score\",\"even\",\"safe\",\"engineer\",\".s\",\"every\",\"too\",\"universities\",\"songs\",\"food\",\"women\",\"age\",\"gate\",\"countries\",\"write\",\"true\",\"graduate\",\"apply\",\"delete\",\"experience\",\"run\",\"happens\",\"play\",\"term\",\"plan\",\"differ\",\"meaning\",\"course\",\"relationship\",\"guys\",\"g\",\"whatsapp\",\"behind\",\"sydney\",\"story\",\"digital\",\"marketing\",\"girlfriend\",\"boyfriend\",\"makes\",\"majors\",\"looking\",\"gmail\",\"family\",\"support\",\"recover\",\"level\",\"height\",\"these\",\"correct\",\"courses\",\"hate\",\"series\",\"companies\",\"eat\",\"deal\",\"song\",\"education\",\"mechanical\",\"speed\",\"youtube\",\"canada\",\"skills\",\"preparation\",\"open\",\"effects\",\"part\",\"national\",\"civil\",\"universe\",\"against\",\"days\",\"must\",\"great\",\"porn\",\"test\",\"doing\",\"available\",\"pass\",\"development\",\"light\",\"improvement\",\"never\",\"windows\",\"ca\",\"law\",\"game\",\"views\",\"choose\",\"products\",\"always\",\"tax\",\"germany\",\"join\",\"heard\",\"exist\",\"hair\",\"anyone\",\"follow\",\"startup\",\"given\",\"exactly\",\"pros\",\"cons\",\"apps\",\"considered\",\"reason\",\"earth\",\"control\",\"myself\",\"healthy\",\"fall\",\"culture\",\"iq\",\"side\",\"both\",\"answers\",\"based\",\"main\",\"american\",\"man\",\"blog\",\"market\",\"international\",\"students\",\"united\",\"speak\",\"city\",\"matter\",\"advice\",\"private\",\"presidential\",\"code\",\"another\",\"drive\",\"quality\",\"end\",\"effect\",\"post\",\"design\",\"show\",\"popular\",\"uk\",\"order\",\"rupee\",\"places\",\"network\",\"type\",\"terms\",\"uber\",\"physics\",\"easy\",\"f\",\"states\",\"political\",\"oil\",\"early\",\"search\",\"mind\",\"humans\",\"employees\",\"got\",\"apple\",\"such\",\"created\",\"dream\",\"gold\",\"low\",\"narendra\",\"house\",\"decision\",\"seo\",\"per\",\"games\",\"bollywood\",\"today\",\"option\",\"reduce\",\"rid\",\"last\",\"benefits\",\"public\",\"cbse\",\"step\",\"invest\",\"share\",\"etc\",\"download\",\"gain\",\"also\",\"institute\",\"product\",\"access\",\"late\",\"list\",\"period\",\"marks\",\"body\",\"move\",\"facts\",\"cell\",\"problem\",\"daily\",\"date\",\"develop\",\"children\",\"bangalore\",\"won\",\"view\",\"space\",\"happened\",\"messages\",\"size\",\"often\",\"making\",\"application\",\"around\",\"european\",\"lost\",\"income\",\"successful\",\"site\",\"chances\",\"source\",\"kerala\",\"small\",\"hyderabad\",\"security\",\"supporters\",\"others\",\"infinite\",\"cities\",\"something\",\"near\",\"general\",\"useful\",\"drug\",\"biggest\",\"compared\",\"sell\",\"media\",\"theory\",\"police\",\"hard\",\"whole\",\"training\",\"service\",\"cheap\",\"masturbation\",\"care\",\"army\",\"currency\",\"easily\",\"asked\",\"credit\",\"living\",\"legal\",\"center\",\"mumbai\",\"investment\",\"industry\",\"microsoft\",\"research\",\"delhi\",\"human\",\"advanced\",\"indians\",\"favorite\",\"call\",\"n\",\"green\",\"current\",\"tips\",\"worth\",\"recruit\",\"star\",\"female\",\"eye\",\"happy\",\"technology\",\"taking\",\"machine\",\"websites\",\"says\",\"options\",\"fast\",\"ex\",\"dogs\",\"iit\",\"japanese\",\"ms\",\"build\",\"chance\",\"program\",\"tourist\",\"economy\",\"minimum\",\"information\",\"required\",\"words\",\"necessary\",\"planning\",\"issue\",\"amount\",\"parents\",\"seen\",\"price\",\"woman\",\"area\",\"personal\",\"coaching\",\"degree\",\"worst\",\"set\",\"text\",\"well\",\"address\",\"visa\",\"presidency\",\"master\",\"final\",\"because\",\"try\",\"away\",\"mail\",\"grads\",\"male\",\"child\",\"office\",\"field\",\"gdp\",\"stock\",\"estate\",\"gpa\",\"avoid\",\"japan\",\"films\",\"function\",\"western\",\"hit\",\"tea\",\"those\",\"net\",\"factors\",\"similar\",\"file\",\"macbook\",\"character\",\"three\",\"add\",\"solar\",\"turn\",\"left\",\"cars\",\"re\",\"turkey\",\"mexico\",\"changed\",\"teacher\",\"intelligence\",\"doctor\",\"sentence\",\"actually\",\"wake\",\"sleep\",\"teach\",\"moment\",\"middle\",\"australia\",\"mother\",\"thoughts\",\"snapchat\",\"model\",\"transgender\",\"expanding\",\"nt\",\"point\",\"note\",\"global\",\"obama\",\"remember\",\"database\",\"else\",\"faster\",\"inr\",\"social\",\"pictures\",\"marked\",\"needing\",\"strongest\",\"forward\",\"prefer\",\"deleted\",\"wifi\",\"significance\",\"able\",\"enough\",\"short\",\"core\",\"rank\",\"idea\",\"traffic\",\"yourself\",\"coming\",\"sim\",\"birth\",\"meet\",\"blocked\",\"ios\",\"letter\",\"across\",\"said\",\"services\",\"amazon\",\"force\",\"page\",\"pro\",\"languages\",\"writing\",\"single\",\"blood\",\"systems\",\"british\",\"opinion\",\"multiple\",\"neet\",\"mba\",\"banking\",\"quickly\",\"caste\",\"plant\",\"cat\",\"schools\",\"heat\",\"everyday\",\"comment\",\"yes\",\"past\",\"alcohol\",\"python\",\"south\",\"didn\",\"rate\",\"pune\",\"anything\",\"flight\",\"board\",\"penis\",\"solve\",\"laws\",\"hp\",\"society\",\"sports\",\"few\",\"dark\",\"types\",\"muslims\",\"block\",\"budget\",\"head\",\"currently\",\"offer\",\"afraid\",\"may\",\"phones\",\"wife\",\"shows\",\"rupees\",\"easiest\",\"colleges\",\"growth\",\"walking\",\"cs\",\"smart\",\"developing\",\"strong\",\"haven\",\"banning\",\"beer\",\"understand\",\"break\",\"everything\",\"dog\",\"cse\",\"fake\",\"shopping\",\"boy\",\"overcome\",\"drink\",\"feelings\",\"spouse\",\"attractions\",\"vs\",\"nothing\",\"california\",\"treat\",\"demonetization\",\"hacks\",\"eating\",\"favourite\",\"anime\",\"stay\",\"related\",\"ban\",\"medical\",\"tamil\",\"clear\",\"evidence\",\"jesus\",\"reading\",\"gre\",\"studying\",\"likes\",\"picture\",\"leave\",\"second\",\"scope\",\"dollars\",\"algorithms\",\"found\",\"names\",\"happiness\",\"rehab\",\"county\",\"close\",\"bigger\",\"mains\",\"within\",\"talk\",\"acne\",\"equation\",\"air\",\"sales\",\"property\",\"effective\",\"pakistani\",\"prime\",\"marriage\",\"crack\",\"drawing\",\"brand\",\"sqrt\",\"sun\",\"instead\",\"sexual\",\"users\",\"numbers\",\"night\",\"likely\",\"whether\",\"regular\",\"normal\",\"harry\",\"wants\",\"management\",\"attack\",\"union\",\"powerful\",\"pain\",\"applying\",\"electrical\",\"register\",\"hole\",\"mass\",\"talking\",\"least\",\"hours\",\"red\",\"joke\",\"london\",\"hindu\",\"spotify\",\"animals\",\"battle\",\"contrast\",\"batman\",\"channels\",\"install\",\"engineers\",\"promises\",\"whom\",\"jio\",\"trust\",\"sent\",\"german\",\"smoking\",\"message\",\"masters\",\"among\",\"khan\",\"self\",\"cricket\",\"inside\",\"imported\",\"iran\",\"platform\",\"hand\",\"wrong\",\"less\",\"starting\",\"chemical\",\"iii\",\"atm\",\"mutual\",\"thinking\",\"kashmir\",\"north\",\"somebody\",\"payment\",\"gadgets\",\"role\",\"hacker\",\"mathematics\",\"exams\",\"america\",\"humanity\",\"feeling\",\"weather\",\"experiences\",\"muslim\",\"summer\",\"since\",\"completing\",\"million\",\"attractive\",\"health\",\"suicide\",\"convert\",\"remove\",\"completely\",\"hindi\",\"pan\",\"wear\",\"driving\",\"inpatient\",\"please\",\"applications\",\"told\",\"complete\",\"anti\",\"might\",\"korea\",\"skin\",\"destination\",\"foreign\",\"times\",\"let\",\"sign\",\"ip\",\"pok\",\"mon\",\"nuclear\",\"potential\",\"june\",\"r\",\"balance\",\"paid\",\"special\",\"gets\",\"thrones\",\"upsc\",\"expect\",\"followers\",\"nd\",\"gst\",\"french\",\"content\",\"potter\",\"files\",\"procedure\",\"manager\",\"higher\",\"charge\"],\"x\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"shape\":[1000]},\"y\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"shape\":[1000]}},\"selected\":{\"id\":\"1052\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1051\",\"type\":\"UnionRenderers\"}},\"id\":\"1002\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1027\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"LinearScale\"},{\"attributes\":{\"callback\":null},\"id\":\"1004\",\"type\":\"DataRange1d\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":{\"id\":\"1023\",\"type\":\"WheelZoomTool\"},\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1022\",\"type\":\"PanTool\"},{\"id\":\"1023\",\"type\":\"WheelZoomTool\"},{\"id\":\"1024\",\"type\":\"BoxZoomTool\"},{\"id\":\"1025\",\"type\":\"SaveTool\"},{\"id\":\"1026\",\"type\":\"ResetTool\"},{\"id\":\"1027\",\"type\":\"HelpTool\"},{\"id\":\"1042\",\"type\":\"HoverTool\"}]},\"id\":\"1028\",\"type\":\"Toolbar\"},{\"attributes\":{\"data_source\":{\"id\":\"1002\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1038\",\"type\":\"Scatter\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1039\",\"type\":\"Scatter\"},\"selection_glyph\":null,\"view\":{\"id\":\"1041\",\"type\":\"CDSView\"}},\"id\":\"1040\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1030\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"source\":{\"id\":\"1002\",\"type\":\"ColumnDataSource\"}},\"id\":\"1041\",\"type\":\"CDSView\"},{\"attributes\":{\"formatter\":{\"id\":\"1046\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1013\",\"type\":\"BasicTicker\"}},\"id\":\"1012\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"token\",\"@token\"]]},\"id\":\"1042\",\"type\":\"HoverTool\"},{\"attributes\":{\"callback\":null},\"id\":\"1006\",\"type\":\"DataRange1d\"},{\"attributes\":{\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1013\",\"type\":\"BasicTicker\"}},\"id\":\"1016\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1008\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"BasicTicker\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"1044\",\"type\":\"Title\"},{\"attributes\":{\"formatter\":{\"id\":\"1048\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1018\",\"type\":\"BasicTicker\"}},\"id\":\"1017\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1046\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1018\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1048\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1018\",\"type\":\"BasicTicker\"}},\"id\":\"1021\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1051\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1039\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1052\",\"type\":\"Selection\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.25},\"fill_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0.25},\"line_color\":{\"field\":\"color\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1038\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1022\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"WheelZoomTool\"}],\"root_ids\":[\"1003\"]},\"title\":\"Bokeh Application\",\"version\":\"1.0.4\"}};\n",
       "  var render_items = [{\"docid\":\"3a17b3b2-46ac-4137-a334-8d4d8cbb16be\",\"roots\":{\"1003\":\"07020903-ae7c-4af8-904e-1c4e3b9ec206\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1003"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    output_notebook()\n",
    "    \n",
    "    if isinstance(color, str): \n",
    "        color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: \n",
    "        pl.show(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def get_tsne_projection(word_vectors):\n",
    "    tsne = TSNE(n_components=2, verbose=100)\n",
    "    return scale(tsne.fit_transform(word_vectors))\n",
    "    \n",
    "    \n",
    "def visualize_embeddings(embeddings, index2word, word_count):\n",
    "    word_vectors = embeddings[1: word_count + 1]\n",
    "    words = index2word[1: word_count + 1]\n",
    "    \n",
    "    word_tsne = get_tsne_projection(word_vectors)\n",
    "    draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)\n",
    "    \n",
    "    \n",
    "visualize_embeddings(embeddings, index2word, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Language Modeling\n",
    "\n",
    "Recall that in an n-gram language model, given a sequence of words\n",
    "$w$, we want to compute\n",
    "\n",
    "\\begin{align}P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )\\end{align}\n",
    "\n",
    "Where $w_i$ is the ith word of the sequence.\n",
    "\n",
    "In this example, we will compute the loss function on some training\n",
    "examples and update the parameters with backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[518.1398809110159, 515.6318509353883, 513.1396400811631, 510.66284127663016, 508.19781289771703, 505.7445230296734, 503.3024996208805, 500.8712962733133, 498.4513797301769, 496.0399662879884]\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XGfhLR6x8D3r"
   },
   "source": [
    "### Continuous Bag of Words (CBoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UuVr2IsaYhX"
   },
   "source": [
    "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep\n",
    "learning. It is a model that tries to predict words given the context of\n",
    "a few words before and a few words after the target word. This is\n",
    "distinct from language modeling, since CBOW is not sequential and does\n",
    "not have to be probabilistic. Typcially, CBOW is used to quickly train\n",
    "word embeddings, and these embeddings are used to initialize the\n",
    "embeddings of some more complicated model. Usually, this is referred to\n",
    "as *pretraining embeddings*. It almost always helps performance a couple\n",
    "of percent.\n",
    "\n",
    "The CBOW model is as follows. Given a target word $w_i$ and an\n",
    "$N$ context window on each side, $w_{i-1}, \\dots, w_{i-N}$\n",
    "and $w_{i+1}, \\dots, w_{i+N}$, referring to all context words\n",
    "collectively as $C$, CBOW tries to minimize\n",
    "\n",
    "\\begin{align}-\\log p(w_i | C) = -\\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b)\\end{align}\n",
    "\n",
    "where $q_w$ is the embedding for word $w$.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/CBOW.png\" width=\"50%\">\n",
    "\n",
    "Implement this model in Pytorch by filling in the class below. Some tips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NP5VmnnjtsXn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "contexts = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    contexts.append((context, target))\n",
    "\n",
    "print(contexts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 39,  4, 37])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "make_context_vector(contexts[0][0], word_to_ix)  # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cbow_batchs_iter(contexts, window_size, batch_size):\n",
    "    data = np.array([context for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
    "    labels = np.array([word for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
    "        \n",
    "    batchs_count = int(math.ceil(len(data) / batch_size))\n",
    "    \n",
    "    print('Initializing batchs generator with {} batchs per epoch'.format(batchs_count))\n",
    "    \n",
    "    print(data)\n",
    "    print(labels)\n",
    "    \n",
    "#     while True:\n",
    "#         indices = np.arange(len(contexts))\n",
    "#         np.random.shuffle(indices)\n",
    "\n",
    "#         for i in range(batchs_count):\n",
    "#             batch_begin, batch_end = i * batch_size, min((i + 1) * batch_size, len(contexts))\n",
    "#             batch_indices = indices[batch_begin: batch_end]\n",
    "\n",
    "#             batch_data, batch_labels = [], []\n",
    "\n",
    "#             for data_ind in batch_indices:\n",
    "#                 central_word, context = central_words[data_ind], contexts[data_ind]\n",
    "                \n",
    "#                 words_to_use = random.sample(contex] num_skips)\n",
    "#                 batch_data.extend([central_word] * num_skips)\n",
    "#                 batch_labels.extend(words_to_use)\n",
    "            \n",
    "#             yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing batchs generator with 1 batchs per epoch\n",
      "['idea' 'that' 'they' 'with']\n",
      "[['study' 'the' 'of' 'a']\n",
      " ['abstract' 'beings' 'inhabit' 'computers.']\n",
      " ['computers.' 'As' 'evolve,' 'processes']\n",
      " ['the' 'computer' 'our' 'spells.']]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'central_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-448-beb982a7cbea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_cbow_batchs_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-446-0217980814fa>\u001b[0m in \u001b[0;36mmake_cbow_batchs_iter\u001b[0;34m(contexts, window_size, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdata_ind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mcentral_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcentral_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mwords_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_skips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'central_words' is not defined"
     ]
    }
   ],
   "source": [
    "next(make_cbow_batchs_iter(contexts, window_size=2, batch_size=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing batchs generator with 1 batchs per epoch\n"
     ]
    }
   ],
   "source": [
    "batch, labels = next(make_cbow_batchs_iter(contexts, window_size=2, batch_size=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19,\n",
       " 19,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 600,\n",
       " 600,\n",
       " 13,\n",
       " 13,\n",
       " 149,\n",
       " 149,\n",
       " 179,\n",
       " 179,\n",
       " 11,\n",
       " 11,\n",
       " 517,\n",
       " 517,\n",
       " 807,\n",
       " 807,\n",
       " 2,\n",
       " 2,\n",
       " 16,\n",
       " 16,\n",
       " 1109,\n",
       " 1109,\n",
       " 8,\n",
       " 8,\n",
       " 493,\n",
       " 493]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mHKLbMwx4c5"
   },
   "outputs": [],
   "source": [
    "class CBoWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBoWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output = self.embeddings(inputs)\n",
    "        output = self.out_layer(output)\n",
    "        return output\n",
    "      \n",
    "\n",
    "model = CBoWModel(VOCAB_SIZE, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xiIgaofEyyJ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing batchs generator with 1313 batchs per epoch\n",
      "Step = 1000, Avg Loss = 5.4950, Time = 8.14s\n",
      "Step = 2000, Avg Loss = 5.0360, Time = 8.18s\n",
      "Step = 3000, Avg Loss = 4.9155, Time = 7.52s\n",
      "Step = 4000, Avg Loss = 4.8623, Time = 7.57s\n",
      "Step = 5000, Avg Loss = 4.7669, Time = 7.86s\n",
      "Step = 6000, Avg Loss = 4.7298, Time = 8.20s\n",
      "Step = 7000, Avg Loss = 4.7059, Time = 8.20s\n",
      "Step = 8000, Avg Loss = 4.7011, Time = 8.24s\n",
      "Step = 9000, Avg Loss = 4.6778, Time = 8.11s\n",
      "Step = 10000, Avg Loss = 4.6427, Time = 8.73s\n"
     ]
    }
   ],
   "source": [
    "loss_every_nsteps = 1000\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "stop = 0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "for step, (batch, labels) in enumerate(make_skip_gram_batchs_iter(contexts, window_size=2, num_skips=4, batch_size=128)):\n",
    "    batch = torch.LongTensor(batch)\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(batch)\n",
    "    \n",
    "    loss = criterion(output, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if step != 0 and step % loss_every_nsteps == 0:\n",
    "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
    "                                                                    time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "        stop += 1\n",
    "    if stop == 10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CON4VOyG3iET"
   },
   "source": [
    "### Negative Sampling\n",
    "\n",
    "What is the hardest thing now? Calculating softmax and applying gradients to all words in $ V $.\n",
    "\n",
    "One way to handle this is to use * Negative Sampling *.\n",
    "\n",
    "In fact, instead of predicting the index of a word by context, it is predicted that such a word $ w $ can be in this context $ c $: $ P (D = 1 | w, c) $.\n",
    "\n",
    "You can use a regular sigmoid to get this probability:\n",
    "$$ P (D = 1 | w, c) = \\sigma (v_w ^ T u_c) = \\frac 1 {1 + \\exp (-v ^ T_w u_c)}. $$\n",
    "\n",
    "The learning process then looks like this: for each pair, the word and its context generate a set of negative examples:\n",
    "<img src=\"https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/Negative_Sampling.png\" width=\"50%\">\n",
    "\n",
    "For CBoW, the loss function will look like this:\n",
    "\n",
    "$$ - \\log \\sigma (v_c ^ T u_c) - \\sum_ {k = 1} ^ K \\log \\sigma (- \\tilde v_k ^ T u_c), $$\n",
    "\n",
    "where $ v_c $ is the vector of the central word, $ u_c $ is the context vector (sum of context vectors), $ \\tilde v_1, \\ldots, \\tilde v_K $ are the sampled negative examples.\n",
    "\n",
    "Compare this formula with the usual CBoW:\n",
    "$$ - v_c ^ T u_c + \\log \\sum_ {i = 1} ^ {| V |} \\exp (v_i ^ T u_c). $$\n",
    "\n",
    "Usually words are sampled from $ U ^ {3/4} $, where $ U $ is the unigram distribution, that is, the frequency of occurrence of words divided by the total number of words.\n",
    "\n",
    "Frequencies we have already considered: they are obtained in `Counter (words)`. Simply convert them to probabilities and multiply these probabilities by $ \\frac 3 4 $. Why $ \\frac 3 4 $? Some intuition can be found in the following example:\n",
    "\n",
    "$$P(\\text{is}) = 0.9, \\ P(\\text{is})^{3/4} = 0.92$$\n",
    "$$P(\\text{Constitution}) = 0.09, \\ P(\\text{Constitution})^{3/4} = 0.16$$\n",
    "$$P(\\text{bombastic}) = 0.01, \\ P(\\text{bombastic})^{3/4} = 0.032$$\n",
    "\n",
    "The probability for high-frequency words is not particularly increased (relatively), but low-frequency ones will fall out with a noticeably greater probability.\n",
    "\n",
    "**Task** Implement your Negative Sampling.\n",
    "\n",
    "First, let's set the distribution for sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zcX4vRBLlXy6"
   },
   "outputs": [],
   "source": [
    "words_sum_count = sum(words_counter.values())\n",
    "word_distribution = np.array([(words_counter[word] / words_sum_count) ** (3 / 4) for word in index2word])\n",
    "# Ð’Ð¾Ð¾Ð±Ñ‰Ðµ-Ñ‚Ð¾, Ñ‚ÑƒÑ‚ Ð½ÐµÑ‡ÐµÑÑ‚Ð½Ð¾ ÑÐ´ÐµÐ»Ð°Ð½Ð½Ð¾, Ð¼Ð¾Ð¶Ð½Ð¾ Ð»ÑƒÑ‡ÑˆÐµ\n",
    "word_distribution /= word_distribution.sum()\n",
    "\n",
    "indices = np.arange(len(word_distribution))\n",
    "\n",
    "np.random.choice(indices, p=word_distribution, size=(32, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_o2pzsue16Lu"
   },
   "outputs": [],
   "source": [
    "class NegativeSamplingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, targets, num_samples):\n",
    "        '''\n",
    "        inputs: (batch_size, context_size)\n",
    "        targets: (batch_size)\n",
    "        num_samples: int\n",
    "        '''\n",
    "        \n",
    "        <calculate u_c's>\n",
    "        \n",
    "        <calculate v_c>\n",
    "        \n",
    "        <sample indices>\n",
    "        <calculate negative vectors v'_c>\n",
    "        \n",
    "        <apply F.logsigmoid to v_c * u_c and to -v'_c * u_c>\n",
    "        \n",
    "        <calc result loss>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6wz2iRanqzlq"
   },
   "outputs": [],
   "source": [
    "model = NegativeSamplingModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  \n",
    "\n",
    "loss_every_nsteps = 1000\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for step, (batch, labels) in enumerate(make_cbow_batchs_iter(contexts, window_size=2, batch_size=128)):\n",
    "    <copy-paste (mostly) learning cycle>\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if step != 0 and step % loss_every_nsteps == 0:\n",
    "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
    "                                                                    time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe: Global Vectors\n",
    "\n",
    "The Global Vector (GloVe) model proposed by Pennington et al. ([2014](http://www.aclweb.org/anthology/D14-1162)) aims to combine the count-based matrix factorization and the context-based skip-gram model together.\n",
    "\n",
    "We all know the counts and co-occurrences can reveal the meanings of words. To distinguish from $p(w_O \\vert w_I)$ in the context of a word embedding word, we would like to define the co-ocurrence probability as:\n",
    "\n",
    "$$\n",
    "p_{\\text{co}}(w_k \\vert w_i) = \\frac{C(w_i, w_k)}{C(w_i)}\n",
    "$$\n",
    "\n",
    "$C(w_i, w_k)$ counts the co-occurrence between words $w_i$ and $w_k$.\n",
    "\n",
    "\n",
    "Say, we have two words, $w_i$=\"ice\" and $w_j$=\"steam\". The third word $\\tilde{w}_k$=\"solid\" is related to \"ice\" but not \"steam\", and thus we expect $p_{\\text{co}}(\\tilde{w}_k \\vert w_i)$ to be much larger than $p_{\\text{co}}(\\tilde{w}_k \\vert w_j)$ and therefore $\\frac{p_{\\text{co}}(\\tilde{w}_k \\vert w_i)}{p_{\\text{co}}(\\tilde{w}_k \\vert w_j)}$ to be very large. If the third word $\\tilde{w}_k$ = \"water\" is related to both or $\\tilde{w}_k$ = \"fashion\" is unrelated to either of them, $\\frac{p_{\\text{co}}(\\tilde{w}_k \\vert w_i)}{p_{\\text{co}}(\\tilde{w}_k \\vert w_j)}$ is expected to be close to one. \n",
    "\n",
    "The intuition here is that the word meanings are captured by the ratios of co-occurrence probabilities rather than the probabilities themselves. The global vector models the relationship between two words regarding to the third context word as:\n",
    "\n",
    "$$\n",
    "F(w_i, w_j, \\tilde{w}_k) = \\frac{p_{\\text{co}}(\\tilde{w}_k \\vert w_i)}{p_{\\text{co}}(\\tilde{w}_k \\vert w_j)}\n",
    "$$\n",
    "\n",
    "\n",
    "Further, since the goal is to learn meaningful word vectors, $F$ is designed to be a function of the linear difference between two words $w_i - w_j$:\n",
    "\n",
    "$$\n",
    "F((w_i - w_j)^\\top \\tilde{w}_k) = \\frac{p_{\\text{co}}(\\tilde{w}_k \\vert w_i)}{p_{\\text{co}}(\\tilde{w}_k \\vert w_j)}\n",
    "$$\n",
    "\n",
    "With the consideration of $F$ being symmetric between target words and context words, the final solution is to model $$F$$ as an **exponential** function. Please read the original paper ([Pennington et al., 2014](http://www.aclweb.org/anthology/D14-1162)) for more details of the equations.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "F({w_i}^\\top \\tilde{w}_k) &= \\exp({w_i}^\\top \\tilde{w}_k) = p_{\\text{co}}(\\tilde{w}_k \\vert w_i) \\\\\n",
    "F((w_i - w_j)^\\top \\tilde{w}_k) &= \\exp((w_i - w_j)^\\top \\tilde{w}_k) = \\frac{\\exp(w_i^\\top \\tilde{w}_k)}{\\exp(w_j^\\top \\tilde{w}_k)} = \\frac{p_{\\text{co}}(\\tilde{w}_k \\vert w_i)}{p_{\\text{co}}(\\tilde{w}_k \\vert w_j)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally,\n",
    "\n",
    "$$\n",
    "{w_i}^\\top \\tilde{w}_k = \\log p_{\\text{co}}(\\tilde{w}_k \\vert w_i) = \\log \\frac{C(w_i, \\tilde{w}_k)}{C(w_i)} = \\log C(w_i, \\tilde{w}_k) - \\log C(w_i)\n",
    "$$\n",
    "\n",
    "Since the second term $-\\log C(w_i)$ is independent of $k$, we can add bias term $$b_i$$ for $w_i$ to capture $-\\log C(w_i)$. To keep the symmetric form, we also add in a bias $\\tilde{b}_k$ for $\\tilde{w}_k$.\n",
    "\n",
    "$$\n",
    "\\log C(w_i, \\tilde{w}_k) = {w_i}^\\top \\tilde{w}_k + b_i + \\tilde{b}_k\n",
    "$$\n",
    "\n",
    "The loss function for the GloVe model is designed to preserve the above formula by minimizing the sum of the squared errors:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\theta = \\sum_{i=1, j=1}^V f(C(w_i,w_j)) ({w_i}^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log C(w_i, \\tilde{w}_j))^2\n",
    "$$\n",
    "\n",
    "The weighting schema $f(c)$ is a function of the co-occurrence of $w_i$ and $w_j$ and it is an adjustable model configuration. It should be close to zero as $c \\to 0$; should be non-decreasing as higher co-occurrence should have more impact; should saturate when $c$ become extremely large. The paper proposed the following weighting function.\n",
    "\n",
    "$$\n",
    "f(c) = \n",
    "  \\begin{cases}\n",
    "  (\\frac{c}{c_{\\max}})^\\alpha & \\text{if } c < c_{\\max} \\text{, } c_{\\max} \\text{ is adjustable.} \\\\\n",
    "  1 & \\text{if } \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xqDmuu7m_PB5"
   },
   "source": [
    "# Referrence\n",
    "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
    "[On word embeddings - Part 2: Approximating the Softmax, Sebastian Ruder](http://ruder.io/word-embeddings-softmax/index.html)  \n",
    "[Word2Vec Tutorial - The Skip-Gram Model, Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  \n",
    "[Word2Vec Tutorial Part 2 - Negative Sampling, Chris McCormick](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/) \n",
    "\n",
    "[Word2vec Parameter Learning Explained (2014), Xin Rong](https://arxiv.org/abs/1411.2738)  \n",
    "[Neural word embedding as implicit matrix factorization (2014), Levy, Omer, and Yoav Goldberg](http://u.cs.biu.ac.il/~nlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf) \n",
    "[Two/Too Simple Adaptations of Word2Vec for Syntax Problems (2015), Ling, Wang, et al.](https://www.aclweb.org/anthology/N/N15/N15-1142.pdf)  \n",
    "[Not All Neural Embeddings are Born Equal (2014)](https://arxiv.org/pdf/1410.0718.pdf)  \n",
    "[Retrofitting Word Vectors to Semantic Lexicons (2014), M. Faruqui, et al.](https://arxiv.org/pdf/1411.4166.pdf)  \n",
    "[All-but-the-top: Simple and Effective Postprocessing for Word Representations (2017), Mu, et al.](https://arxiv.org/pdf/1702.01417.pdf)  \n",
    "\n",
    "[Skip-Thought Vectors (2015), Kiros, et al.](https://arxiv.org/pdf/1506.06726)  \n",
    "\n",
    "[Backpropagation, Intuitions, cs231n + next parts in the Module 1](http://cs231n.github.io/optimization-2/)   \n",
    "[Calculus on Computational Graphs: Backpropagation, Christopher Olah](http://colah.github.io/posts/2015-08-Backprop/)\n",
    "\n",
    "[cs224n \"Lecture 2 - Word Vector Representations: word2vec\"](https://www.youtube.com/watch?v=ERibwqs9p38&index=2&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)  \n",
    "[cs224n \"Lecture 5 - Backpropagation\"](https://www.youtube.com/watch?v=isPiE-DBagM&index=5&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)   \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 03 - Word Embeddings (Part 2).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
