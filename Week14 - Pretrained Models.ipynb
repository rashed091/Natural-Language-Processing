{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4WlMyJVRkzQ"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch==0.4.1\n",
    "!pip -qq install torchtext==0.3.1\n",
    "!pip -qq install spacy==2.0.16\n",
    "!pip -qq install gensim==3.6.0\n",
    "!pip -qq install allennlp==0.7.2\n",
    "!pip -qq install pytorch-pretrained-bert==0.1.2\n",
    "!python -m spacy download en\n",
    "\n",
    "!git clone https://github.com/rowanz/swagaf.git\n",
    "!wget -O conll_2003.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1z7swIvfWs97lKkTpHKIeV7Cgv4NKxGC0\"\n",
    "!unzip conll_2003.zip\n",
    "!wget -qq https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/week08_multitask/conlleval.py\n",
    "!wget -O fintech.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=110Mi9nF0J_FTv1MHhf1G-FsZIWEErMzK\"\n",
    "!unzip fintech.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvJKy3mtVOpw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQXfZ9MCmZBb"
   },
   "source": [
    "# Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h5YtUOd_m1eR"
   },
   "source": [
    "One of the most notable trends in NLP in 2018 is the use of pre-trained language models as components in models that are trained for specific tasks.\n",
    "\n",
    "In fact, we are already familiar with this approach: word2vec is also a language model, but very simple. The difference is that from embeddingings of words without context, we turn to embeddingings of words in context.\n",
    "\n",
    "A rather pathetic explanation of why this is important: [NLP's ImageNet now has arrived] (http://ruder.io/nlp-imagenet/).\n",
    "\n",
    "At the same time, the quality increases and the speed drops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KST58kDCovBH"
   },
   "source": [
    "## Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJpNBSkkq5vm"
   },
   "source": [
    "Let's start a little not with the language model: [Universal Sentence Encoder] (https://arxiv.org/pdf/1803.11175.pdf). This model was pre-trained in the multi-task learning mode: the encoder learned to produce \"universal\" representations of proposals, in which specific task decoders learned such things as the prediction of the previous and next sentences (Skip-Thoughts like model) or the usual classification on marked data.\n",
    "\n",
    "The main point is that the representations are quite self-interpretable, even without some special training for the desired task.\n",
    "\n",
    "* Unfortunately, I do not know how to use this model on pytorch, so I have to live with tensorflow ... *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nmVS7Tdconz_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "universal_sentence_encoder = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\", trainable=False)\n",
    "sess.run([tf.global_variables_initializer(), tf.tables_initializer()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9WZ_IacoueT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "inputs = tf.placeholder(tf.string, shape=[None])\n",
    "outputs = universal_sentence_encoder(inputs)\n",
    "\n",
    "lines = [\n",
    "    \"How old are you?\",                                                                 # 0\n",
    "    \"Attempting to light a cigarette, someone fumbles with the lighter and drops it.\",  # 1\n",
    "    \"This is the story of a man named Neil Fisk, and how he came to love God.\",         # 2\n",
    "    \"What is your age?\",                                                                # 3\n",
    "    \"Do you have a moment to talk about our Lord?\",                                     # 4\n",
    "]\n",
    "\n",
    "result = sess.run(outputs, {\n",
    "    inputs: lines\n",
    "})\n",
    "\n",
    "plt.title('phrase similarity')\n",
    "plt.imshow(result.dot(result.T), interpolation='none', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhUsLpQRtlEp"
   },
   "source": [
    "For example, the phrase \"How old are you?\" and \"What is your age?\" do not have common words, but their cosine proximity is quite high. Similarly, with the second and fourth phrases.\n",
    "\n",
    "Note that the vectors at the exit of the model are already normalized - therefore, the cosine proximity is considered as a scalar product.\n",
    "\n",
    "As an example of using these ideas almost for free, let's solve this (flooded) task dragged away from another course: https://www.kaggle.com/c/fintech-tinkoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHAD5rL_5pTi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LuB3jS_U5zXU"
   },
   "source": [
    "Дано 60 тысяч пар похожих вопросов + куча вопросов из категории \"другое\". Нужно определить, к какой паре ближе данный вопрос или сказать, что он из другого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txbQGR4O5x5A"
   },
   "outputs": [],
   "source": [
    "train_data.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sf8xw3856ef0"
   },
   "source": [
    "Модель вполне себе умеет говорить, что вопросы похожи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EDRd1WMj6aKL"
   },
   "outputs": [],
   "source": [
    "result = sess.run(outputs, {\n",
    "    inputs: train_data.iloc[:10].text\n",
    "})\n",
    "\n",
    "plt.title('phrase similarity')\n",
    "plt.imshow(result.dot(result.T), interpolation='none', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Kj8zX_S6wwW"
   },
   "source": [
    "Напишем функцию для подсчета эмбеддингов всех предложений в датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRBz8AYh6qqX"
   },
   "outputs": [],
   "source": [
    "def calc_vectors(data):\n",
    "    BATCH_SIZE = 1024\n",
    "\n",
    "    vectors = []\n",
    "    for batch_begin in range(0, len(data), BATCH_SIZE):\n",
    "        batch_end = min(len(data), batch_begin + BATCH_SIZE)\n",
    "\n",
    "        vectors.append(\n",
    "            sess.run(outputs, {\n",
    "                inputs: data.iloc[batch_begin: batch_end].text\n",
    "            })\n",
    "        )\n",
    "\n",
    "    return np.concatenate(vectors, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CmGq2dFE67F8"
   },
   "outputs": [],
   "source": [
    "train_vectors = calc_vectors(train_data)\n",
    "train_labels = train_data['labels'].values\n",
    "\n",
    "test_vectors = calc_vectors(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ML59PBun6_iN"
   },
   "outputs": [],
   "source": [
    "for test_ind in range(10):\n",
    "    print(test_data.iloc[test_ind].text, train_data.iloc[(train_vectors * test_vectors[test_ind]).sum(-1).argmax()].text, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HpfewmpJ7ogN"
   },
   "source": [
    "** Task ** We have a base of vectors and their corresponding tags. Implement 1NN - search for the nearest neighbor for queries from the test sample.\n",
    "\n",
    "As a label, then you can take the label of this nearest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_kV7t5H8JKf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yHxji0I88J1F"
   },
   "source": [
    "In general, to make a search quickly over a large database, use approximate algorithms such as: [HNSW] (https://github.com/nmslib/hnswlib) In this case, this is not relevant, but you can still try to replace your 1NN with 2NN from that library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6OUA2XWet3mj"
   },
   "source": [
    "## ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9J9kpe8Lt-94"
   },
   "source": [
    "Another story in the case of [ELMo] (https://arxiv.org/pdf/1802.05365.pdf). This is the usual language model:\n",
    "<img src=\"https://i.ibb.co/dpp00wG/elmo.png\" width=\"50%\">\n",
    "\n",
    "*From [Improving a Sentiment Analyzer using ELMo — Word Embeddings on Steroids](http://www.realworldnlpbook.com/blog/improving-sentiment-analyzer-using-elmo.html)*\n",
    "\n",
    "Well, almost ordinary. First, word embeddings are constructed using a convolution network over symbols.\n",
    "\n",
    "Secondly, two language models study at once, forward and backward, which are then concatenated.\n",
    "\n",
    "Finally, thirdly, the meaning of the model is that it gives the embedding of the word according to its context. This could be the output of the last LSTM layer, but the guys did it more slyly: for each word we have several embeddingings at once: the outputs of each of the LSTM layers + the output of the convolutional network above the characters. When training the final model for the desired task, the word embedding is considered as a weighted sum of the embedding data. Weights learn by task.\n",
    "\n",
    "As a result, when we want to use ELMo in our model, we take this language model and substitute our usual embeddings instead. All, a couple of lines of change - but more good and much slower inserts.\n",
    "\n",
    "You can train the language model for the task - then you will have to retrain all these millions of parameters, which is slow. And you can not train out - then inside ELMo will only learn `(num_layers + 1)` parameter - the weight of the mix of embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnDv4lNTwlWz"
   },
   "source": [
    "### NER\n",
    "\n",
    "Such embeddings can be used anywhere, but they look best in tasks related to sequence markup (it’s more critical to get embeddingings from a context perspective, whereas Universal Sentence Encoder looks more logical in ELMo for classification tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aXsVTn7frq2V"
   },
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        words, tags = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line and words:\n",
    "                data.append((words, tags))\n",
    "                words, tags = [], []\n",
    "                continue\n",
    "            word, pos_tag, synt_tag, ner_tag = line.split()\n",
    "            words.append(word)\n",
    "            tags.append(ner_tag)\n",
    "        if words:\n",
    "            data.append((words, tags))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLPzgKJ7s8cd"
   },
   "outputs": [],
   "source": [
    "train_data = read_dataset('train.txt')\n",
    "val_data = read_dataset('valid.txt')\n",
    "test_data = read_dataset('test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJ5_M6Nhxxwm"
   },
   "source": [
    "Мы уже смотрели на NER, но вообще он такой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKAkEGvjtEZB"
   },
   "outputs": [],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UoVVubMIyBCq"
   },
   "source": [
    "Соберем датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aW8WYM3Zrqeh"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, Example, Dataset, BucketIterator\n",
    "\n",
    "tokens_field = Field(unk_token=None, batch_first=True)\n",
    "tags_field = Field(unk_token=None, batch_first=True)\n",
    "\n",
    "fields = [('tokens', tokens_field), ('tags', tags_field)]\n",
    "\n",
    "train_dataset = Dataset([Example.fromlist(example, fields) for example in train_data], fields)\n",
    "val_dataset = Dataset([Example.fromlist(example, fields) for example in val_data], fields)\n",
    "test_dataset = Dataset([Example.fromlist(example, fields) for example in test_data], fields)\n",
    "\n",
    "tokens_field.build_vocab(train_dataset, val_dataset, test_dataset)\n",
    "tags_field.build_vocab(train_dataset)\n",
    "\n",
    "print('Vocab size =', len(tokens_field.vocab))\n",
    "print('Tags count =', len(tags_field.vocab))\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "    datasets=(train_dataset, val_dataset, test_dataset), batch_sizes=(32, 128, 128), \n",
    "    shuffle=True, device=DEVICE, sort=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJEwsOXqvjUB"
   },
   "source": [
    "* Please note: the dictionary is based on all datasets. It is not that it is very ethical, but we will not complete any embeddingings - therefore nothing is dishonest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_D02bmmzyWOs"
   },
   "source": [
    "### Baseline\n",
    "\n",
    "To begin with, we will teach a model with pre-trained word embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBrt0DZkbq3c"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-MvdZmecYRC"
   },
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(tokens_field.vocab), w2v_model.vectors.shape[1]))\n",
    "\n",
    "for i, token in enumerate(tokens_field.vocab.itos):\n",
    "    if token.lower() in w2v_model.vocab:\n",
    "        embeddings[i] = w2v_model.get_vector(token.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RzVHzUqwywF5"
   },
   "source": [
    "**Задание** Допишите модель простого теггера.  \n",
    "Обратите внимание `batch_first=True` в `fields` (это для ELMo понадобится)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jvQkY09FuY7_"
   },
   "outputs": [],
   "source": [
    "class BaselineTagger(nn.Module):\n",
    "    def __init__(self, embeddings, tags_count, emb_dim=100, rnn_dim=256, num_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        <init layers>\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        <apply layers>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OIX-u6i0y1gN"
   },
   "source": [
    "**Задание** Допишите тренировщик модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1ORojhZvRlk"
   },
   "outputs": [],
   "source": [
    "class ModelTrainer():\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self._model = model\n",
    "        self._criterion = criterion\n",
    "        self._optimizer = optimizer\n",
    "        \n",
    "    def on_epoch_begin(self, is_train, name, batches_count):\n",
    "        \"\"\"\n",
    "        Initializes metrics\n",
    "        \"\"\"\n",
    "        self._epoch_loss = 0\n",
    "        self._correct_count, self._total_count = 0, 0\n",
    "        self._is_train = is_train\n",
    "        self._name = name\n",
    "        self._batches_count = batches_count\n",
    "        \n",
    "        self._model.train(is_train)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Outputs final metrics\n",
    "        \"\"\"\n",
    "        return '{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "            self._name, self._epoch_loss / self._batches_count, self._correct_count / self._total_count\n",
    "        )\n",
    "        \n",
    "    def on_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Performs forward and (if is_train) backward pass with optimization, updates metrics\n",
    "        \"\"\"        \n",
    "        loss = <calc loss>\n",
    "        correct_count, total_count = <and this stuff>\n",
    "        \n",
    "        self._correct_count += correct_count\n",
    "        self._total_count += total_count\n",
    "        self._epoch_loss += loss.item()\n",
    "        \n",
    "        if self._is_train:\n",
    "            self._optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self._model.parameters(), 1.)\n",
    "            self._optimizer.step()\n",
    "\n",
    "        return '{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "            self._name, loss.item(), correct_count / total_count\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_VlSd9DCwKoR"
   },
   "source": [
    "Воспользуемся уже знакомой функцией для оценки теггера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A46PovRQwwaF"
   },
   "outputs": [],
   "source": [
    "from conlleval import evaluate\n",
    "\n",
    "def eval_tagger(model, test_iter):\n",
    "    true_seqs, pred_seqs = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            logits = model(batch.tokens)\n",
    "            preds = logits.argmax(-1)\n",
    "\n",
    "            seq_lengths = (batch.tags != 0).sum(-1)\n",
    "\n",
    "            for i, seq_len in enumerate(seq_lengths):\n",
    "                true_seqs.append(' '.join(tags_field.vocab.itos[ind] for ind in batch.tags[i, :seq_len]))\n",
    "                pred_seqs.append(' '.join(tags_field.vocab.itos[ind] for ind in preds[i, :seq_len]))\n",
    "\n",
    "    print('Precision = {:.2f}%, Recall = {:.2f}%, F1 = {:.2f}%'.format(*evaluate(true_seqs, pred_seqs, verbose=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2B5gBXVTwNIw"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.get_lock().locks = []\n",
    "\n",
    "\n",
    "def do_epoch(trainer, data_iter, is_train, name=None):\n",
    "    trainer.on_epoch_begin(is_train, name, batches_count=len(data_iter))\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=len(data_iter)) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                batch_progress = trainer.on_batch(batch)\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description(batch_progress)\n",
    "                \n",
    "            epoch_progress = trainer.on_epoch_end()\n",
    "            progress_bar.set_description(epoch_progress)\n",
    "            progress_bar.refresh()\n",
    "\n",
    "            \n",
    "def fit(trainer, train_iter, epochs_count=1, val_iter=None):\n",
    "    best_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        do_epoch(trainer, train_iter, is_train=True, name=name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            do_epoch(trainer, val_iter, is_train=False, name=name_prefix + '  Val:')\n",
    "            eval_tagger(trainer._model, val_iter)\n",
    "            print(flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQqkOaDJ0oNm"
   },
   "source": [
    "Запустим обучение модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TW-efkm_wNAr"
   },
   "outputs": [],
   "source": [
    "model = BasicTagger(len(tokens_field.vocab), tags_count=len(tags_field.vocab)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "trainer = ModelTrainer(model, criterion, optimizer)\n",
    "\n",
    "fit(trainer, train_iter, epochs_count=32, val_iter=val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WiaypapRzENE"
   },
   "source": [
    "### ELMo Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_TaJcmSYzOmZ"
   },
   "source": [
    "Take a pre-trained model from the authors (a description of working with it is in. [ELMo how to](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDy9eHUB2a76"
   },
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "elmo = Elmo(options_file, weight_file, num_output_representations=1,\n",
    "            dropout=0, vocab_to_cache=tokens_field.vocab.itos).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6veC-uyzsZY"
   },
   "source": [
    "Вообще надо сначала преобразовать предложение в символьное представление:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dr1W_aDDzglP"
   },
   "outputs": [],
   "source": [
    "sentences = [['First', 'sentence', '.'], ['Another', '.']]\n",
    "character_ids = batch_to_ids(sentences).to(DEVICE)\n",
    "\n",
    "character_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byLMOMtpz6ax"
   },
   "source": [
    "А потом засунуть это в модель elmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZwB4k0G1z1QD"
   },
   "outputs": [],
   "source": [
    "elmo(character_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnrdSEWN0uG7"
   },
   "source": [
    "We are interested in `elmo_representations`.\n",
    "\n",
    "Such an interface is not very convenient - you need to transfer strings to `batch_to_ids`, which means writing a batch generator from scratch. In addition, it is not good to transfer large batch files to gpu - and with a symbolic representation from the batch `(batch_size, seq_len)` we get the batch in `max_word_len` times larger. Finally, the calculation of word embeds for characters is not free (regarding the request to the table of embeddings).\n",
    "\n",
    "Therefore, when creating the model, we cached all embeddings: the `vocab_to_cache = tokens_field.vocab.itos` parameter.\n",
    "\n",
    "In order to use cached values ​​in `inputs`, we will transfer some garbage, and in` word_inputs` - embedding indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NiaqSTFz_e8W"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "\n",
    "elmo(inputs=batch.tokens.new_empty((batch.tokens.shape[0], batch.tokens.shape[1], 50)), \n",
    "     word_inputs=batch.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HMk_T5gy_t0b"
   },
   "source": [
    "**Задание** Обновите модель, заменив эмбеддинги на ELMo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSU3PjpewM1V"
   },
   "outputs": [],
   "source": [
    "class ELMoTagger(nn.Module):\n",
    "    def __init__(self, elmo, tags_count, emb_dim=1024, rnn_dim=256, num_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        <init layers>\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        <apply layers>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFUGuPqDAu4h"
   },
   "outputs": [],
   "source": [
    "model = ELMoTagger(elmo, tags_count=len(tags_field.vocab)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "trainer = ModelTrainer(model, criterion, optimizer)\n",
    "\n",
    "fit(trainer, train_iter, epochs_count=32, val_iter=val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OruZBPUq_0dR"
   },
   "source": [
    "### CRF\n",
    "\n",
    "This is not relevant to what is happening, generally speaking, but also useful information: in order to work better with the task of predicting sequence tags, you can, instead of independently predicting each tag individually, predict a tag under the condition of the previous tag.\n",
    "\n",
    "It looks like a decoder in Seq2Seq models - only instead of the previous word, the previous tag is input.\n",
    "\n",
    "This prediction can be implemented in different ways. One of them is using CRF (Conditional Random Field) (A good description is[здесь](http://www.davidsbatista.net/blog/2017/11/13/Conditional_Random_Fields/).\n",
    "\n",
    "We now have a model with fully connected output layer. At each step of its output, the probability of which tag has the given word is estimated — the speeds are simply normalized using softmax: $ p [i] = \\frac {e ^ {s [i]}} {\\sum_ {j = 1} ^ 9 e ^ {s [j]}} $.\n",
    "\n",
    "And instead of local normalization, let's do global, for the entire sequence. In addition, let's learn the transition probabilities from one tag in the previous step to another tag in the next $ T [y_t, y_ {t + 1}] $. For example, it should be learned that the probability after `O` to meet` I-LOC` is zero (`I-LOC` can only be after` B-LOC` or another `I-LOC`).\n",
    "\n",
    "Then each sequence will be evaluated using the following formula:\n",
    "$$\\begin{align*}\n",
    "C(y_1, \\ldots, y_m) &= b[y_1] &+ \\sum_{t=1}^{m} s_t [y_t] &+ \\sum_{t=1}^{m-1} T[y_{t}, y_{t+1}] &+ e[y_m]\\\\\n",
    "                    &= \\text{begin} &+ \\text{scores} &+ \\text{transitions} &+ \\text{end}\n",
    "\\end{align*}$$\n",
    "\n",
    "For example, there may be two sequence variants:\n",
    "<img src=\"https://guillaumegenthial.github.io/assets/crf1.png\" width=\"50%\">\n",
    "<img src=\"https://guillaumegenthial.github.io/assets/crf2.png\" width=\"50%\">\n",
    "\n",
    "*From [Sequence Tagging with Tensorflow](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html)*\n",
    "\n",
    "The best of them is the one that has the lower the amount of tags' tags + the sum of transitions between the tags.\n",
    "\n",
    "In this case, you just need to add the ConditionalRandomField module - it will learn $ T $ transitions, consider the global loss for the entire sequence, and also do decoding.\n",
    "\n",
    "Counting a loss is done this way:\n",
    "\n",
    "```python\n",
    "crf = ConditionalRandomField (tags_count)\n",
    "loss = -crf (output, tags, mask)\n",
    "```\n",
    "where `output` is the output of the fully connected layer that was previously the last.\n",
    "\n",
    "Decoding is done like this:\n",
    "```python\n",
    "decoded_sequences = crf.viterbi_tags (output, mask)\n",
    "```\n",
    "\n",
    "**Task** Update the tagger and learning functions with the assessment of the quality of the tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlnBFTUWc4Gt"
   },
   "outputs": [],
   "source": [
    "from allennlp.modules import ConditionalRandomField\n",
    "\n",
    "class CRFTagger(nn.Module):\n",
    "    def __init__(self, embeddings, tags_count, emb_dim=100, rnn_dim=256, num_layers=1):\n",
    "        super().__init__()\n",
    "        <init layers (embeddings can be from either glove or elmo)>\n",
    "        \n",
    "    def forward(self, inputs, mask, tags=None):\n",
    "        <apply layers like in previos models>\n",
    "        \n",
    "        if tags is not None:\n",
    "            return <crf loss>\n",
    "        return <viterbi decoding>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKFL_N7aF_eE"
   },
   "source": [
    "## BERT\n",
    "\n",
    "Finally, the third variant of the model (something in between with a bunch of its buns) is BERT (yes, dudes know how to name their models).\n",
    "\n",
    "Similarity to ELMo is also a language model. Differences:\n",
    "1. This is a Transformer, not BiLSTM.\n",
    "<img src=\"https://i.ibb.co/PQ4qXtr/2018-12-22-21-13-14.png\" width=\"50%\">\n",
    "\n",
    "To learn the language model in the framework of the transformer, they accidentally threw out some words from the sentence and tried to predict them using the network. Thus, in the prediction of the token, the entire context was available, and not just the left or just the right, as in ELMo.\n",
    "\n",
    "2. An additional task in the style of Skip-Thoughts was used - a prediction of the following sentence:\n",
    "<img src=\"https://i.ibb.co/WWdwmPD/2018-12-22-21-12-59.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "A couple of sentences were recorded in a row (focus on SEP) and the model learned to predict whether they were coming in succession (focus on CLS).\n",
    "\n",
    "As a result, CLS embedding was learned in such a way that it contained information about the whole context - and then it can easily be used for classification, as in the first model. But at the same time context embeddingings of all words in the sentence are also learned, therefore the model can be used similarly to ELMo in the task of tagging.\n",
    "\n",
    "BERT is remarkable for breaking the results of all currently existing models, as well as human performance on some tasks ([for example, SQuAD 1] (https://rajpurkar.github.io/SQuAD-explorer/)).\n",
    "\n",
    "We use it for the task [Swag] (https://rowanzellers.com/swag/) - the choice of the correct continuation for the text. Dataset, as the name implies, was formed in such a way that it was difficult to do for a soulless machine (but after a few months BERT came out and all was nothing :()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tuvwIAN6Ktfx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('swagaf/data/train.csv')\n",
    "val_data = pd.read_csv('swagaf/data/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ueTJGlcKuA3"
   },
   "outputs": [],
   "source": [
    "train_data.sample(10)[['startphrase', 'sent1', 'sent2', 'ending0', 'ending1', 'ending2', 'ending3', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "siUT4-SOKyX-"
   },
   "source": [
    "Обратите внимание на токенизатор - модель работает с подсловами как в bpe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CmRl9Moz2j7"
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(tokenizer.vocab)\n",
    "\n",
    "tokens = tokenizer.tokenize('Why do I do this stuff...')\n",
    "print(tokens)\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ameGUADW0N3j"
   },
   "source": [
    "Соберем датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbDtPRD6KxR-"
   },
   "outputs": [],
   "source": [
    "def collect_samples(data, tokenizer):\n",
    "    choices_list, labels_list, segment_ids_list = [], [], []\n",
    "    \n",
    "    for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        first_phrase = tokenizer.tokenize(row.sent1)\n",
    "        second_phrase = tokenizer.tokenize(row.sent2)\n",
    "        choices, segment_ids = [], []\n",
    "        for i, ending in enumerate(row[['ending0', 'ending1', 'ending2', 'ending3']]):\n",
    "            ending = tokenizer.tokenize(ending)\n",
    "            tokens = [\"[CLS]\"] + first_phrase + [\"[SEP]\"] + second_phrase + ending + [\"[SEP]\"]\n",
    "            tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            choices.append(tokens)\n",
    "            \n",
    "            segment_ids.append([0] * (len(first_phrase) + 2) + [1] * (len(tokens) - len(first_phrase) - 2))\n",
    "            \n",
    "        choices_list.append(choices)\n",
    "        segment_ids_list.append(segment_ids)\n",
    "        labels_list.append(row.label)\n",
    "\n",
    "    return np.array(choices_list), np.array(labels_list), np.array(segment_ids_list)\n",
    "\n",
    "\n",
    "train_data, train_labels, train_segment_ids = collect_samples(train_data, tokenizer)\n",
    "val_data, val_labels, val_segment_ids = collect_samples(val_data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c8Bd8Z8Wze3k"
   },
   "source": [
    "Будем сэмплы формировать также, как было при обучении модели - `[CLS]<first text>[SEP]<next possible text>[SEP]`.\n",
    "\n",
    "Чтобы модель знала, где заканчивается первый сегмент и начинается второй передается заодно еще `train_segment_ids` - 0 соответствует токену из первого сегмента, а 1 - из второго."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eJbfkFB4zYeD"
   },
   "outputs": [],
   "source": [
    "train_data[:2], train_labels[:2], train_segment_ids[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mhwbwINiK2Fh"
   },
   "source": [
    "Итератор батчей таки придется написать свой..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sArV5QNOLOOx"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def to_matrix(choices_list):\n",
    "    batch_size = len(choices_list)\n",
    "    num_options = len(choices_list[0])\n",
    "    seq_len = max(len(choice) for choices in choices_list for choice in choices)\n",
    "    \n",
    "    matrix = np.zeros((batch_size, num_options, seq_len))\n",
    "    for i, choices in enumerate(choices_list):\n",
    "        for j, choice in enumerate(choices):\n",
    "            matrix[i, j, :len(choice)] = choice\n",
    "\n",
    "    return matrix\n",
    "    \n",
    "\n",
    "class BatchIterator():\n",
    "    def __init__(self, data, labels, segment_ids, batch_size, shuffle=True):\n",
    "        self._data = data\n",
    "        self._labels = labels\n",
    "        self._segment_ids = segment_ids\n",
    "        self._num_samples = len(data)\n",
    "        self._batch_size = batch_size\n",
    "        self._shuffle = shuffle\n",
    "        self._batches_count = int(math.ceil(len(data) / batch_size))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._batches_count\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self._iterate_batches()\n",
    "\n",
    "    def _iterate_batches(self):\n",
    "        indices = np.arange(self._num_samples)\n",
    "        if self._shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for start in range(0, self._num_samples, self._batch_size):\n",
    "            end = min(start + self._batch_size, self._num_samples)\n",
    "\n",
    "            batch_indices = indices[start: end]\n",
    "            \n",
    "            choices = to_matrix(self._data[batch_indices])\n",
    "            mask = (choices != 0).astype(np.int)\n",
    "            yield {\n",
    "                'choices': choices,\n",
    "                'segment_ids': to_matrix(self._segment_ids[batch_indices]),\n",
    "                'mask': mask,\n",
    "                'label': self._labels[batch_indices]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ll77HRDVLUea"
   },
   "source": [
    "Тренироваться на colab можно только с очень маленьким батчем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ck5QcejCLUFQ"
   },
   "outputs": [],
   "source": [
    "train_iter = BatchIterator(train_data, train_labels, train_segment_ids, 8)\n",
    "val_iter = BatchIterator(val_data, val_labels, val_segment_ids, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KgD4Br8CLcU6"
   },
   "source": [
    "Загружаем предобученный BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0mohrzKlLahe"
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased').to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yrIEAZiYLiLm"
   },
   "source": [
    "We build such a model:\n",
    "<img src = \"https://i.ibb.co/JFcwW6D/2018-12-22-21-23-23-20.png\" width = \"50%\">\n",
    "\n",
    "BERT builds four representations $ C_0, \\ldots, C_3 $ for four options for continuing the sentence. We will learn the parameter $ V $, whose scalar product with $ C_i $ should be maximal for the relevant continuation.\n",
    "\n",
    "For this we will simply use the cross-entropy loss and assume softmax:\n",
    "$ P_i = \\frac {e ^ {V \\cdot C_i}} {\\ um_j e ^ {V \\cdot C_j}} $.\n",
    "\n",
    "That is, the joke is that the only task-specific that we are learning is the vector $ V $!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9Xh2f5eNIwo"
   },
   "outputs": [],
   "source": [
    "class MultipleChoiceModel(nn.Module):\n",
    "    def __init__(self, bert, num_choices):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._bert = bert\n",
    "        self._num_choices = num_choices\n",
    "        self._dropout = nn.Dropout(0.1)\n",
    "        self._classifier = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self, choices, segment_ids, mask):\n",
    "        \"\"\"\n",
    "        choices: LongTensor of shape [batch_size, num_choices, seq_len] with token ids\n",
    "        segment_ids: LongTensor of shape [batch_size, num_choices, seq_len] with token types (0 for first segment, 1 for second)\n",
    "        mask: LongTensor of shape [batch_size, num_choices, seq_len] with mask for padding tokens\n",
    "        returns logits - FloatTensor of shape [batch_size, num_choices]\n",
    "        \"\"\"\n",
    "        choices = choices.view(-1, choices.size(-1))\n",
    "        segment_ids = segment_ids.view(-1, segment_ids.size(-1))\n",
    "        mask = mask.view(-1, mask.size(-1))\n",
    "        \n",
    "        _, pooled_output = self._bert(choices, segment_ids, mask, output_all_encoded_layers=False)\n",
    "        \n",
    "        pooled_output = self._dropout(pooled_output)\n",
    "        logits = self._classifier(pooled_output)\n",
    "        return logits.view(-1, self._num_choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GFsvZg501W7S"
   },
   "source": [
    "**Задание** Доделайте обучалку для модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AgbnYjwlNLj1"
   },
   "outputs": [],
   "source": [
    "class ModelTrainer():\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self._model = model\n",
    "        self._criterion = criterion\n",
    "        self._optimizer = optimizer\n",
    "        \n",
    "    def on_epoch_begin(self, is_train, name, batches_count):\n",
    "        \"\"\"\n",
    "        Initializes metrics\n",
    "        \"\"\"\n",
    "        self._epoch_loss = 0\n",
    "        self._correct_count, self._total_count = 0, 0\n",
    "        self._is_train = is_train\n",
    "        self._name = name\n",
    "        self._batches_count = batches_count\n",
    "        \n",
    "        self._model.train(is_train)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Outputs final metrics\n",
    "        \"\"\"\n",
    "        return '{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "            self._name, self._epoch_loss / self._batches_count, self._correct_count / self._total_count\n",
    "        )\n",
    "        \n",
    "    def on_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Performs forward and (if is_train) backward pass with optimization, updates metrics\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = <calc loss>\n",
    "        correct_count, total_count = <and this stuff>\n",
    "        \n",
    "        self._correct_count += correct_count\n",
    "        self._total_count += total_count\n",
    "        self._epoch_loss += loss.item()\n",
    "        \n",
    "        if self._is_train:\n",
    "            self._optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self._model.parameters(), 1.)\n",
    "            self._optimizer.step()\n",
    "\n",
    "        return '{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "            self._name, loss.item(), correct_count / total_count\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XCHZ6-PNOoa"
   },
   "source": [
    "Нужно также немного магии для инициализации оптимизатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R47LnhS4NNS_"
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "model = MultipleChoiceModel(bert, 4).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "params = [(name, param) for name, param in model.named_parameters() if 'pooler' not in name]\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [param for name, param in params if not any(nd in name for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [param for name, param in params if any(nd in name for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=5e-5, warmup=0.1, t_total=len(train_iter) * 2)\n",
    "\n",
    "trainer = ModelTrainer(model, criterion, optimizer)\n",
    "\n",
    "fit(trainer, train_iter, 2, val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JLmy77H-NdT4"
   },
   "source": [
    "**Task** The larger batch does not fit into the memory. And I want to train with a big batch (Google had 16).\n",
    "\n",
    "To solve this, there is such a simple (if on pytorch, and not on tensorflow) reception - the accumulation of gradients. You can simply optimize the model not at every step, but every few steps of training:\n",
    "\n",
    "[Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups] -gpu-distributed-setups-ec88c3e51255)\n",
    "\n",
    "Implement this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2x9-j4oz08p"
   },
   "source": [
    "# Referrence\n",
    "Universal Language Model Fine-tuning for Text Classification [[pdf]](https://arxiv.org/pdf/1801.06146)  \n",
    "Deep contextualized word representations [[pdf]](https://arxiv.org/pdf/1802.05365)  \n",
    "Improving Language Understanding by Generative Pre-Training [[pdf]](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)  \n",
    "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [[pdf]](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "\n",
    "Dissecting Contextual Word Embeddings: Architecture and Representation [[pdf]](http://aclweb.org/anthology/D18-1179)\n",
    "\n",
    "[NLP's ImageNet moment has arrived](http://ruder.io/nlp-imagenet/)  \n",
    "[The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/)  \n",
    "[Improving a Sentiment Analyzer using ELMo — Word Embeddings on Steroids](http://www.realworldnlpbook.com/blog/improving-sentiment-analyzer-using-elmo.html)\n",
    "\n",
    "[Sequence Tagging with Tensorflow](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html)  \n",
    "[Conditional Random Field Tutorial in PyTorch](https://towardsdatascience.com/conditional-random-field-tutorial-in-pytorch-ca0d04499463)  \n",
    "[Conditional Random Fields for Sequence Prediction](http://www.davidsbatista.net/blog/2017/11/13/Conditional_Random_Fields/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 14 - Pretrained Models.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
