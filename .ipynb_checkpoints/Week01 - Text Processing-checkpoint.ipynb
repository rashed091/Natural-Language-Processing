{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "To solve this problem we need several processing steps. First we need to convert the raw text-words into so-called tokens which are integer values. These tokens are really just indices into a list of the entire vocabulary. Then we convert these integer-tokens into so-called embeddings which are real-valued vectors, whose mapping will be trained along with the neural network, so as to map words with similar meanings to similar embedding-vectors. Then we input these embedding-vectors to a Recurrent Neural Network which can take sequences of arbitrary length as input and output a kind of summary of what it has seen in the input. This output is then squashed using a Sigmoid-function to give us a value between 0.0 and 1.0, where 0.0 is taken to mean a negative sentiment and 1.0 means a positive sentiment. This whole process allows us to classify input-text as either having a negative or positive sentiment.\n",
    "\n",
    "The flowchart of the algorithm is roughly:\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/natural_language.png\" width=\"20%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import datasets\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The quick fox jumped over a lazy dog.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vGxzf4oXmXqw"
   },
   "source": [
    "### Bag of words\n",
    "\n",
    "How will we present the text? The easiest way is with a bag of words.\n",
    "\n",
    "Let's get a big-big dictionary - a list of all the words in the training set. Then each sentence can be represented as a vector in which it will be written, how many times each of the possible words has been encountered:\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/BOW.png\" width=\"15%\">\n",
    "</center>\n",
    "\n",
    "A simple and enjoyable way to do this is to stuff the texts into the `CountVectorizer`.\n",
    "\n",
    "It has the following signature:\n",
    "\n",
    "```python\n",
    "CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=r'(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64'>)\n",
    "```\n",
    "\n",
    "\n",
    "To begin with, pay attention to the parameters `lowercase = True` and` max_df = 1.0, min_df = 1, max_features = None` - they mean that by default all words will be converted to lower case and all words found in the texts will be included in the dictionary .\n",
    "\n",
    "If desired, it would be possible to remove too rare or too frequent words - until we do this.\n",
    "\n",
    "Let's look at a simple example of how it will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Odnum4iyGDr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 1]\n",
      " [1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "dummy_data = ['The movie was excellent', 'the movie was awful']\n",
    "\n",
    "dummy_matrix = vectorizer.fit_transform(dummy_data)\n",
    "\n",
    "print(dummy_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awful', 'excellent', 'movie', 'the', 'was']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q3zC1ItWybVc"
   },
   "source": [
    "How exactly does vectorizer define word boundaries? Note the parameter `token_pattern = r '(? U) \\ b \\ w \\ w + \\ b'` - how will it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZCRef3pyCRC"
   },
   "source": [
    "What they wanted was a vector with a bow (i.e., bag-of-words) representation of the source text.\n",
    "\n",
    "And how can this information help? Well, all the same - some words are positive color, some - negative. Most are generally neutral, yes.\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/BOW_weights.png\" width=\"15%\">\n",
    "</center>\n",
    "\n",
    "I would like, probably, to choose the coefficients that will determine the level of color, right? It is necessary to select by the training sample, and not as we did before.\n",
    "\n",
    "For example, for sampling\n",
    "\n",
    "```\n",
    "1 The movie was excellent\n",
    "0 the movie was awful\n",
    "```\n",
    "\n",
    "It’s easy to pick odds on the eye: something like `+ 1` for` excellent`, `-1` for` awful` and zeros for everything else.\n",
    "\n",
    "Let's build a linear model that will do this. She will learn to build a separating hyperplane in the space of bow-vectors.\n",
    "\n",
    "Check out how the logistic regression can handle our super sample of a couple of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6WVgK4LtUn2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awful', 'excellent', 'movie', 'the', 'was']\n",
      "[[-0.40104279  0.40104279  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "dummy_data = ['The movie was excellent',\n",
    "              'the movie was awful']\n",
    "dummy_labels = [1, 0]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(dummy_data, dummy_labels)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(classifier.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dz7GSFzIlv4V"
   },
   "source": [
    "## Tf-idf\n",
    "\n",
    "Now we look at all words with the same weight - although some of them are more rare, some more frequent, and this frequency is useful, generally speaking, information.\n",
    "\n",
    "The easiest way to add statistical information about frequencies is to do * tf-idf * weighting:\n",
    "\n",
    "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$$\n",
    "\n",
    "*tf* - term-frequency - frequency of the word `t` in a specific document` d` (reviews in our case). This is exactly what we already thought.\n",
    "\n",
    "*idf* - inverse document-frequency - coefficient, which is greater, the smaller the number of documents met this word. It is considered something like this:\n",
    "\n",
    "$$\\text{idf}(t) = \\text{log}\\frac{1 + n_d}{1 + n_{d(t)}} + 1$$\n",
    "\n",
    "where $n_d$ is the number of all documents, and $ n_{d (t)} $ is the number of documents with the word `t`.\n",
    "\n",
    "Using it is easy - you need to replace `CountVectorizer` with` TfidfVectorizer`.\n",
    "\n",
    "**Task** Try running `TfidfVectorizer`. Look at the mistakes that he learned to correct, and the mistakes that he began to make - compared to `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3DjjiJglvT3"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "# eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xe_CJxQ0tFP9"
   },
   "source": [
    "### N-gram\n",
    "\n",
    "Until now, we looked at the texts as a bag of words - but it is obvious that there is a difference between the `good movie` and` not good movie`.\n",
    "\n",
    "Add information (at least some) about the sequences of words - we will also extract the digrams of words.\n",
    "\n",
    "In Vectorizers, this has the option `ngram_range = (n_1, n_2)` - it says that we need n_1 -... n_2-grams.\n",
    "\n",
    "**Task** Try an increased range and interpret the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RDpdrT0HuKYN"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "# eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrBoThj6wl2F"
   },
   "source": [
    "### N-grams characters\n",
    "\n",
    "Character n-grams provide an easy way to learn useful roots and suffixes without being associated with this linguistics of yours - just statistics, only hardcore.\n",
    "\n",
    "For example, the word `badass` we can represent in the form of such a sequence of trigrams:\n",
    "\n",
    "`##b #ba bad ada das ass ss# s##`\n",
    "\n",
    "So interpretable, is not it?\n",
    "\n",
    "It’s still as easy to implement as you need to put an analyzer = 'char'` in your favorite Vectorizer and choose the size of `ngram_range`.\n",
    "\n",
    "**Task** File a classifier on n-grams of characters and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QFaWmUrGyY3n"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(2, 6), max_features=20000, analyzer='char')\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# model.fit(train_df['review'], train_df['is_positive'])\n",
    "\n",
    "# eval_model(model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YkywIvbp4N-L"
   },
   "source": [
    "## Lemmatization and stemming\n",
    "\n",
    "If you look closely, you can find the forms of one word with different semantic coloring according to the classifier. Or not?\n",
    "\n",
    "**Assignment** Find the word forms with different semantic coloring.\n",
    "\n",
    "Believe that they are, try something to do with it.\n",
    "\n",
    "For example, lemmatizing - we reduce all words to the initial form. The spacy library will help in this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Pfup3O5r30m"
   },
   "source": [
    "**Task** Make a classifier on lemmatized texts.\n",
    "\n",
    "An easier way to normalize words is to use stemming. It is a little dull, does not take into account the context, but sometimes it turns out to be even more effective than lemmatization - and, most importantly, faster.\n",
    "\n",
    "In essence, this is just a set of rules how to cut a word to get a stem (stem):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cr0w_hVyrqFx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "becom\n",
      "becom\n",
      "becam\n"
     ]
    }
   ],
   "source": [
    "from nltk import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('become'))\n",
    "print(stemmer.stem('becomes'))\n",
    "print(stemmer.stem('became'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 01.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
