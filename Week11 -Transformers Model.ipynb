{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OE7fXh-OSJYF"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch==0.4.1\n",
    "!pip -qq install torchtext==0.3.1\n",
    "!pip install sacremoses==0.0.5\n",
    "!wget -O news.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1hIVVpBqM6VU4n3ERkKq4tFaH4sKN0Hab\"\n",
    "!unzip news.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhvfH55PUJ8K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txWqIO_74A4s"
   },
   "source": [
    "# Abstactive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJ7JQJMO2R7z"
   },
   "source": [
    "The task is to generate an excerpt from the text.\n",
    "\n",
    "For example, let's try to generate headlines on the news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzNGUFOcXMUs"
   },
   "outputs": [],
   "source": [
    "!shuf -n 10 news.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOVlO5_Qlg5y"
   },
   "source": [
    "Token them. We will use a single dictionary for text and headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsOvtO0fpCHa"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, Example, Dataset, BucketIterator\n",
    "\n",
    "BOS_TOKEN = '<s>'\n",
    "EOS_TOKEN = '</s>'\n",
    "\n",
    "word_field = Field(tokenize='moses', init_token=BOS_TOKEN, eos_token=EOS_TOKEN, lower=True)\n",
    "fields = [('source', word_field), ('target', word_field)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VO-gix7yoBjg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = pd.read_csv('news.csv', delimiter=',')\n",
    "\n",
    "examples = []\n",
    "for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    source_text = word_field.preprocess(row.text)\n",
    "    target_text = word_field.preprocess(row.title)\n",
    "    examples.append(Example.fromlist([source_text, target_text], fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8uCsMEglm6V"
   },
   "source": [
    "Построим датасеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOBgLAgVTrk1"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(examples, fields)\n",
    "\n",
    "train_dataset, test_dataset = dataset.split(split_ratio=0.85)\n",
    "\n",
    "print('Train size =', len(train_dataset))\n",
    "print('Test size =', len(test_dataset))\n",
    "\n",
    "word_field.build_vocab(train_dataset, min_freq=7)\n",
    "print('Vocab size =', len(word_field.vocab))\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    datasets=(train_dataset, test_dataset), batch_sizes=(16, 32), shuffle=True, device=DEVICE, sort=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHQGgoit2ljv"
   },
   "source": [
    "## Seq2seq for Abstractive Summarization\n",
    "\n",
    "In general, the task is not much different from machine translation:\n",
    "<center>\n",
    "<img src = \"https://image.ibb.co/jAf3S0/2018-11-20-9-42-17.png\" width = \"25%\">\n",
    "</center>\n",
    "    \n",
    "* From [Get To The Point: Summarization With Pointer-Generator Networks] (https://arxiv.org/pdf/1704.04368.pdf) *\n",
    "\n",
    "Here at each step the decoder spies on all tokens - more precisely, their embeddings after BiRNN.\n",
    "\n",
    "The question arises - why even RNN, if then we still look at everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FYJe2CA8GcY"
   },
   "source": [
    "# Transformer\n",
    "\n",
    "From this idea - the rejection of the RNN - and turned Transformer.\n",
    "<center>\n",
    "<img src = \"https://hsto.org/webt/59/f0/44/59f04410c0e56192990801.png\" width=\"20%\">\n",
    "</center>\n",
    "* From Attention is all you need *\n",
    "\n",
    "As in the case of RNN, at each step we apply the same operation (LSTM cell) to the current input, and here - only now there are no connections between timestamps and we can process them almost in parallel.\n",
    "\n",
    "* The code further relies heavily on a smart article [The Annotated Transformer] (http://nlp.seas.harvard.edu/2018/04/03/attention.html). *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "env8rDS_dM86"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "Let's start with the encoder:\n",
    "<center>\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_resideual_layer_norm.png\" width = \"20%\">\n",
    "</center>\n",
    "* From [Illustrated Transformer] (http://jalammar.github.io/illustrated-transformer/) *\n",
    "\n",
    "It is a sequence of identical blocks with self-attention + fully connected layers.\n",
    "\n",
    "You can imagine that this is an LSTM cell: it also applies to each input with the same weights. The main difference is in the absence of recurrent links: due to this, the encoder can be applied simultaneously to all inputs of the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rXjWcnpCJY92"
   },
   "source": [
    "### Positional Encoding\n",
    "\n",
    "It is necessary to somehow encode information about where in the sentence the token is placed. Dudes suggested doing so:\n",
    "\n",
    "$$ PE_{(pos, 2i)} = sin (pos / 10000^{2i / d _ {\\text {model}}}) $$\n",
    "$$ PE_{(pos, 2i + 1)} = cos (pos / 10000^{2i / d _ {\\text {model}}}) $$\n",
    "\n",
    "where $ (pos, i) $ is the position in the sentence and the index in the hidden vector of dimension to $ d_ {model} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zI2rMiZhJcKX"
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEL9VppyKCBz"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe(torch.zeros(1, 100, 20))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a7OeqTcOdgud"
   },
   "source": [
    "As a result, token embeddings are obtained as the sum of ordinary embedding and embedding positions:\n",
    "<center>\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png\" width=\"20%\">\n",
    "</center>\n",
    "    \n",
    "*From [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rvG4ZQ_icKH"
   },
   "source": [
    "### Residual Connection\n",
    "\n",
    "Let us analyze the block of the encoder - repeating N times the combination of operations in the first figure.\n",
    "\n",
    "The simplest thing here is the residual connection. Instead of the output of an arbitrary function $ F $, its input is added.\n",
    "\n",
    "$$ y = F(x) \\quad \\to \\quad y = F (x) + x $$\n",
    "\n",
    "The idea is that ordinary networks are difficult to make too deep - gradients fade out. And through this residual input $ x $ gradients flow nothing. As a result, in the pictures, thanks to such blocks, it turned out to be done before the layers and improve the quality (see ResNet).\n",
    "\n",
    "Nothing prevents us from doing the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7QLCqNeEjKpD"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self._norm = LayerNorm(size)\n",
    "        self._dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs, sublayer):\n",
    "        return inputs + self._dropout(sublayer(self._norm(inputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHUqCsifmd5e"
   },
   "source": [
    "### Layer Norm\n",
    "\n",
    "Additionally, LayerNorm normalization is applied.\n",
    "\n",
    "** Batch normalization **\n",
    "We did not understand at all, but BatchNorm works like this:\n",
    "$$\\mu_j = \\frac{1}{m}\\sum_{i=1}^{m}x_{ij} \\\\    \\sigma_j^2 = \\frac{1}{m}\\sum_{i=1}^{m}(x_{ij} - \\mu_j)^2 \\\\    \\hat{x}_{ij} = \\frac{x_{ij} - \\mu_j}{\\sqrt{\\sigma_j^2 + \\epsilon}}$$\n",
    "$$y_{ij} = \\gamma \\ \\hat{x}_{ij} + \\beta$$\n",
    "\n",
    "On each batch, these $ \\mu $ and $ \\sigma $ are recalculated, updating statistics. Inferense uses accumulated statistics.\n",
    "\n",
    "Its main drawback is that it does not work well with recurrent networks. To overcome this came up:\n",
    "\n",
    "** Layer normalization **\n",
    "And now we will use slightly different formulas:\n",
    "$$\\mu_i = \\frac{1}{m}\\sum_{j=1}^{m}x_{ij} \\\\    \\sigma_i^2 = \\frac{1}{m}\\sum_{j=1}^{m}(x_{ij} - \\mu_i)^2 \\\\    \\hat{x}_{ij} = \\frac{x_{ij} - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}$$\n",
    "$$y_{ij} = \\gamma \\ \\hat{x}_{ij} + \\beta$$\n",
    "\n",
    "<center>\n",
    "<img src=\"https://image.ibb.co/hjtuX0/layernorm.png\" width=\"20%\">\n",
    "</center>\n",
    "  \n",
    "*From [Weight Normalization and Layer Normalization Explained ](http://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/)*\n",
    "\n",
    "If in BatchNorm, statistics are considered for each feature as averaging over a batch, then now for each entry, averaging over features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q5XLZZ3zrK24"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._gamma = nn.Parameter(torch.ones(features))\n",
    "        self._beta = nn.Parameter(torch.zeros(features))\n",
    "        self._eps = eps\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        <calc it>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrWvtymp2G8o"
   },
   "source": [
    "### Attention\n",
    "\n",
    "The whole Transformer relies on the idea of ​​self-attention. It looks like this:\n",
    "<center>\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\" width=\"20%\">\n",
    "</center>\n",
    "\n",
    "*From [Tensor2Tensor Tutorial](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)*\n",
    "\n",
    "The embedding of the word * it * is constructed as a combination of all the embeddingdings of a sentence.\n",
    "\n",
    "The article came up with the idea of ​​doing this attention:\n",
    "\n",
    "$$ \\mathrm {Attention} (Q, K, V) = \\mathrm {softmax} \\left (\\frac {QK ^ T} {\\sqrt {d_k}} \\right) V $$\n",
    "\n",
    "This is approximately like dot-attention in the last lesson: the request (** Q ** uery) is multiplied by the keys (** K ** ey) scalarly, then the softmax is taken - estimates are obtained of how interesting the different timestamps are from the values ​​(** V ** alue). \n",
    "\n",
    "For example, $ \\mathrm {emb} (\\text {it}) = \\mathrm {Attention} (\\text {it}, \\ldots \\text {because it was too tired}, \\ldots \\text {because it was too tired }) $.\n",
    "\n",
    "Only now with the $ \\frac {1} {\\sqrt {d_k}} $ parameter, where $ d_k $ is the dimension of the key. It is argued that this works better for large key sizes $ d_k $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ApuVJZn5i4R"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        <calc it>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZ-xQbgM6MNl"
   },
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "<center>\n",
    "<img src=\"https://hsto.org/webt/59/f0/44/59f0440f1109b864893781.png\" width=\"20%\">\n",
    "</center>\n",
    "\n",
    "The important idea why attention (and, most importantly, self-attention) has earned is the use of several heads (multi-head).\n",
    "\n",
    "In general, when we make attention - we determine the similarity of the key and the request. Many heads helps (should) determine this similarity by different criteria - syntactically, semantically, etc.\n",
    "\n",
    "For example, in the picture two heads are used and one head looks at * the animal * when generating * it *, the second - at * tired *:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://hsto.org/webt/59/f0/44/59f0440f1109b864893781.png\" width=\"20%\">\n",
    "</center>\n",
    "    \n",
    "*From [Tensor2Tensor Tutorial](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)*\n",
    "\n",
    "\n",
    "It is applied this way:\n",
    "$$\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ...,\n",
    "\\mathrm{head_h})W^O    \\\\\n",
    "    \\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$\n",
    "    \n",
    "Where $W^Q_i \\in \\mathbb{R}^{d_{model} \\times d_k}, W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}, W^V_i \\in \\mathbb{R}^{d_{model} \\times d_v}, W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$.\n",
    "\n",
    "The original article used $ h = 8 $, $ d_k = d_v = d _ {\\text {model}} / h = 64 $.\n",
    "\n",
    "The process of applying this:\n",
    "<center>\n",
    "<img src=\"http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\" width=\"20%\">\n",
    "</center>\n",
    "    \n",
    "*From Illustrated Transformer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rg-CxvPDAJPP"
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, heads_count, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % heads_count == 0\n",
    "\n",
    "        self._d_k = d_model // heads_count\n",
    "        self._heads_count = heads_count\n",
    "        self._attention = ScaledDotProductAttention(dropout_rate)\n",
    "        self._attn_probs = None\n",
    "        \n",
    "        self._w_q = nn.Linear(d_model, d_model)\n",
    "        self._w_k = nn.Linear(d_model, d_model)\n",
    "        self._w_v = nn.Linear(d_model, d_model)\n",
    "        self._w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        <calc it>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pOKwneaKGaJi"
   },
   "source": [
    "### Position-wise Feed-Forward Networks\n",
    "\n",
    "Линейный блок в энкодере выглядит так:\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uh1UVkAUGiwh"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(inputs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_dZoU1JIt6QP"
   },
   "source": [
    "### Encoder block\n",
    "\n",
    "Соберем все в блок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nh7wQL65sBmk"
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._self_attn = self_attn\n",
    "        self._feed_forward = feed_forward\n",
    "        self._self_attention_block = ResidualBlock(size, dropout_rate)\n",
    "        self._feed_forward_block = ResidualBlock(size, dropout_rate)\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        outputs = self._self_attention_block(inputs, lambda inputs: self._self_attn(inputs, inputs, inputs, mask))\n",
    "        return self._feed_forward_block(outputs, self._feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8ndCRZLl4ZZ"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_ff, blocks_count, heads_count, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._emb = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, d_model),\n",
    "            PositionalEncoding(d_model, dropout_rate)\n",
    "        )\n",
    "        \n",
    "        block = lambda: EncoderBlock(\n",
    "            size=d_model, \n",
    "            self_attn=MultiHeadedAttention(heads_count, d_model, dropout_rate), \n",
    "            feed_forward=PositionwiseFeedForward(d_model, d_ff, dropout_rate),\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        self._blocks = nn.ModuleList([block() for _ in range(blocks_count)])\n",
    "        self._norm = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, inputs, mask):\n",
    "        inputs = self._emb(inputs)\n",
    "        \n",
    "        for block in self._blocks:\n",
    "            inputs = block(inputs, mask)\n",
    "        return self._norm(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_pphRcbTvqnq"
   },
   "source": [
    "## Decoder\n",
    "\n",
    "<center>\n",
    "<img src=\"https://hsto.org/webt/59/f0/44/59f0440f7d88f805415140.png\" width=\"10%\">\n",
    "</center>\n",
    "\n",
    "The decoder block (gray part) consists of three parts:\n",
    "1. First - the same self-attention as in the encoder\n",
    "2. Then - the standard attention to the outputs from the encoder + the current state of the decoder (it was the same in seq2seq with attention)\n",
    "3. Finally - feed-forward block\n",
    "\n",
    "All this, of course, with residual links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6LTWjKUXx2LP"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, encoder_attn, feed_forward, dropout_rate):\n",
    "        super().__init__()\n",
    "                \n",
    "        self._self_attn = self_attn\n",
    "        self._encoder_attn = encoder_attn\n",
    "        self._feed_forward = feed_forward\n",
    "        self._self_attention_block = ResidualBlock(size, dropout_rate)\n",
    "        self._attention_block = ResidualBlock(size, dropout_rate)\n",
    "        self._feed_forward_block = ResidualBlock(size, dropout_rate)\n",
    " \n",
    "    def forward(self, inputs, encoder_output, source_mask, target_mask):\n",
    "        outputs = self._self_attention_block(\n",
    "            inputs, lambda inputs: self._self_attn(inputs, inputs, inputs, target_mask)\n",
    "        )\n",
    "        outputs = self._attention_block(\n",
    "            outputs, lambda inputs: self._encoder_attn(inputs, encoder_output, encoder_output, source_mask)\n",
    "        )\n",
    "        return self._feed_forward_block(outputs, self._feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Un0AOmdqLPp_"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_ff, blocks_count, heads_count, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._emb = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, d_model),\n",
    "            PositionalEncoding(d_model, dropout_rate)\n",
    "        )\n",
    "        \n",
    "        block = lambda: DecoderLayer(\n",
    "            size=d_model, \n",
    "            self_attn=MultiHeadedAttention(heads_count, d_model, dropout_rate),\n",
    "            encoder_attn=MultiHeadedAttention(heads_count, d_model, dropout_rate),\n",
    "            feed_forward=PositionwiseFeedForward(d_model, d_ff, dropout_rate),\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        self._blocks = nn.ModuleList([block() for _ in range(blocks_count)])\n",
    "        self._norm = LayerNorm(d_model)\n",
    "        self._out_layer = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs, encoder_output, source_mask, target_mask):\n",
    "        inputs = self._emb(inputs)\n",
    "        for block in self._blocks:\n",
    "            inputs = block(inputs, encoder_output, source_mask, target_mask)\n",
    "        return self._out_layer(self._norm(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbEkHtwWz0Yh"
   },
   "source": [
    "В декодере нужно аттентиться только на предыдущие токены - сгенерируем маску для этого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbVkZSRq0cD5"
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    mask = torch.ones(size, size, device=DEVICE).triu_()\n",
    "    return mask.unsqueeze(0) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8X_fqeL0jeW"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7UEsntVj9lb"
   },
   "source": [
    "## Полная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLIGjPOiO7X9"
   },
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size, d_model=256, d_ff=1024, \n",
    "                 blocks_count=4, heads_count=8, dropout_rate=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.encoder = Encoder(source_vocab_size, d_model, d_ff, blocks_count, heads_count, dropout_rate)\n",
    "        self.decoder = Decoder(target_vocab_size, d_model, d_ff, blocks_count, heads_count, dropout_rate)\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        \n",
    "    def forward(self, source_inputs, target_inputs, source_mask, target_mask):\n",
    "        encoder_output = self.encoder(source_inputs, source_mask)\n",
    "        return self.decoder(target_inputs, encoder_output, source_mask, target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmucSaUOjlmh"
   },
   "outputs": [],
   "source": [
    "def make_mask(source_inputs, target_inputs, pad_idx):\n",
    "    source_mask = (source_inputs != pad_idx).unsqueeze(-2)\n",
    "    target_mask = (target_inputs != pad_idx).unsqueeze(-2)\n",
    "    target_mask = target_mask & subsequent_mask(target_inputs.size(-1)).type_as(target_mask)\n",
    "    return source_mask, target_mask\n",
    "\n",
    "\n",
    "def convert_batch(batch, pad_idx=1):\n",
    "    source_inputs, target_inputs = batch.source.transpose(0, 1), batch.target.transpose(0, 1)\n",
    "    source_mask, target_mask = make_mask(source_inputs, target_inputs, pad_idx)\n",
    "    \n",
    "    return source_inputs, target_inputs, source_mask, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9iWSl6m6jfbl"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_qVuSL8QJg4"
   },
   "outputs": [],
   "source": [
    "model = FullModel(source_vocab_size=len(word_field.vocab), target_vocab_size=len(word_field.vocab)).to(DEVICE)\n",
    "\n",
    "model(*convert_batch(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vX6PrVksnaq7"
   },
   "source": [
    "## Оптимизатор\n",
    "\n",
    "Тоже очень важно в данной модели - использовать правильный оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMhopCgTnh-w"
   },
   "outputs": [],
   "source": [
    "class NoamOpt(object):\n",
    "    def __init__(self, model_size, factor=2, warmup=4000, optimizer=None):\n",
    "        if optimizer is not None:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuYc21J5oIdb"
   },
   "source": [
    "Идея в том, чтобы повышать learning rate в течении первых warmup шагов линейно, а затем понижать его по сложной формуле:\n",
    "\n",
    "$$lrate = d_{\\text{model}}^{-0.5} \\cdot\n",
    "  \\min({step\\_num}^{-0.5},\n",
    "    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMBL261hoA58"
   },
   "outputs": [],
   "source": [
    "opts = [NoamOpt(512, 1, 4000, None), \n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W71i85Q4pdOS"
   },
   "source": [
    "## Тренировка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_E2JxfRuphch"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.get_lock().locks = []\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data_iter, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = len(data_iter)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                source_inputs, target_inputs, source_mask, target_mask = convert_batch(batch)                                \n",
    "                logits = model.forward(source_inputs, target_inputs[:, :-1], source_mask, target_mask[:, :-1, :-1])\n",
    "                \n",
    "                logits = logits.contiguous().view(-1, logits.shape[-1])\n",
    "                target = target_inputs[:, 1:].contiguous().view(-1)\n",
    "                loss = criterion(logits, target)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
    "                                                                                         math.exp(loss.item())))\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
    "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
    "            )\n",
    "            progress_bar.refresh()\n",
    "\n",
    "    return epoch_loss / batches_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_iter, epochs_count=1, val_iter=None):\n",
    "    best_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss = do_epoch(model, criterion, train_iter, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            val_loss = do_epoch(model, criterion, val_iter, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5X2kYDU_rCjP"
   },
   "outputs": [],
   "source": [
    "model = FullModel(source_vocab_size=len(word_field.vocab), target_vocab_size=len(word_field.vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = word_field.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(DEVICE)\n",
    "\n",
    "optimizer = NoamOpt(model.d_model)\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRMFkzDV-wI9"
   },
   "source": [
    "** Task ** Add a generator for the model.\n",
    "\n",
    "** Task ** Add a rating for the model using the ROUGE metric (for example, from the package https://pypi.project/pyrouge/0.1.3/)\n",
    "\n",
    "** Task ** Add visualization (can be peeped in the code by links)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ij1GiNx54Ke5"
   },
   "source": [
    "## Model improvements\n",
    "\n",
    "** Task ** Try to share the matrix of embeddings - there are three of them (input to the encoder and decoder + decoder output).\n",
    "\n",
    "** Task ** Change Loss to LabelSmoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "goFvQj18--iP"
   },
   "source": [
    "# Pointer-Generator Networks\n",
    "\n",
    "A cool idea specific to self-freezing:\n",
    "<center>\n",
    "<img src = \"https://image.ibb.co/eijTc0/2018-11-20-10-18-52.png\" width = \"25%\">\n",
    "</center>\n",
    "\n",
    "** Task ** Try to implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4-3pYqVJIKA"
   },
   "source": [
    "# Referrence\n",
    "Attention Is All You Need, 2017 [[pdf]](https://arxiv.org/pdf/1706.03762.pdf)  \n",
    "Get To The Point: Summarization with Pointer-Generator Networks, 2017 [[pdf]](https://arxiv.org/pdf/1704.04368.pdf)  \n",
    "Universal Transformers, 2018 [[arxiv]](https://arxiv.org/abs/1807.03819)\n",
    "\n",
    "[Transformer — новая архитектура нейросетей для работы с последовательностями](https://habr.com/post/341240/)  \n",
    "[The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)  \n",
    "[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)  \n",
    "[Weighted Tranformer](https://einstein.ai/research/blog/weighted-transformer)  \n",
    "[Your tldr by an ai: a deep reinforced model for abstractive summarization](https://einstein.ai/research/blog/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 11 - Transformers.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
