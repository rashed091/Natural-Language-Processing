{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dF1fio53UKN6"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3lN5pl5Stpp"
   },
   "source": [
    "# Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9J6KBv-cniLw"
   },
   "source": [
    "## Surname classification\n",
    "\n",
    "We will learn to predict whether a word is a surname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>nationality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Woodford</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coté</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kore</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Koury</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lebzak</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    surname nationality\n",
       "0  Woodford     English\n",
       "1      Coté      French\n",
       "2      Kore     English\n",
       "3     Koury      Arabic\n",
       "4    Lebzak     Russian"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('.data/surnames.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(df['surname'], df['nationality'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YN1XYen8UauD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ponidelko', 'Russian'),\n",
       " ('Wruck', 'German'),\n",
       " ('Ahmad', 'English'),\n",
       " ('Wakelin', 'English'),\n",
       " ('Oneil', 'English')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(train_data, train_labels))[::1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uFlOaCNt3fuV"
   },
   "source": [
    "The data is also very balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ikKB2DaK08-n"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "positive_count = np.sum(train_labels == 1)\n",
    "negative_count = len(train_labels) - positive_count\n",
    " \n",
    "plt.bar(np.arange(2), [negative_count, positive_count], align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(2), ('Negative', 'Positive'))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UuqFWWu38UwV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.00%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy = {:.2%}'.format((train_labels == 0).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LtOFSmy93pDl"
   },
   "source": [
    "However, it will be rather useless to always say that the word is not a surname. This, of course, is a question - what is worse, in vain to declare a word a last name (a mistake of the first kind) or not to find a last name.\n",
    "\n",
    "<img src=\"https://effectsizefaq.files.wordpress.com/2010/05/type-i-and-type-ii-errors.jpg\" style=\"border:none;width:35%\">\n",
    "\n",
    "We will measure precision, recall and their combination - $ F_1 $ -measure.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png\" style=\"border:none;width:25%\">\n",
    "\n",
    "$$\\text{precision} = \\frac{tp}{tp + fp}.$$\n",
    "$$\\text{recall} = \\frac{tp}{tp + fn}.$$\n",
    "$$\\text{F}_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9bl6O_bAtVqd"
   },
   "source": [
    "Let's start with a baseline on the regulars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJJtiGMK1ATy"
   },
   "source": [
    "And now seriously - a baseline on logistic regression over N-gram characters.\n",
    "\n",
    "**Assignment** Make a classification with the LogisticRegression model. Calculate the F1-measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8COAoh7b0TXs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "      ...imators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)\n",
    "classifier = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "model.fit(df['surname'], df['nationality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 96.08%\n"
     ]
    }
   ],
   "source": [
    "# test_preds = model.predict(df['surname'])\n",
    "# print('F1-score = {:.2%}'.format(f1_score(df['nationality'], test_preds)))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "preds = model.predict(test_data)\n",
    "print('Test accuracy = {:.2%}'.format(accuracy_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vN46GwPrzeJW"
   },
   "source": [
    "Look at the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-P8OSD7yZQt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0134\n",
       "                \n",
       "                    &plusmn; 0.0057\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                a\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 81.52%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0119\n",
       "                \n",
       "                    &plusmn; 0.0083\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                e\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 82.41%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0111\n",
       "                \n",
       "                    &plusmn; 0.0155\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                v\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 82.84%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0107\n",
       "                \n",
       "                    &plusmn; 0.0212\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ov\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 83.45%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0102\n",
       "                \n",
       "                    &plusmn; 0.0039\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                i\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 83.47%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0102\n",
       "                \n",
       "                    &plusmn; 0.0137\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ev\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 84.10%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0096\n",
       "                \n",
       "                    &plusmn; 0.0054\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                n\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 85.42%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0085\n",
       "                \n",
       "                    &plusmn; 0.0032\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                o\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 85.77%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0082\n",
       "                \n",
       "                    &plusmn; 0.0071\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                h\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.25%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0078\n",
       "                \n",
       "                    &plusmn; 0.0026\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                s\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 86.41%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0077\n",
       "                \n",
       "                    &plusmn; 0.0033\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                r\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.34%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0070\n",
       "                \n",
       "                    &plusmn; 0.0030\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                t\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.10%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0064\n",
       "                \n",
       "                    &plusmn; 0.0025\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                l\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.20%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0063\n",
       "                \n",
       "                    &plusmn; 0.0045\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                k\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.28%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0062\n",
       "                \n",
       "                    &plusmn; 0.0025\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                u\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.04%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0057\n",
       "                \n",
       "                    &plusmn; 0.0104\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ky\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.30%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0055\n",
       "                \n",
       "                    &plusmn; 0.0042\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                y\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0052\n",
       "                \n",
       "                    &plusmn; 0.0058\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ll\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 89.77%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0051\n",
       "                \n",
       "                    &plusmn; 0.0030\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                d\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.14%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0049\n",
       "                \n",
       "                    &plusmn; 0.0105\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                kov\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.15%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0049\n",
       "                \n",
       "                    &plusmn; 0.0061\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                on\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.22%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0048\n",
       "                \n",
       "                    &plusmn; 0.0043\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                er\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0045\n",
       "                \n",
       "                    &plusmn; 0.0046\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                in\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.07%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0042\n",
       "                \n",
       "                    &plusmn; 0.0016\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                m\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.14%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0042\n",
       "                \n",
       "                    &plusmn; 0.0077\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ou\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.18%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0041\n",
       "                \n",
       "                    &plusmn; 0.0030\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                S\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.42%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0040\n",
       "                \n",
       "                    &plusmn; 0.0027\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                z\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.88%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0037\n",
       "                \n",
       "                    &plusmn; 0.0027\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ha\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.95%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0036\n",
       "                \n",
       "                    &plusmn; 0.0085\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ey\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.09%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0035\n",
       "                \n",
       "                    &plusmn; 0.0017\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                K\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.17%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0035\n",
       "                \n",
       "                    &plusmn; 0.0019\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                c\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.36%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0034\n",
       "                \n",
       "                    &plusmn; 0.0019\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                A\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.44%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0033\n",
       "                \n",
       "                    &plusmn; 0.0021\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                g\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.45%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0033\n",
       "                \n",
       "                    &plusmn; 0.0012\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                M\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.46%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0033\n",
       "                \n",
       "                    &plusmn; 0.0031\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                an\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.78%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0031\n",
       "                \n",
       "                    &plusmn; 0.0048\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ko\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.78%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0031\n",
       "                \n",
       "                    &plusmn; 0.0088\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                nov\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.80%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0031\n",
       "                \n",
       "                    &plusmn; 0.0030\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                f\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.82%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0031\n",
       "                \n",
       "                    &plusmn; 0.0020\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ar\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.85%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0031\n",
       "                \n",
       "                    &plusmn; 0.0025\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                b\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 92.85%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 7992 more &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "\n",
    "eli5.show_weights(model, vec=vectorizer, top=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYXljseDyd77"
   },
   "outputs": [],
   "source": [
    "# eli5.show_prediction(model, df[10], vec=vectorizer, targets=['surname'], target_names=['word', 'surname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQF3JlD7twQK"
   },
   "source": [
    "In addition to the blunt counting of the F1-score, you can look at precision-recall curves. First, they are beautiful. Secondly, it is clear from them that you can improve the quality (F1-score) by selecting another threshold - ** although it is impossible to do this on the test **."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9187054759579859"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "precision_score(test_labels, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8978852714706169"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_labels, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7oyvJ-2EG0m"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# precision, recall, _ = precision_recall_curve(df['surname'], model.predict_proba(df['nationality'])[:, 1])\n",
    "\n",
    "# plt.figure(figsize=(7, 7))\n",
    "# f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "# lines = []\n",
    "# labels = []\n",
    "# for f_score in f_scores:\n",
    "#     x = np.linspace(0.01, 1)\n",
    "#     y = f_score * x / (2 * x - f_score)\n",
    "#     l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "#     plt.annotate('F1 = {0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "# plt.plot(recall, precision)\n",
    "\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Precision-Recall Curve')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tL6Bnl8ftoYP"
   },
   "source": [
    "**Task** Come up with signs to improve the quality of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rRhcCJcAS4S5"
   },
   "source": [
    "## Character-Level Convolutions\n",
    "\n",
    "### General description of convolutions\n",
    "\n",
    "Let me remind you that convolutions are what started the HYIP of neural networks in the area of ​​the 2012th.\n",
    "\n",
    "They work like this:\n",
    "<img src=\"https://image.ibb.co/e6t8ZK/Convolution.gif\" width=\"50%\">\n",
    "\n",
    "From [Feature extraction using convolution](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution).\n",
    "\n",
    "Formally - filter sets are learned, each of which is scalar multiplied by elements of the matrix of attributes. In the picture above, the original matrix collapses with the filter.\n",
    "$$\n",
    " \\begin{pmatrix}\n",
    "  1 & 0 & 1 \\\\\n",
    "  0 & 1 & 0 \\\\\n",
    "  1 & 0 & 1\n",
    " \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "But we must not forget that convolutions usually have another dimension, such as the number of channels. For example, pictures usually have three channels: RGB.\n",
    "It clearly demonstrates how filters look like [here] (http://cs231n.github.io/convolutional-networks/#conv).\n",
    "\n",
    "After convolutions, pooling layers usually follow. They help reduce the dimension of the tensor with which to work. The most common is max-pooling:\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/maxpool.jpeg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-M3lCE1ealB"
   },
   "source": [
    "### Text wrappers\n",
    "\n",
    "For convolution texts, they work as n-gram detectors (approximately). A canonical example of a symbolic convolution network:\n",
    "\n",
    "<img src=\"https://image.ibb.co/bC3Xun/2018_03_27_01_24_39.png\" width=\"50%\">\n",
    "\n",
    "From [Character-Aware Neural Language Models](https://arxiv.org/abs/1508.06615)\n",
    "\n",
    "* How many filters does the example use? *\n",
    "\n",
    "The picture shows how 2, 3, and 4 grams are extracted from a word. For example, yellow is trigrams. A yellow filter is applied to all trigrams in a word, and then the strongest signal is extracted using global max-pooling.\n",
    "\n",
    "What does this mean, if specifically?\n",
    "\n",
    "Each character is displayed using embeddings in some vector. And their sequences are in the concatenation of embeddings.\n",
    "\n",
    "For example, \"abs\" $ \\ to [v_a; v_b; v_s] \\in \\mathbb {R} ^ {3 d} $, where $ d $ is the embedding dimension. The yellow filter $ f_k $ has the same dimension $ 3d $.\n",
    "\n",
    "Its attachment is the scalar product $ \\left ([v_a; v_b; v_s] \\odot f_k \\right) \\in \\mathbb R $ (one of the yellow squares in the feature map for this filter).\n",
    "\n",
    "Max-pooling selects $ max_i \\left ([v_ {i-1}; v_ {i}; v_ {i + 1}] \\odot f_k \\right) $, where $ i $ runs over all indexes of the word from 1 to $ | w | - $ 1 (or over a larger range, if there are paddings).\n",
    "This maximum corresponds to the trigram that is closest to the filter over the cosine distance.\n",
    "\n",
    "As a result, after max-pooling, the vector encodes information about which of the n-grams met in the word: if a trigram close to our $ f_k $ met, then the vector will have a great value in the $ k $ position of the vector, otherwise small\n",
    "\n",
    "And we just learn the filters. That is, the network must learn to determine which of the n-grams are significant and which are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xz7ApwLZ05kc"
   },
   "source": [
    "### Toy example\n",
    "\n",
    "Let's look at an example of what is happening there. Take the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7fbpo630iZa"
   },
   "outputs": [],
   "source": [
    "word = 'Hello Word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x99Q65ot02Vi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'H': 0, 'o': 1, 'l': 2, 'e': 3, 'd': 4, 'r': 5, 'W': 6, ' ': 7}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2index = {symb: ind for ind, symb in enumerate(set(word))}\n",
    "\n",
    "char2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sxJYZEs1M8y"
   },
   "source": [
    "Each character is associated with embedding. The easiest way to do embeddingings is to take a single matrix. When we had tens of thousands of words, such embeddings were not very good, but now only a few characters are quite adequate to assign them orthogonal vectors of small dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbApeJdb1EGq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.eye(len(char2index))\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mP9i5pPx2BGs"
   },
   "source": [
    "Construct the tensor of the indexes of the characters of the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBIIlHy91ohP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 2, 2, 1, 7, 6, 1, 5, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tensor = torch.LongTensor([char2index[symb] for symb in word])\n",
    "\n",
    "word_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eL3qLw-p2Htk"
   },
   "source": [
    "Let's display it in embeddings. Got the same rectangle as in the picture (transposition is necessary to look in the same direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fkChxtt1yqY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embs = embeddings[word_tensor].t()\n",
    "\n",
    "word_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QL4o1EI-2O_I"
   },
   "source": [
    "Now it came to convolutions. Let's make the filter-detector of the trigram `new`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ToW8KCUY1723"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_name = 'Hel'\n",
    "\n",
    "kernel_indices = torch.LongTensor([char2index[symb] for symb in kernel_name])\n",
    "kernel_weights = embeddings[kernel_indices].t()\n",
    "\n",
    "kernel_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kTxfchOy4c9w"
   },
   "source": [
    "To calculate the convolution, use the function:\n",
    "```python\n",
    "F.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n",
    "```\n",
    "\n",
    "input: input tensor of shape ($N \\times C_{in} \\times H_{in} \\times W_{in}$)  \n",
    "weight: filters of shape ($C_{out} \\times C_{in} \\times H_{out} \\times W_{out}$)\n",
    "\n",
    "$ N $ - the size of the batch (1 for us). $ C_ {in} $ - the number of channels. In our case, it will always be 1 (for now). $ C_ {out} $ - the number of filters. It is still 1.\n",
    "\n",
    "We will need four-dimensional tensors, for this we use `view`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embs = word_embs.view(1, 1, word_embs.shape[0], word_embs.shape[1])\n",
    "word_embs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oyd3Na7V3JMr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv = tensor([[[[3., 1., 0., 0., 0., 0., 0., 0.]]]])\n",
      "Max pooling = tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "kernel_weights = kernel_weights.view(1, 1, kernel_weights.shape[0], kernel_weights.shape[1])\n",
    "\n",
    "conv_result = F.conv2d(word_embs, kernel_weights)[0, 0]\n",
    "\n",
    "print('Conv =', conv_result)\n",
    "print('Max pooling =', conv_result.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVAiQ2PG6SOd"
   },
   "source": [
    "Convolution said that this filter is in the last position. Puling said, do not care what position - the main thing, he is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7ZVbNOGoV5z"
   },
   "source": [
    "### Data preparation\n",
    "\n",
    "The first step is to determine how long our words are. We confine ourselves to some number, and the longer ones will be cut off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEtZ6g78Wtj-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word length for 99% of words is 12\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "    \n",
    "def find_max_len(counter, threshold):\n",
    "    sum_count = sum(counter.values())\n",
    "    cum_count = 0\n",
    "    for i in range(max(counter)):\n",
    "        cum_count += counter[i]\n",
    "        if cum_count > sum_count * threshold:\n",
    "            return i\n",
    "    return max(counter)\n",
    "\n",
    "word_len_counter = Counter()\n",
    "for word in train_data:\n",
    "    word_len_counter[len(word)] += 1\n",
    "    \n",
    "threshold = 0.99\n",
    "MAX_WORD_LEN = find_max_len(word_len_counter, threshold)\n",
    "\n",
    "print('Max word length for {:.0%} of words is {}'.format(threshold, MAX_WORD_LEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MdKoOBdE8uj4"
   },
   "source": [
    "Let's collect a mapping from characters to indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YyMoPEXGVs3s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R': 1, 'ü': 2, 'q': 3, 'ú': 4, 'V': 5, 'M': 6, 'ã': 7, 'C': 8, 'Ż': 9, 'Á': 10, 'h': 11, ':': 12, 'F': 13, 'N': 14, 'S': 15, 'ö': 16, 'ç': 17, 'I': 18, 'ì': 19, 'A': 20, 'è': 21, 'u': 22, 'ñ': 23, '/': 24, 'k': 25, 'f': 26, 'v': 27, 't': 28, 'E': 29, 'Z': 30, 'ń': 31, 'X': 32, 'y': 33, 'g': 34, 'c': 35, 'm': 36, 'W': 37, 'e': 38, 'ò': 39, 'L': 40, 'O': 41, 'ê': 42, 'z': 43, 'à': 44, 'Q': 45, 'É': 46, 'b': 47, 'j': 48, 'K': 49, 'D': 50, 'ó': 51, 'õ': 52, '1': 53, 'P': 54, 'é': 55, 'B': 56, 'w': 57, 'ä': 58, 'o': 59, 'H': 60, 'r': 61, 'á': 62, 'Ś': 63, \"'\": 64, 'i': 65, 'n': 66, 'G': 67, 's': 68, 'T': 69, '-': 70, 'Y': 71, 'x': 72, 'l': 73, 'd': 74, 'U': 75, 'a': 76, 'J': 77, 'í': 78, 'ß': 79, 'p': 80, '<pad>': 0}\n"
     ]
    }
   ],
   "source": [
    "chars = set()\n",
    "for word in train_data:\n",
    "    chars.update(word)\n",
    "\n",
    "char_index = {c : i + 1 for i, c in enumerate(chars)}\n",
    "char_index['<pad>'] = 0\n",
    "    \n",
    "print(char_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4spzGF2CImtL"
   },
   "source": [
    "**Task** Convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qETmYKm8W_TX"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, max_word_len, char_index):\n",
    "    return <np array>\n",
    "\n",
    "X_train = convert_data(train_data, MAX_WORD_LEN, char_index)\n",
    "X_test = convert_data(test_data, MAX_WORD_LEN, char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VufrP006Vk-y"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "        \n",
    "        batch_idx = indices[start: end]\n",
    "        \n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkXlKB7lobYE"
   },
   "source": [
    "### ConvCharNN\n",
    "\n",
    "Now we build a convolutional model.\n",
    "\n",
    "Let it build trigrams - that is, apply filters for 3 characters.\n",
    "\n",
    "Let's start with the sequence: `nn.Embedding -> nn.Conv2d -> nn.ReLU -> max pooling -> nn.Linear`\n",
    "\n",
    "`nn.Conv2d` is the layer containing the creation and initialization of filters, and calling` F.conv2d` to them and the input.\n",
    "\n",
    "* Life hacking: sequences of operations can be packed in `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2T4AorsZ530"
   },
   "outputs": [],
   "source": [
    "class ConvClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, filters_count):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._embedding = ...\n",
    "        self._dropout = nn.Dropout(0.2)\n",
    "        self._conv3 = ...\n",
    "        self._out_layer = ...\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        inputs - LongTensor with shape (batch_size, max_word_len)\n",
    "        outputs - FloatTensor with shape (batch_size,)\n",
    "        '''\n",
    "        \n",
    "        outputs = self.embed(inputs)\n",
    "        return self._out_layer(outputs).squeeze(-1)\n",
    "    \n",
    "    def embed(self, inputs):\n",
    "        <calc word embedding>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iTDc9rEAHN6"
   },
   "source": [
    "Проверьте, что всё работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WiAdrSaU_wjQ"
   },
   "outputs": [],
   "source": [
    "X_batch, y_batch = next(iterate_batches(X_train, train_labels, 32))\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "model = ConvClassifier(len(char_index) + 1, 24, 64)\n",
    "logits = model(X_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x39p9w-tA_Ds"
   },
   "source": [
    "**Задание** Подсчитайте precision, recall и F1-score для полученных предсказаний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVg2Ws7mA40a"
   },
   "outputs": [],
   "source": [
    "<calc precision, recall, f1-score>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1TFsKlF9BV8X"
   },
   "source": [
    "**Задание** Напишем теперь цикл обучения, который не слишком сложно будет переиспользовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MsCtTJucVjMH"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None):\n",
    "    epoch_loss, epoch_tp, epoch_fp, epoch_fn = 0, 0, 0, 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    model.train(is_train)\n",
    "    \n",
    "    data, labels = data\n",
    "    batchs_count = math.ceil(data.shape[0] / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (X_batch, y_batch) in enumerate(iterate_batches(data, labels, batch_size)):\n",
    "            X_batch, y_batch = LongTensor(X_batch), FloatTensor(y_batch)\n",
    "\n",
    "            logits = <calc logits>\n",
    "\n",
    "            loss = <calc loss>\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if is_train:\n",
    "                <how to optimize the beast?>\n",
    "\n",
    "            <u can move the stuff to some function>\n",
    "            tp = <calc true positives>\n",
    "            fp = <calc false positives>\n",
    "            fn = <calc false negatives>\n",
    "\n",
    "            precision = ...\n",
    "            recall = ...\n",
    "            f1 = ...\n",
    "            \n",
    "            epoch_tp += tp\n",
    "            epoch_fp += fp\n",
    "            epoch_fn += fn\n",
    "\n",
    "            print('\\r[{} / {}]: Loss = {:.4f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'.format(\n",
    "                  i, batchs_count, loss.item(), precision, recall, f1), end='')\n",
    "        \n",
    "    precision = ...\n",
    "    recall = ...\n",
    "    f1 = ...\n",
    "        \n",
    "    return epoch_loss / batchs_count, recall, precision, f1\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_recall, train_precision, train_f1 = do_epoch(\n",
    "            model, criterion, train_data, batch_size, optimizer\n",
    "        )\n",
    "        \n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'\n",
    "        if not val_data is None:\n",
    "            val_loss, val_recall, val_precision, val_f1 = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, \n",
    "                                     train_loss, train_recall, train_precision, train_f1,\n",
    "                                     val_loss, val_recall, val_precision, val_f1))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlq63hAXh0Gr"
   },
   "outputs": [],
   "source": [
    "model = ConvClassifier(len(char_index) + 1, 24, 128).cuda()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad])\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, train_labels), epochs_count=200, \n",
    "    batch_size=512, val_data=(X_test, test_labels), val_batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4o-1AEcFCjAk"
   },
   "source": [
    "**Task** Check the classifier on your last name.\n",
    "\n",
    "It is necessary not to forget to transfer the model to the inference mode - some layers on the train and inference behave differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "svO9OrF4CiLI"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "surname = \"...\"\n",
    "surname_tensor = ...\n",
    "print('P({} is surname) = {}'.format(surname, torch.sigmoid(model(surname_tensor))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djkksZKcDPA1"
   },
   "source": [
    "**Задание** Постройте precision-recall curve для данного классификатора и предыдущей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZm7O56pDcH_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FPTfBzSDheP"
   },
   "source": [
    "### Embeddingd visualization\n",
    "\n",
    "**Assignment** Visualize word embeddings, as was done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DeW5av_ASrn4"
   },
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    output_notebook()\n",
    "    \n",
    "    if isinstance(color, str): \n",
    "        color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: \n",
    "        pl.show(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def get_tsne_projection(word_vectors):\n",
    "    tsne = TSNE(n_components=2, verbose=100)\n",
    "    return scale(tsne.fit_transform(word_vectors))\n",
    "    \n",
    "    \n",
    "def visualize_embeddings(embeddings, token, colors):\n",
    "    tsne = get_tsne_projection(embeddings)\n",
    "    draw_vectors(tsne[:, 0], tsne[:, 1], color=colors, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67KNX5lBTdrt"
   },
   "outputs": [],
   "source": [
    "word_indices = np.random.choice(np.arange(len(test_data)), 1000, replace=False)\n",
    "words = [test_data[ind] for ind in word_indices]\n",
    "labels = test_labels[word_indices]\n",
    "\n",
    "word_tensor = convert_data(words, max(len(x) for x in words), char_index)\n",
    "embeddings = <calc embeddings>\n",
    "\n",
    "colors = ['red' if label else 'blue' for label in labels]\n",
    "\n",
    "visualize_embeddings(embeddings, words, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xOt1tcOkvDBY"
   },
   "source": [
    "### Visualization of the received bundles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tljoUWceE2lH"
   },
   "source": [
    "Among other things, we have a logistic regression from above. You can visualize it as in eli5.\n",
    "\n",
    "**Assignment** Achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJpHT_YkLvCf"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "word = 'Смирнов'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LNMXpgh8FNIo"
   },
   "source": [
    "Count the probability that the word is a surname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hl-VZs1mFKxM"
   },
   "outputs": [],
   "source": [
    "inputs = word -> LongTensor\n",
    "prob = torch.sigmoid(model(inputs)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2SZi1FIFj6o"
   },
   "source": [
    "Calculate the result of convolution and pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-1SdKbDFFhOk"
   },
   "outputs": [],
   "source": [
    "convs = ...\n",
    "maxs, positions = convs.squeeze().max(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JLTu8mlfFtXJ"
   },
   "source": [
    "Multiply the output of the pool by the weight of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRMkbFXdFwys"
   },
   "outputs": [],
   "source": [
    "linear_weights = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qV3dK-IzGFXh"
   },
   "source": [
    "Calculate the weight of the characters: each filter is applied to a position - add its weight to the covered characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0y8epEgGEU8"
   },
   "outputs": [],
   "source": [
    "symb_weights = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "utVltLCUGXTy"
   },
   "source": [
    "Visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2UKjz86F7fz"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def get_color_hex(weight):\n",
    "    cmap = plt.get_cmap(\"RdYlGn\")\n",
    "    rgba = cmap(weight, bytes=True)\n",
    "    return '#%02X%02X%02X' % rgba[:3]\n",
    "\n",
    "symb_template = '<span style=\"background-color: {color_hex}\">{symb}</span>'\n",
    "res = '<p>P(surname) = {:.2%}</p>'.format(prob)\n",
    "for symb, weight in zip(word, symb_weights):\n",
    "    res += symb_template.format(color_hex=get_color_hex(weight), symb=symb)\n",
    "res = '<p>' + res + '</p>'\n",
    "\n",
    "HTML(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGhfZsleGZi0"
   },
   "source": [
    "Combine everything into functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PUi51X4fQIIT"
   },
   "outputs": [],
   "source": [
    "def calc_weights(word):\n",
    "    <calc>\n",
    "    \n",
    "    return prob, symb_weights\n",
    "\n",
    "def visualize(word):\n",
    "    prob, symb_weights = calc_weights(word)\n",
    "    \n",
    "    symb_template = '<span style=\"background-color: {color_hex}\">{symb}</span>'\n",
    "    res = '<p>P(surname) = {:.2%}</p>'.format(prob)\n",
    "    for symb, weight in zip(word, symb_weights):\n",
    "        res += symb_template.format(color_hex=get_color_hex(weight), symb=symb)\n",
    "    res = '<p>' + res + '</p>'\n",
    "    return HTML(res)\n",
    "\n",
    "\n",
    "visualize('Королев')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXee135DEJuy"
   },
   "source": [
    "## Model improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2_8bNRyG3CF"
   },
   "source": [
    "**Task** To improve the stability of the model, it is worth adding the dropout `nn.Dropout` - a way to nullify a part of the scales at each epoch to regularize the model. Try adding it after embeddings and after convolution (or else somewhere else).\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1044/1*iWQzxhVlvadk6VAJjsgXgg.png\" width=\"50%\">\n",
    "\n",
    "**Task** Another way to regularize a model is to use BatchNormalization (`nn.BatchNorm2d`). Try adding it after convolution.\n",
    "\n",
    "**Task** Another way to improve the model is to add a bundle. Implement the model as in the picture at the beginning of the laptop: with convolutions of 2, 3, 4 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qstoDysVsSQ2"
   },
   "source": [
    "**Assignment** Distinguish between Narrow and Wide convolutions - in fact, whether zero padding is added or not. For texts, this difference looks like this:<img src=\"https://image.ibb.co/eqGZaS/2018_03_28_11_23_17.png\" width=\"50%\">\n",
    "\n",
    "*From Neural Network Methods in Natural Language Processing.*\n",
    "\n",
    "On the left, there is no padding, on the right, there is. Try adding padding and see what happens. Potentially, it will help to learn good word prefixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v39_tgnMnr0J"
   },
   "source": [
    "# Referrence\n",
    "[Convolutional Neural Networks, cs231n](http://cs231n.github.io/convolutional-networks/)  \n",
    "[Understanding Convolutions, Christopher Olah](http://colah.github.io/posts/2014-07-Understanding-Convolutions/)  \n",
    "[Understanding Convolutional Neural Networks for NLP, Denny Britz](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n",
    "\n",
    "[Character-Aware Neural Language Models, Yoon Kim et al, 2015](https://arxiv.org/abs/1508.06615)  \n",
    "[Character-level Convolutional Networks for Text Classification, Zhang et al., 2015](https://arxiv.org/abs/1509.01626)  \n",
    "[A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification Zhang et al., 2015](https://arxiv.org/abs/1510.03820)\n",
    "[Learning Character-level Representations for Part-of-Speech Tagging, dos Santos et al, 2014](http://proceedings.mlr.press/v32/santos14.pdf)\n",
    "\n",
    "[cs224n \"Lecture 13: Convolutional Neural Networks\"](https://www.youtube.com/watch?v=Lg6MZw_OOLI)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 04 - Convolutional Neural Networks.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
