{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uaD4Nk85FsZc"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8xZhIfJ9smN",
    "outputId": "a0a02d0d-ce24-4591-ec93-e77d47d8204e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7471c80f10>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djyFDC4QFwVh",
    "outputId": "505420cc-2678-42be-c170-e1f5492e5e04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6B_uOuSTF0PU",
    "outputId": "4d791658-4d99-48e7-ed52-5c446d0c3204"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5QwnBkTcfNo",
    "outputId": "0bee43de-0fa5-4462-9405-7410669b33df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ''}"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pp-LECVmcgSt",
    "outputId": "54b6bfb3-abf5-44f9-e145-cd2465b4d77e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' = Valkyria Chronicles III = \\n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKcltvAsfgHW"
   },
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCJYUEGrfxo6",
    "outputId": "9e9771a7-c09c-4f1d-d5fd-240733665653"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'how', 'are', 'you', '?']"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('hello world how are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4ipPKSwf0jq",
    "outputId": "b180fb8f-3981-4649-f0fa-59a4e1a2beef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['=', 'valkyria', 'chronicles', 'iii', '=']"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset['train'][1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aODmmThhcpTx"
   },
   "outputs": [],
   "source": [
    "def tokenize_data(example, tokenizer):\n",
    "    tokens = {'tokens': tokenizer(example['text'])}\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5x9lbSFc5Yh",
    "outputId": "9798a3d5-f4d9-4fef-d92d-d05b052ac5cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-38e31fad4a61d72e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-2181ba6714368d4f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-12708e5ca86f73dd.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcGF9LYAiEqB",
    "outputId": "28b1fdfe-2a9f-4745-fb92-efa7e7c0cb68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['=', 'valkyria', 'chronicles', 'iii', '=']}"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c1NIY8yiHJo"
   },
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'],\n",
    "                                                  min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9onJczdaiq1K",
    "outputId": "3cd05069-0232-4803-a630-b83fdd8466b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'and', 'in', 'to', 'a', '=', 'was']"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QzS26a0GIERG",
    "outputId": "ff380691-0d5a-4660-ad01-8fb4e0df7155"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29471"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUSGIDkP_YvP",
    "outputId": "e8ea85d8-abbf-49d5-e527-429aa79394f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hello' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54CbiCkw_5P3"
   },
   "outputs": [],
   "source": [
    "vocab.insert_token('<unk>', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pTNbJipk_k5b",
    "outputId": "ac535b76-09f7-4a2d-eaf1-1be5ab0a563d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', 'the', ',', '.', 'of', 'and', 'in', 'to', 'a', '=']"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grwHG5UN_eKN"
   },
   "outputs": [],
   "source": [
    "unk_index = vocab['<unk>']\n",
    "vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_JUTdrJ_m7j",
    "outputId": "08978606-d856-416c-e14e-074d1083f0e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYKf-779yFCB"
   },
   "outputs": [],
   "source": [
    "vocab.insert_token('<eos>', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LtYdBoybyI9B",
    "outputId": "144c1aae-d75c-4172-cf6c-283c407fb671"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<eos>', 'the', ',', '.', 'of', 'and', 'in', 'to', 'a']"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMUwATIrwkAi"
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        if example['tokens']:\n",
    "            tokens = example['tokens'].append('<eos>')\n",
    "            tokens = [vocab[token] for token in example['tokens']]\n",
    "            data.extend(tokens)\n",
    "    data = torch.LongTensor(data)\n",
    "    n_batches = data.shape[0] // batch_size\n",
    "    data = data.narrow(0, 0, n_batches * batch_size)\n",
    "    data = data.view(batch_size, -1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtjtIvuNisST"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsmojuCbivzC",
    "outputId": "45871921-68a3-4d1b-9d66-b377e70c9931"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 16214])"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "038OtCf3hbeY"
   },
   "outputs": [],
   "source": [
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data = get_data(tokenized_dataset['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAKBc-UUjO0-"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate, tie_weights):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        if tie_weights:\n",
    "            assert embedding_dim == hidden_dim, 'If tying weights then embedding_dim must equal hidden_dim'\n",
    "            self.embedding.weight = self.fc.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        cell = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden, cell\n",
    "\n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, input: Tenso):\n",
    "        # input = [batch size, seq len]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        embedding = self.dropout(self.embedding(input))\n",
    "        # embedding = [batch size, seq len, embedding dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        # output = [batch size, seq len, hidden dim]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        output = self.dropout(output)\n",
    "        prediction = self.fc(output)\n",
    "        # prediction = [batch size, seq len, vocab size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBYYhJ3WjWpY"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 1024\n",
    "hidden_dim = 1024\n",
    "n_layers = 2\n",
    "dropout_rate = 0.65\n",
    "tie_weights = True\n",
    "\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate, tie_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0C9nTPpxsclE",
    "outputId": "fc5389ad-fd5c-4aec-964c-72600428a3f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 47,003,425 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXwUDCA-92-y"
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkggfxKJBZ04"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMfUy_H2IdUo",
    "outputId": "2d0f6f8c-0a34-41b9-8687-c4b026f9f741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoPhYH3PJB-S"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5omPylb8Kag_"
   },
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, max_seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    n_tokens = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for offset in range(0, n_tokens - 1, max_seq_len):\n",
    "        optimizer.zero_grad()\n",
    "        input, target, seq_len = get_batch(data, max_seq_len, n_tokens, offset)\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        # input = [batch size, seq len]\n",
    "        # target = [batch size, seq len]\n",
    "        batch_size, seq_len = input.shape\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        prediction, hidden = model(input, hidden)\n",
    "        # prediction = [batch size, seq len, vocab size]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "        target = target.reshape(-1)\n",
    "        # prediction = [batch size * seq len, vocab size]\n",
    "        # target = [batch size * seq len]\n",
    "        loss = criterion(prediction, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iikSU_1YT0dV"
   },
   "outputs": [],
   "source": [
    "def get_batch(data, max_seq_len, n_tokens, offset):\n",
    "    seq_len = min(max_seq_len, n_tokens - offset - 1)\n",
    "    input = data[:, offset:offset+seq_len]\n",
    "    target = data[:, offset+1:offset+seq_len+1]\n",
    "    return input, target, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ev0pAlFMAgH"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, max_seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    n_tokens = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for offset in range(0, n_tokens - 1, max_seq_len):\n",
    "            input, target, seq_len = get_batch(data, max_seq_len, n_tokens, offset)\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            # input = [batch size, seq len]\n",
    "            # target = [batch size, seq len]\n",
    "            batch_size, seq_len = input.shape\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            # hidden = [n layers, batch size, hidden dim]\n",
    "            prediction, hidden = model(input, hidden)\n",
    "            # prediction = [batch size, seq len, vocab size]\n",
    "            # hidden = [n layers, batch size, hidden dim]\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "            # prediction = [batch size * seq len, vocab size]\n",
    "            # target = [batch size * seq len]\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_qDt7hWi3wJ"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9q6owq2DwP9Y"
   },
   "outputs": [],
   "source": [
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYVEjus7MElw",
    "outputId": "55852428-2e30-401a-c250-0854caa09a0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 623.137\n",
      "\tValid Perplexity: 279.474\n",
      "Epoch: 02 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 303.460\n",
      "\tValid Perplexity: 202.249\n",
      "Epoch: 03 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 222.573\n",
      "\tValid Perplexity: 168.837\n",
      "Epoch: 04 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 179.383\n",
      "\tValid Perplexity: 146.603\n",
      "Epoch: 05 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 152.362\n",
      "\tValid Perplexity: 137.059\n",
      "Epoch: 06 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 133.825\n",
      "\tValid Perplexity: 127.296\n",
      "Epoch: 07 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 120.237\n",
      "\tValid Perplexity: 120.745\n",
      "Epoch: 08 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 109.518\n",
      "\tValid Perplexity: 115.251\n",
      "Epoch: 09 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 100.898\n",
      "\tValid Perplexity: 112.620\n",
      "Epoch: 10 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 93.745\n",
      "\tValid Perplexity: 110.718\n",
      "Epoch: 11 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 88.096\n",
      "\tValid Perplexity: 108.855\n",
      "Epoch: 12 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 83.094\n",
      "\tValid Perplexity: 107.296\n",
      "Epoch: 13 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 78.756\n",
      "\tValid Perplexity: 106.687\n",
      "Epoch: 14 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 74.926\n",
      "\tValid Perplexity: 104.361\n",
      "Epoch: 15 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 71.699\n",
      "\tValid Perplexity: 104.082\n",
      "Epoch: 16 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 68.818\n",
      "\tValid Perplexity: 103.994\n",
      "Epoch: 17 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 66.408\n",
      "\tValid Perplexity: 103.884\n",
      "Epoch: 18 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 64.036\n",
      "\tValid Perplexity: 102.454\n",
      "Epoch: 19 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 61.971\n",
      "\tValid Perplexity: 102.674\n",
      "Epoch: 20 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 57.797\n",
      "\tValid Perplexity: 100.732\n",
      "Epoch: 21 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 55.863\n",
      "\tValid Perplexity: 101.289\n",
      "Epoch: 22 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 53.704\n",
      "\tValid Perplexity: 100.059\n",
      "Epoch: 23 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 52.689\n",
      "\tValid Perplexity: 99.763\n",
      "Epoch: 24 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 51.935\n",
      "\tValid Perplexity: 99.658\n",
      "Epoch: 25 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 51.186\n",
      "\tValid Perplexity: 99.547\n",
      "Epoch: 26 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 50.619\n",
      "\tValid Perplexity: 99.584\n",
      "Epoch: 27 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.643\n",
      "\tValid Perplexity: 99.638\n",
      "Epoch: 28 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.486\n",
      "\tValid Perplexity: 99.027\n",
      "Epoch: 29 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.246\n",
      "\tValid Perplexity: 98.958\n",
      "Epoch: 30 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.031\n",
      "\tValid Perplexity: 99.194\n",
      "Epoch: 31 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.196\n",
      "\tValid Perplexity: 98.686\n",
      "Epoch: 32 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 48.881\n",
      "\tValid Perplexity: 98.615\n",
      "Epoch: 33 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 48.674\n",
      "\tValid Perplexity: 98.623\n",
      "Epoch: 34 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 48.942\n",
      "\tValid Perplexity: 98.333\n",
      "Epoch: 35 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 48.851\n",
      "\tValid Perplexity: 98.364\n",
      "Epoch: 36 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.126\n",
      "\tValid Perplexity: 98.184\n",
      "Epoch: 37 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.214\n",
      "\tValid Perplexity: 98.128\n",
      "Epoch: 38 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.169\n",
      "\tValid Perplexity: 98.091\n",
      "Epoch: 39 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.357\n",
      "\tValid Perplexity: 98.025\n",
      "Epoch: 40 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.279\n",
      "\tValid Perplexity: 98.017\n",
      "Epoch: 41 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.470\n",
      "\tValid Perplexity: 97.985\n",
      "Epoch: 42 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.730\n",
      "\tValid Perplexity: 97.957\n",
      "Epoch: 43 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.691\n",
      "\tValid Perplexity: 97.937\n",
      "Epoch: 44 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.719\n",
      "\tValid Perplexity: 97.929\n",
      "Epoch: 45 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.791\n",
      "\tValid Perplexity: 97.925\n",
      "Epoch: 46 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.810\n",
      "\tValid Perplexity: 97.923\n",
      "Epoch: 47 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.809\n",
      "\tValid Perplexity: 97.921\n",
      "Epoch: 48 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.847\n",
      "\tValid Perplexity: 97.921\n",
      "Epoch: 49 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.848\n",
      "\tValid Perplexity: 97.920\n",
      "Epoch: 50 | Epoch Time: 1m 29s\n",
      "\tTrain Perplexity: 49.743\n",
      "\tValid Perplexity: 97.920\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "max_seq_len = 50\n",
    "clip = 0.25\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    train_loss = train(model, train_data, optimizer, criterion, batch_size, max_seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, max_seq_len, device)\n",
    "    \n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'lstm_lm.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0Y0oNdgRkad",
    "outputId": "ecf4590e-208d-478e-8e96-776222dee355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 93.684\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('lstm_lm.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, max_seq_len, device)\n",
    "\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GexY2GcCRtXu"
   },
   "outputs": [],
   "source": [
    "def generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(0)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_gen_tokens):\n",
    "            input = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(input, hidden)\n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1) \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "            indices.append(prediction)\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R778iX9gR38L"
   },
   "outputs": [],
   "source": [
    "prompt = 'the'\n",
    "n_gen_tokens = 25\n",
    "temperature = 0.5\n",
    "seed = 0\n",
    "\n",
    "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1MCYs3mR7nm",
    "outputId": "84d1e524-7982-4539-b757-c88bf1e6a6d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'highest',\n",
       " '@-@',\n",
       " 'paid',\n",
       " 'of',\n",
       " 'the',\n",
       " 'year',\n",
       " '.',\n",
       " 'it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'critical',\n",
       " 'success',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'first',\n",
       " 'two',\n",
       " '@-@',\n",
       " 'year',\n",
       " 'run',\n",
       " ',',\n",
       " 'the',\n",
       " 'first',\n",
       " 'time',\n",
       " 'in']"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wpz2dfdcR8mc"
   },
   "outputs": [],
   "source": [
    "temperature = 0.1\n",
    "\n",
    "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87N3LkyVpEIL",
    "outputId": "4247d86b-9927-45c9-e7fa-24c74fd5bb3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " ',',\n",
       " 'which',\n",
       " 'was',\n",
       " 'the',\n",
       " 'first',\n",
       " 'to',\n",
       " 'be',\n",
       " 'built',\n",
       " 'in',\n",
       " 'the',\n",
       " '<unk>',\n",
       " '.',\n",
       " '<eos>',\n",
       " '=',\n",
       " '=',\n",
       " '=',\n",
       " '=',\n",
       " 'chapel',\n",
       " 'of',\n",
       " 'our',\n",
       " 'lady',\n",
       " 'of',\n",
       " 'our']"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbGOerWrpHnc"
   },
   "outputs": [],
   "source": [
    "temperature = 1.5\n",
    "\n",
    "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ZLB5jwgpKUQ",
    "outputId": "294be7f2-b391-49e0-fba1-fb1885a866d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'hide',\n",
       " 'swap',\n",
       " 'just',\n",
       " 'leads',\n",
       " 'landmarks',\n",
       " 'and',\n",
       " 'arranged',\n",
       " 'discussions',\n",
       " '3',\n",
       " 'agree',\n",
       " 'specifically',\n",
       " 'with',\n",
       " 'the',\n",
       " 'friend',\n",
       " 'harvest',\n",
       " 'as',\n",
       " 'captains',\n",
       " 'like',\n",
       " 'tom',\n",
       " 'bradley',\n",
       " 'giger',\n",
       " 'viewed',\n",
       " 'the',\n",
       " 'team',\n",
       " \"'\"]"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYNUGi9uy-qr"
   },
   "outputs": [],
   "source": [
    "temperature = 0.75\n",
    "\n",
    "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CAf98XY-9-T6",
    "outputId": "10e7b4a2-0454-489a-c542-5a65deb1783f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'highest',\n",
       " '<unk>',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " '.',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'oldman',\n",
       " 'city',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'st',\n",
       " '.',\n",
       " 'louis',\n",
       " 'rail',\n",
       " 'district',\n",
       " 'has',\n",
       " 'a',\n",
       " 'population',\n",
       " 'of',\n",
       " '17']"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clWCfGOF-N8G"
   },
   "outputs": [],
   "source": [
    "temperature = 0.8\n",
    "\n",
    "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJs4jw1n-N6D",
    "outputId": "8f8f57c2-1079-45aa-b9af-97c17d28e283"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'highest',\n",
       " 'swap',\n",
       " 'in',\n",
       " 'the',\n",
       " 'era',\n",
       " '.',\n",
       " 'the',\n",
       " 'old',\n",
       " '3',\n",
       " '@',\n",
       " '.',\n",
       " '@',\n",
       " '06',\n",
       " 'm',\n",
       " '(',\n",
       " '3',\n",
       " '@',\n",
       " '.',\n",
       " '@',\n",
       " '6',\n",
       " 'ft',\n",
       " ')',\n",
       " 'wide',\n",
       " ',',\n",
       " 'fifth']"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJxcBjpN-N4F"
   },
   "outputs": [],
   "source": [
    "temperature = 0.7\n",
    "\n",
    "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2WLOsMmi-NzF",
    "outputId": "e8674f95-a49c-4c2f-c77e-dd1e81482bf1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'highest',\n",
       " '<unk>',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " '.',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " '<unk>',\n",
       " '@-@',\n",
       " '<unk>',\n",
       " 'and',\n",
       " 'a',\n",
       " '@-@',\n",
       " '<unk>',\n",
       " '@-@',\n",
       " 'chorus',\n",
       " 'sample',\n",
       " ',',\n",
       " 'which',\n",
       " 'features',\n",
       " 'the',\n",
       " '<unk>']"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "lstm_lm_master.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
