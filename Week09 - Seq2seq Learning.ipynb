{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OE7fXh-OSJYF"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch==0.4.1\n",
    "!pip -qq install torchtext==0.3.1\n",
    "!pip -qq install torchvision==0.2.1\n",
    "!pip -qq install spacy==2.0.16\n",
    "!python -m spacy download en\n",
    "!pip install sacremoses==0.0.5\n",
    "!pip install subword_nmt==0.3.5\n",
    "!wget -qq http://www.manythings.org/anki/rus-eng.zip \n",
    "!unzip rus-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhvfH55PUJ8K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txWqIO_74A4s"
   },
   "source": [
    "# Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GRiSIqhQcP2W"
   },
   "source": [
    "We have already looked at this picture several times:\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" width=\"50%\">\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "In POS tagging, we (well, some accurately) used an important idea: first, a certain function above the symbols was used to embed a word (for example, many to one rnn'koy in the picture). Then another rnn'ka built embeddingings of words according to their context. And then it is all classified by logistic regression.\n",
    "\n",
    "Here it is important that we teach the encoder to build end2end embeddings - right in the network (this is the main difference between neural networks and classical approaches - in the ability to do end2end).\n",
    "\n",
    "Another thing we did was language models. Here, like this:\n",
    "<img src=\"https://hsto.org/web/dc1/7c2/c4e/dc17c2c4e9ac434eb5346ada2c412c9a.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "Pay attention to the red arrow - it shows the transfer of the hidden state, which is responsible for the network memory.\n",
    "\n",
    "Now let's combine these two ideas:\n",
    "<img src=\"https://raw.githubusercontent.com/tensorflow/nmt/master/nmt/g3doc/img/seq2seq.jpg\" width=\"50%\">\n",
    "\n",
    "*From [tensorflow/nmt](https://github.com/tensorflow/nmt)*\n",
    "\n",
    "Everything looks almost like a language model, but in the blue part the predictions are not made, only the last hidden state is used.\n",
    "\n",
    "The blue part of the network is called the encoder, it builds the embedding sequence. The red part is a decoder, it works like a normal language model, but takes into account the result of the work of the encoder.\n",
    "\n",
    "As a result, the encoder learns to efficiently extract meaning from a sequence of words, and the decoder must build on them a new sequence. This can be a sequence of translation words, or a sequence of words in the chat bot reply, or something else depending on your corruption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JyNkst1XkYN6"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tRLNIzJkkqkM"
   },
   "source": [
    "Let's start by reading the data. Take them from anki, so they are a bit specific:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2vjzBVqZF0b"
   },
   "outputs": [],
   "source": [
    "!shuf -n 10 rus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOVlO5_Qlg5y"
   },
   "source": [
    "Токенизируем их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsOvtO0fpCHa"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, Example, Dataset, BucketIterator\n",
    "\n",
    "BOS_TOKEN = '<s>'\n",
    "EOS_TOKEN = '</s>'\n",
    "\n",
    "source_field = Field(tokenize='spacy', init_token=None, eos_token=EOS_TOKEN)\n",
    "target_field = Field(tokenize='moses', init_token=BOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "fields = [('source', source_field), ('target', target_field)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qA15-tcudjw"
   },
   "outputs": [],
   "source": [
    "source_field.preprocess(\"It's surprising that you haven't heard anything about her wedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HguPFHc5sjcD"
   },
   "outputs": [],
   "source": [
    "target_field.preprocess('Удивительно, что ты ничего не слышал о её свадьбе.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VO-gix7yoBjg"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "MAX_TOKENS_COUNT = 16\n",
    "SUBSET_SIZE = .3\n",
    "\n",
    "examples = []\n",
    "with open('rus.txt') as f:\n",
    "    for line in tqdm(f, total=328190):\n",
    "        source_text, target_text = line.split('\\t')\n",
    "        source_text = source_field.preprocess(source_text)\n",
    "        target_text = target_field.preprocess(target_text)\n",
    "        if len(source_text) <= MAX_TOKENS_COUNT and len(target_text) <= MAX_TOKENS_COUNT:\n",
    "            if np.random.rand() < SUBSET_SIZE:\n",
    "                examples.append(Example.fromlist([source_text, target_text], fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8uCsMEglm6V"
   },
   "source": [
    "Построим датасеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOBgLAgVTrk1"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(examples, fields)\n",
    "\n",
    "train_dataset, test_dataset = dataset.split(split_ratio=0.85)\n",
    "\n",
    "print('Train size =', len(train_dataset))\n",
    "print('Test size =', len(test_dataset))\n",
    "\n",
    "source_field.build_vocab(train_dataset, min_freq=3)\n",
    "print('Source vocab size =', len(source_field.vocab))\n",
    "\n",
    "target_field.build_vocab(train_dataset, min_freq=3)\n",
    "print('Target vocab size =', len(target_field.vocab))\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    datasets=(train_dataset, test_dataset), batch_sizes=(32, 256), shuffle=True, device=DEVICE, sort=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N5sMz-hfBvCl"
   },
   "outputs": [],
   "source": [
    "source_field.process([source_field.preprocess(\"It's surprising that you haven't heard anything about her wedding.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M1yFzRg2xAjl"
   },
   "outputs": [],
   "source": [
    "source_field.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19X2G_QpxJEx"
   },
   "outputs": [],
   "source": [
    "target_field.vocab.itos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FYJe2CA8GcY"
   },
   "source": [
    "## Seq2seq модель\n",
    "\n",
    "It's time to write simple seq2seq. We divide the model into several modules - Encoder, Decoder and their combination.\n",
    "\n",
    "Encoder should be similar to the character reticule in POS tagging: to attach tokens and start rnn (in this case we will use GRU) and give the last hidden state.\n",
    "\n",
    "The decoder is almost the same, only it predicts tokens at each step.\n",
    "\n",
    "** Task ** Implement models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySJ4tUAqvFvB"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8ndCRZLl4ZZ"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, rnn_hidden_dim=256, num_layers=1, bidirectional=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self._rnn = nn.GRU(input_size=emb_dim, hidden_size=rnn_hidden_dim, \n",
    "                           num_layers=num_layers, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        <implement me>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Un0AOmdqLPp_"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, rnn_hidden_dim=256, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self._rnn = nn.GRU(input_size=emb_dim, hidden_size=rnn_hidden_dim, num_layers=num_layers)\n",
    "        self._out = nn.Linear(rnn_hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, encoder_output, hidden=None):\n",
    "        <implement me>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9nsO1HCmgn3"
   },
   "source": [
    "Модель перевода будет просто сперва вызывать Encoder, а потом передавать его скрытое состояние декодеру в качестве начального."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLIGjPOiO7X9"
   },
   "outputs": [],
   "source": [
    "class TranslationModel(nn.Module):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size, emb_dim=128, \n",
    "                 rnn_hidden_dim=256, num_layers=1, bidirectional_encoder=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(source_vocab_size, emb_dim, rnn_hidden_dim, num_layers, bidirectional_encoder)\n",
    "        self.decoder = Decoder(target_vocab_size, emb_dim, rnn_hidden_dim, num_layers)\n",
    "        \n",
    "    def forward(self, source_inputs, target_inputs):\n",
    "        encoder_hidden = self.encoder(source_inputs)\n",
    "        \n",
    "        return self.decoder(target_inputs, encoder_hidden, encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_qVuSL8QJg4"
   },
   "outputs": [],
   "source": [
    "model = TranslationModel(source_vocab_size=len(source_field.vocab), target_vocab_size=len(target_field.vocab)).to(DEVICE)\n",
    "\n",
    "model(batch.source, batch.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pz4Ckgh1mwm4"
   },
   "source": [
    "We implement a simple translation - greedy. At each step we will issue the most likely of the predicted tokens:<img src=\"https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/greedy_dec.jpg\" width=\"50%\"> \n",
    "*From [tensorflow/nmt](https://github.com/tensorflow/nmt)*\n",
    "\n",
    "** Task ** Implement function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9gmcOC9DwiS"
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, source_text, source_field, target_field):\n",
    "    bos_index = target_field.vocab.stoi[BOS_TOKEN]\n",
    "    eos_index = target_field.vocab.stoi[EOS_TOKEN]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = [] # list of predicted tokens indices\n",
    "        <implement me>\n",
    "            \n",
    "        return ' '.join(target_field.vocab.itos[ind.squeeze().item()] for ind in result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mM58pAd6FBml"
   },
   "outputs": [],
   "source": [
    "greedy_decode(model, \"Do you believe?\", source_field, target_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-ha7DI_ngAO"
   },
   "source": [
    "\n",
    "Need to somehow evaluate the model.\n",
    "\n",
    "Usually, [BLEU speed] (https://en.wikipedia.org/wiki/BLEU) is used for this - something like the accuracy of guessing n-gram from the correct (reference) translation.\n",
    "\n",
    "** Task ** Implement the evaluation function: for batches from ʻiterator`, predict their translations, trim by `</ s>` and add the correct variants and predicted in `refs` and` hyps`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YjYA3eohGlOA"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def evaluate_model(model, iterator):\n",
    "    model.eval()\n",
    "    refs, hyps = [], []\n",
    "    eos_index = iterator.dataset.fields['target'].vocab.stoi[EOS_TOKEN]\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            <implement me>\n",
    "            \n",
    "    return corpus_bleu([[ref] for ref in refs], hyps) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_E2JxfRuphch"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.get_lock().locks = []\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data_iter, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = len(data_iter)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):                \n",
    "                logits, _ = model(batch.source, batch.target)\n",
    "                \n",
    "                target = torch.cat((batch.target[1:], batch.target.new_ones((1, batch.target.shape[1]))))\n",
    "                loss = criterion(logits.view(-1, logits.shape[-1]), target.view(-1))\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
    "                                                                                         math.exp(loss.item())))\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
    "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
    "            )\n",
    "            progress_bar.refresh()\n",
    "\n",
    "    return epoch_loss / batches_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_iter, epochs_count=1, val_iter=None):\n",
    "    best_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss = do_epoch(model, criterion, train_iter, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            val_loss = do_epoch(model, criterion, val_iter, None, name_prefix + '  Val:')\n",
    "            print('\\nVal BLEU = {:.2f}'.format(evaluate_model(model, val_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5X2kYDU_rCjP"
   },
   "outputs": [],
   "source": [
    "model = TranslationModel(source_vocab_size=len(source_field.vocab), target_vocab_size=len(target_field.vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = target_field.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "es0D28m5jdNF"
   },
   "source": [
    "## Scheduled Sampling\n",
    "\n",
    "Until now, we have been training the translation using the so-called * teacher forcing *: the decoder always took the correct token as output in the previous step. The problem with this approach is that during the inference the correct token will most likely not get out at least at some step. It turns out that the network studied at the good inputs, it will be used on the bad - it can easily break everything.\n",
    "\n",
    "An alternative approach is to sample the token from the current step right during the training and transfer it to the next.\n",
    "\n",
    "Such an approach is not very well mathematically substantiated (gradients are not passed through sampling), but it is interesting to implement it and it often improves the quality.\n",
    "\n",
    "\n",
    "** Task ** Update `Decoder`: Replace the rnn'c call above the sequence with a loop. At each step, pass the `p` probability as the previous output to the decoder, the correct input, and otherwise - argmax from the previous output (the cycle should be similar to those in` greedy_decode` and evaluate_model`). When passing argmax, call `detach` so that the gradients do not run through. Collect all exits in the list, at the end make `torch.cat`.\n",
    "\n",
    "As a result, with a probability equal to `p = 1`, it should turn out as before, only slower. When training, you can pass `p = 0.5`, in the interest -` p = 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILPhSHsLoGzd"
   },
   "source": [
    "## Beam Search\n",
    "\n",
    "Another way to deal with decoding errors on the interference is to do beam search. In essence, this is a depth search with very strong clipping at each step:\n",
    "<img src=\"https://image.ibb.co/dBRKkA/2018-11-06-13-53-40.png\" width=\"50%\">\n",
    "  \n",
    "*From [cs224n, Machine Translation, Seq2Seq and Attention](http://web.stanford.edu/class/cs224n/lectures/lecture10.pdf)*\n",
    "\n",
    "In the picture, at each step, the two best (according to the network predictions) of the four chain continuation options are selected.\n",
    "\n",
    "For the comparison of beams, the sums of the log probabilities of the tokens included in the beam are used. To get log-likelihoods, you just need to call `F.log_softmax` for logites. The advantage of adding logarithms over the multiplication of probabilities should be clear: there are no such problems with numerical instability — by multiplying probabilities close to zero, we very quickly get zero as a scramble.\n",
    "\n",
    "As a result, you need to implement an analogue of `gready_decoding`.\n",
    "\n",
    "The Beam will consist of a sequence of token indices (at the beginning - `[bos_index]`), total quality (at the beginning of 0) and the last `hidden` (at the beginning of` encoder_hidden`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XVFK1mGD-FVQ"
   },
   "source": [
    "Interactive visualization dragged off https://github.com/yandexdataschool/nlp_course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-6vnrGBI-ETr"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/beam_search.html 2> log\n",
    "from IPython.display import HTML\n",
    "# source: parlament does not support the amendment freeing tymoshenko\n",
    "HTML('./beam_search.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkZjVusArtYj"
   },
   "outputs": [],
   "source": [
    "def beam_search_decode(model, source_text, source_field, target_field, beam_size=5):\n",
    "    bos_index = target_field.vocab.stoi[BOS_TOKEN]\n",
    "    eos_index = target_field.vocab.stoi[EOS_TOKEN]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden = model.encoder(...source_text...)\n",
    "        beams = [([bos_index], 0, encoder_hidden)]\n",
    "        \n",
    "        # 1. make next step from each beam\n",
    "        # 2. create new beams from top beam_size of each continuation (best next token variants for the given token)\n",
    "        # 3. leave only top beam_size beams\n",
    "        # 4. repeat\n",
    "            \n",
    "        return ' '.join(target_field.vocab.itos[ind.squeeze().item()] for ind in result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tK4OdgTRQmqp"
   },
   "source": [
    "## Model improvements\n",
    "\n",
    "\n",
    "**Task** Try to improve the quality of the model. Try:\n",
    "- Bidirectional encoder\n",
    "- Dropout\n",
    "- Stack moar layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37gHrfjwywIK"
   },
   "source": [
    "## Byte-Pair Encoding\n",
    "\n",
    "We can represent the words in one index - and use word embeddings as rows of the embeddingings matrix.\n",
    "We can consider them a set of characters and get word embedding with the help of some function above symbol embeddings.\n",
    "\n",
    "Finally, we can also use an intermediate representation - as a set of subwords.\n",
    "\n",
    "A few years ago, the use of subwords was suggested for the machine translation task: [Neural Machine Translation of Rare Words with Subword Units] (https://arxiv.org/abs/1508.07909). It used byte-pair encoding.\n",
    "\n",
    "In fact, this is the process of combining the most frequent pairs of characters of the alphabet into a new super-symbol. Suppose we have a dictionary consisting of such a set of words:\n",
    "`‘ Low · ’,‘ lowest · ’,‘ newer · ’,‘ wider · ’`\n",
    "(`·` Means the end of the word)\n",
    "\n",
    "Then the first can learn the new character `r ·`, after it `l o` will turn into` lo`. `W` will be attached to this new symbol:` lo w` $ \\ to $ `low`. And so on.\n",
    "\n",
    "It is argued that in this way, firstly, all frequency and short words will be learned, and secondly, all significant subwords. For example, the resulting alphabet should contain `ly ·` and `tion ·`.\n",
    "\n",
    "Then the word can be broken into a set of subwords - and act as with characters.\n",
    "\n",
    "Here you can find pre-trained embeddingings: [BPEmb](https://github.com/bheinzerling/bpemb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7A9WFdh92zHr"
   },
   "source": [
    "We will train a model for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zYsZoEu7zXY5"
   },
   "outputs": [],
   "source": [
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "\n",
    "with open('data.en', 'w') as f_src, open('data.ru', 'w') as f_dst:\n",
    "    for example in examples:\n",
    "        f_src.write(' '.join(example.source) + '\\n')\n",
    "        f_dst.write(' '.join(example.target) + '\\n')\n",
    "\n",
    "bpe = {}\n",
    "for lang in ['en', 'ru']:\n",
    "    with open('./data.' + lang) as f_data, open('bpe_rules.' + lang, 'w') as f_rules:\n",
    "        learn_bpe(f_data, f_rules, num_symbols=3000)\n",
    "    with open('bpe_rules.' + lang) as f_rules:\n",
    "        bpe[lang] = BPE(f_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWYVhaGx2vPN"
   },
   "outputs": [],
   "source": [
    "bpe['en'].process_line(' '.join(examples[10000].source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9g_s4vOO1_yI"
   },
   "outputs": [],
   "source": [
    "bpe['ru'].process_line(' '.join(examples[10000].target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cf5MeBFw22wk"
   },
   "source": [
    "**Задание** Переобучиться с subword'ами вместо слов. Возможно, поменять их число (`num_symbols`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RcAdcldjB0y8"
   },
   "source": [
    "# Image Captioning\n",
    "\n",
    "It is not necessary to encode a sequence of words. For example, you can use a convolutional network for an image encoder - and generate a signature for it:\n",
    "\n",
    "<img src=\"https://image.ibb.co/fpYdkL/image-captioning.png\" width=\"50%\">\n",
    "\n",
    "* From [Image Captioning Tutorial] (https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning) *\n",
    "\n",
    "The result is very cool signatures: [https://cs.stanford.edu/people/karpathy/deepimagesent/] (http: //cs.stanford.edu/people/karpathy/deepimagesent/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BIFL7T5FBXb8"
   },
   "source": [
    "Скачаем данные для обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQJhK7XhBaPi"
   },
   "outputs": [],
   "source": [
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id': '13BP-6Xd6ymhGallRppYfJBO6UUjFCtbB'})\n",
    "downloaded.GetContentFile('image_codes.npy')\n",
    "\n",
    "downloaded = drive.CreateFile({'id': '1O7_3lyTyBMXsBBIt1PwUXwLdkyRQzZML'})\n",
    "downloaded.GetContentFile('sources.txt')\n",
    "\n",
    "downloaded = drive.CreateFile({'id': '1t-Dy8TzoRuTMoM7N9NJZKgWXfaw3b6KF'})\n",
    "downloaded.GetContentFile('texts.txt')\n",
    "\n",
    "!wget http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/Flickr8k_Dataset.zip\n",
    "!unzip Flickr8k_Dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6IQ--RKqDK2Q"
   },
   "source": [
    "Скачем предобученную модельку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SrHFVqVYCn09"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.inception import Inception3\n",
    "\n",
    "class BeheadedInception3(Inception3):\n",
    "    \"\"\" Like torchvision.models.inception.Inception3 but the head goes separately \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.clone()\n",
    "        x[:, 0] = x[:, 0] * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "        x[:, 1] = x[:, 1] * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "        x[:, 2] = x[:, 2] * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        x = self.Mixed_5b(x)\n",
    "        x = self.Mixed_5c(x)\n",
    "        x = self.Mixed_5d(x)\n",
    "        x = self.Mixed_6a(x)\n",
    "        x = self.Mixed_6b(x)\n",
    "        x = self.Mixed_6c(x)\n",
    "        x = self.Mixed_6d(x)\n",
    "        x = self.Mixed_6e(x)\n",
    "        x = self.Mixed_7a(x)\n",
    "        x = self.Mixed_7b(x)\n",
    "        x_for_attn = x = self.Mixed_7c(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = F.avg_pool2d(x, kernel_size=8)\n",
    "        # 1 x 1 x 2048\n",
    "        x_for_capt = x = x.view(x.size(0), -1)\n",
    "        # 2048\n",
    "        x = self.fc(x)\n",
    "        # 1000 (num_classes)\n",
    "        return x_for_attn, x_for_capt, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LuFp0zETCoZD"
   },
   "outputs": [],
   "source": [
    "from torch.utils.model_zoo import load_url\n",
    "\n",
    "inception_model = BeheadedInception3()\n",
    "\n",
    "inception_url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
    "inception_model.load_state_dict(load_url(inception_url))\n",
    "\n",
    "inception_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHc-aSErDN81"
   },
   "source": [
    "Почему это вообще работает? Запустим модельку на картинке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xHAuLf2DCqrt"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import imresize\n",
    "%matplotlib inline\n",
    "    \n",
    "img = plt.imread('Flicker8k_Dataset/1000268201_693b08cb0e.jpg')\n",
    "img = imresize(img, (299, 299)).astype('float32') / 255.\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERVd_yomCtrU"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n",
    "labels = {int(key): value for (key, value) in requests.get(LABELS_URL).json().items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_tensor = torch.tensor(img.transpose([2, 0, 1]), dtype=torch.float32).unsqueeze(0)\n",
    "    _, _, logits = inception_model(img_tensor)\n",
    "    _, top_classes = logits.topk(5)\n",
    "\n",
    "    print('; '.join(labels[ind.item()] for ind in top_classes.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NO_GWOkbDTqF"
   },
   "source": [
    "Она выдает такие классы.\n",
    "\n",
    "Подписи же к картинке такие:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9fgGaG7CvQ1"
   },
   "outputs": [],
   "source": [
    "with open('texts.txt') as f:\n",
    "    text = f.readline().strip().split('\\t')\n",
    "print('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RU4A4rjRDZX8"
   },
   "source": [
    "Загрузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P2Sl4VFHCw18"
   },
   "outputs": [],
   "source": [
    "source_field = Field(sequential=False, use_vocab=False, dtype=torch.float)\n",
    "target_field = Field(init_token=BOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "path_field = Field(sequential=False, use_vocab=True)\n",
    "\n",
    "fields = [('source', source_field), ('target', target_field), ('path', path_field)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXv4MEO0C33z"
   },
   "outputs": [],
   "source": [
    "img_vectors = np.load('image_codes.npy')\n",
    "\n",
    "examples = []\n",
    "with open('texts.txt') as f_texts, open('sources.txt') as f_sources:\n",
    "    for img, texts, source in zip(img_vectors, f_texts, f_sources):\n",
    "        for text in texts.split('\\t'):\n",
    "            examples.append(Example.fromlist([img, target_field.preprocess(text), source.rstrip()], fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8imy6UUC5mo"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(examples, fields)\n",
    "\n",
    "train_dataset, test_dataset = dataset.split(split_ratio=0.85)\n",
    "\n",
    "print('Train size =', len(train_dataset))\n",
    "print('Test size =', len(test_dataset))\n",
    "\n",
    "target_field.build_vocab(train_dataset, min_freq=2)\n",
    "path_field.build_vocab(dataset)\n",
    "print('Target vocab size =', len(target_field.vocab))\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    datasets=(train_dataset, test_dataset), batch_sizes=(32, 512), shuffle=True, device=DEVICE, sort=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ngs3fUFyDbZ4"
   },
   "source": [
    "**Задание** Реализуйте декодер для модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRNmlDAPC7YM"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, cnn_feature_size, emb_dim=128, rnn_hidden_dim=256, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self._cnn_to_h0 = nn.Linear(cnn_feature_size, rnn_hidden_dim)\n",
    "        self._cnn_to_c0 = nn.Linear(cnn_feature_size, rnn_hidden_dim)\n",
    "        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=rnn_hidden_dim, num_layers=num_layers)\n",
    "        self._out = nn.Linear(rnn_hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, encoder_output, inputs, hidden=None):\n",
    "        ...\n",
    "    \n",
    "    def init_hidden(self, encoder_output):\n",
    "        encoder_output = encoder_output.unsqueeze(0)\n",
    "        return self._cnn_to_h0(encoder_output), self._cnn_to_c0(encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhdX4BcqDmLa"
   },
   "source": [
    "Хак, чтобы все работало со старым циклом обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hwl46PpCC_xV"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, iterator):\n",
    "    return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aC2s10FqC92K"
   },
   "outputs": [],
   "source": [
    "model = Decoder(vocab_size=len(target_field.vocab), cnn_feature_size=img_vectors.shape[1]).to(DEVICE)\n",
    "\n",
    "pad_idx = target_field.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v4rA1O3JDqt-"
   },
   "source": [
    "Проверим, что работает генерация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2daTHESDGjX"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(test_iter))\n",
    "\n",
    "img = path_field.vocab.itos[batch.path[0].item()]\n",
    "\n",
    "img = plt.imread('Flicker8k_Dataset/' + img)\n",
    "img = imresize(img, (299, 299)).astype('float32') / 255.\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6G5jlZGCDwSW"
   },
   "source": [
    "**Задание** Напишите цикл генерации из модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2cTrWGPDI0f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4-3pYqVJIKA"
   },
   "source": [
    "# Referrence\n",
    "\n",
    "Sequence to Sequence Learning with Neural Networks, Ilya Sutskever, et al, 2014 [[pdf]](https://arxiv.org/pdf/1409.3215.pdf)  \n",
    "Show and Tell: A Neural Image Caption Generator, Oriol Vinyals et al, 2014 [[arxiv]](https://arxiv.org/abs/1411.4555)  \n",
    "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks, Samy Bengio, et al, 2015 [[arxiv]](https://arxiv.org/abs/1506.03099)  \n",
    "Neural Machine Translation of Rare Words with Subword Units, Rico Sennrich, 2015 [[arxiv]](https://arxiv.org/abs/1508.07909)  \n",
    "Massive Exploration of Neural Machine Translation Architectures, Denny Britz, et al, 2017 [[pdf]](https://arxiv.org/pdf/1703.03906.pdf)\n",
    "\n",
    "Neural Machine Translation (seq2seq) Tutorial [tensorflow/nmt](https://github.com/tensorflow/nmt)  \n",
    "[A Word of Caution on Scheduled Sampling for Training RNNs](https://www.inference.vc/scheduled-sampling-for-rnns-scoring-rule-interpretation/)\n",
    "\n",
    "cs224n, [Machine Translation, Seq2Seq and Attention](https://www.youtube.com/watch?v=IxQtK2SjWWM)\n",
    "\n",
    "[The Annotated Encoder Decoder](https://bastings.github.io/annotated_encoder_decoder/)  \n",
    "[Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models](http://seq2seq-vis.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 09 - Seq2seq.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
