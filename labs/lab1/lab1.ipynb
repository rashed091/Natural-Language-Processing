{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre style=\"float: right\">version 1.0.1</pre>\n",
    "# FNLP: Lab Session 1: Corpora and Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim\n",
    "\n",
    "The aims of this lab session are to \n",
    "1. explore the different uses of language in different documents, authored by different people and \n",
    "2. introduce the construction of language models using Python’s Natural Language Tool Kit (NLTK).\n",
    "\n",
    "This year labs are run through Jupyter Notebooks. Successful completion of this lab is important as the first assignment for FNLP builds on some of the concepts and methods that are introduced here. By the end of this lab session, you should be able to:\n",
    "\n",
    "* Access the corpora provided in NLTK\n",
    "* Compute a frequency distribution\n",
    "* Train a language model\n",
    "* Use a language model to compute bigram probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Help \n",
    "\n",
    "Python contains a built-in help module that runs in an interactive mode. To\n",
    "run the interactive help, type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`help()` will run until interrupted. If a cell is running it will block any other cell from running until it has completed. You can check if a cell is still running by looking at `In [*]:` to the left of any cell. If there is a `*` inside the brackets the cell is still running. As soon as the cell has stopped running the `*` will be replaced by a number. \n",
    "\n",
    "**Before moving on** you will need to interrupt `help()` (make it stop running). To interrupt running cells go to **`kernel/interrupt`** at the top of the webpage. You can also hit the **big black square button** right underneath (if you hover over it it will say interrupt kernel). This is equivalent to hitting CTRL-d to interrupt a running program in the terminal or the python shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know the name of the module that you want to get help on, type:\n",
    "`import <module_name>`\n",
    "`help(<module_name>)`\n",
    "try looking at the help documentation for `matplotlib.pyplot` - a python package introduced in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "help(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know the name of the module and the method that you want to get help\n",
    "on, type `help(<module_name>.<method_name>)` (note you must have imported `<module_name>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The FNLP lab sessions will make use of the Natural Language Tool Kit (NLTK) for Python. NLTK is a platform for writing programs to process human language data, that provides both corpora and modules. For more information on NLTK, please visit http://www.nltk.org/.\n",
    "\n",
    "For each exercise, edit the corresponding function in the notebook, then run the lines which prepare for and invoke that function.\n",
    "\n",
    "Let's start by importing NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Corpora\n",
    "\n",
    "NLTK provides many corpora and covers many genres of text. Some of the\n",
    "corpora are listed below:\n",
    "\n",
    "* Gutenberg: out of copyright books\n",
    "* Brown: a general corpus of texts including novels, short stories and news\n",
    "articles\n",
    "* Inaugural: U.S. Presidential inaugural speeches\n",
    "\n",
    "To see a complete list of available corpora you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(nltk.data.find('corpora')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each corpus contains a number of texts. We’ll work with the inaugural corpus, and explore what the corpus contains. Make sure you have imported the nltk module first and then load the inaugural corpus by typing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To list all of the documents in the inaugural corpus, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inaugural.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on we’ll work with President Barack Obama’s inaugural speech from 2009 (2009-Obama.txt). The contents of each document (in a corpus) may be accessed via a number of corpus readers. The plaintext corpus reader provides methods to view the raw text (raw), a list of words (words) or a list of sentences: to list all of the documents in the inaugural corpus, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inaugural.raw('2009-Obama.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inaugural.words('2009-Obama.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inaugural.sents('2009-Obama.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "implement function ``count_stats`` that for a given inaugural speech finds:\n",
    "* total number of words (tokens)\n",
    "* total number of distinct words (word types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stats(doc_name):\n",
    "    '''\n",
    "    type doc_name: string\n",
    "    param doc_name: Inaugural speech name \n",
    "    rtype1: int\n",
    "    return1: total number of words (tokens)\n",
    "    rtype2: int\n",
    "    return2: totoal number of distinct words (word types)\n",
    "    '''\n",
    "    # Use the plaintext corpus reader to access a pre-tokenised list of words\n",
    "    # for the document specified in \"doc_name\"\n",
    "    doc_words = inaugural.words(doc_name)\n",
    "    # Find the total number of words in the speech\n",
    "    total_words = \n",
    "    # Find the total number of DISTINCT words in the speech\n",
    "    total_distinct_words = \n",
    "    # Return the word counts\n",
    "    return total_words, total_distinct_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your solution, evaluate the count statistics for Obama inaugural speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_name = '2009-Obama.txt'\n",
    "tokens,types = count_stats(speech_name)\n",
    "print('Total words in {}: {}'.format(speech_name, tokens))\n",
    "print('Total distinct words in {}: {}'.format(speech_name, types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Create a function ``average_stats`` to find the average word-type length of the inaugural speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_stats(doc_name):\n",
    "    '''\n",
    "    type doc_name: string\n",
    "    param doc_name: Inaugural speech name \n",
    "    rtype: float\n",
    "    return: average word type lenght per document \n",
    "    '''\n",
    "    doc_words = inaugural.words(doc_name)\n",
    "    # Construct a list that contains the word lengths for each DISTINCT word in the document\n",
    "    distinct_word_lengths = \n",
    "    # Find the average word type length\n",
    "    avg_word_length = \n",
    "    # Return the average word type length of the document\n",
    "    return avg_word_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, test your solution with Obama’s 2009 speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_name = '2009-Obama.txt'\n",
    "avg_length = average_stats(speech_name)\n",
    "print(\"Average word type length for {}: {:.3f}\".format(speech_name, avg_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution\n",
    "\n",
    "A frequency distribution records the number of times each outcome of an experiment has occurred. For example, a frequency distribution could be used to record the number of times each word appears in a document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "# Obtain the words from Barack Obama’s 2009 speech\n",
    "obama_words = inaugural.words('2009-Obama.txt')\n",
    "# Construct a frequency distribution over the lowercased words in the document\n",
    "fd_obama_words = FreqDist(w.lower() for w in obama_words)\n",
    "# Find the top 50 most frequently used words in the speech\n",
    "print(fd_obama_words.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily plot the top 50 words (note `%matplotlib inline` tells jupyter that it should embed plots in the output cell after you run the code. You only need to run it once per notebook, not in every cell with a plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fd_obama_words.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out how many times the words ``peace`` and ``america`` were used in the speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('peace: {}'.format(fd_obama_words['peace']))\n",
    "print('america: {}'.format(fd_obama_words['america']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Write a function ``mostFreq`` that given a name of the inaugural speech estimates the top ``k`` (default 50) most frequent words used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostFreq(doc_name, k=50):\n",
    "    '''\n",
    "    type doc_name: string\n",
    "    param doc_name: Inaugural speech name\n",
    "    type k: int (default 50)\n",
    "    param k: number of most common elements to return\n",
    "    rtype: list of tuples\n",
    "    return: list of (word, frequency) pairs\n",
    "    '''\n",
    "    doc_words = inaugural.words(doc_name)\n",
    "    # Construct a frequency distribution over the lowercased words in the document\n",
    "    fd_doc_words = \n",
    "    # Find the top x most frequently used words in the document\n",
    "    top_words = \n",
    "    # Return the top x most frequently used words\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to estimate the list of top 50 most frequent words of Barack Obama’s 2009 speech and\n",
    "George Washington’s 1789 speech. \n",
    "\n",
    "What can knowing word frequencies tell us about different speeches at different\n",
    "times in history?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 50 words for Obama's 2009 speech: \\n {}\".format(mostFreq('2009-Obama.txt')))\n",
    "print(\"Top 50 words for Washington's 1789 speech: \\n {}\".format(mostFreq('1789-Washington.txt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "A statistical language model assigns a probability to a sequence of words, using a probability distribution. Language models have many applications in Natural Language Processing. For example, in speech recognition, they may be used to predict the next word that a speaker will utter. In machine translation, a language model may be used to score multiple candidate translations of an input sentence to find the most fluent/natural translation from the set of candidates.\n",
    "\n",
    "In this course, to build language models we will use ``nltk_models`` package that you extracted together with this lab. It contains two classes:\n",
    "\n",
    "- ``NgramModel``: word-level ngram builder, given the desired probability estimator\n",
    "- ``LgramModel``: char-level ngram builder, given the desired probability estimator\n",
    "\n",
    "Documentation for this package can be found [here](https://tardis.ed.ac.uk/~fox/entries/nltk-model.html).\n",
    "\n",
    "Each of these classes has the following initialization:\n",
    "\n",
    "```python\n",
    "    def __init__(self, \n",
    "                 n,                    # Order of the Language model:1=unigram; 2=bigram; 3=trigram, etc.\n",
    "                 train,                # Training data (list)\n",
    "                 pad_left=False,       # Perform left padding\n",
    "                 pad_right=False,      # Perform right padding\n",
    "                 estimator=None,       # Probability distribution estimator (may or may not be smoothed)\n",
    "                 *estimator_args,      # Optional arguments for estimator\n",
    "                 **estimator_kwargs): \n",
    "```\n",
    "To import the classes execute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from nltk_model import *  # See the README inside the nltk_model folder for more information\n",
    "except ImportError:\n",
    "    from .nltk_model import * # Compatibility depending on how this script was run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Create a function ``estimateLM`` that estimates a simple a language model using particular document of the Gutenberg corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateLM(doc_name, n):\n",
    "    '''\n",
    "    :type doc_name: string\n",
    "    :param doc_name : name of the document in gutenberg corpus.\n",
    "    :type n: int\n",
    "    :param n: order of the ngram to be estimated.\n",
    "    :rtype: NgramModel:\n",
    "    :return: language model, estimated by nltk.WittenBellProbDist estimator (default)\n",
    "    '''\n",
    "    # Construct a list of lowercase words from the document\n",
    "    words = [w.lower() for w in gutenberg.words(doc_name)]\n",
    "    lm = NgramModel(<order>,<training_data>)\n",
    "    \n",
    "    return lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function by creating a language model for a novel Sense and Sensibility by Jane Austen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = estimateLM('austen-sense.txt', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Probabilities\n",
    "\n",
    "Using the language model, we can work out the probability of a word given its context. In the case of the bigram language model build in Exercise 4, we can use a ``prob`` method of ``NgramModel`` which takes the following arguments:\n",
    "\n",
    "- ``word``: word to which the probability (score) you want to estimate\n",
    "- ``context``: a list of words that occurred before, required for ngram estimation. In the case of the bigram context is a list containing just the previous word.\n",
    "\n",
    "### Exercise 5\n",
    "\n",
    "Using the bigram language model build in Exercise 4, compute the following probabilities:\n",
    "\n",
    "1. ``reason`` followed by ``for``\n",
    "2. ``the`` followed by ``end``\n",
    "3. ``end`` followed by ``the``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Probability of 'reason' followed by 'for': {:.5f}\".format(lm.prob(word=<word>, context=[<context>])))\n",
    "print(\"Probability of 'the' followed by 'end': {:.5f}\".format(lm.prob(word=<word>, context=[<context>])))\n",
    "print(\"Probability of 'end' followed by 'the': {:.5f}\".format(lm.prob(word=<word>, context=[<context>])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Further\n",
    "\n",
    "### Smoothing\n",
    "\n",
    "Try using an estimator which does do smoothing, and see what happens to all three of the bigram probabilities. Try `help(NgramModel)` for help with the operation of this class and how to supply estimators.\n",
    "\n",
    "### Padding\n",
    "\n",
    "So far you’ve treated the data as a flat list of ‘words’, which doesn’t fully address the place of words within sentences. Using `gutenberg.sents(...)` explore the impact of the `pad left` and `pad right` argument to `NgramModel` by further editing `estimateLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.prob(word='The', context=['<s>']))\n",
    "print(lm.prob(word='the', context=['<s>']))\n",
    "print(lm.prob(word='End', context=['<s/>']))\n",
    "print(lm.prob(word='end', context=['<s/>']))\n",
    "print(lm.prob(word='.', context=['<s/>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost vs. probabilities\n",
    "\n",
    "Redo the previous two sub-sections using *costs* instead of probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
