{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9IS9B9-yUU5"
   },
   "source": [
    "## Setup PyTorch\n",
    "All files are stored at /content/csc421/a3/ folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "Z-6MQhMOlHXD",
    "outputId": "b4f07d06-91e8-424a-d7ea-97a41bc40b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.1.1)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
      "Collecting Pillow==4.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl (5.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.6MB 6.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n",
      "\u001b[31mtorchvision 0.2.2.post3 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
      "\u001b[31mfastai 1.0.49 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
      "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
      "Installing collected packages: Pillow\n",
      "  Found existing installation: Pillow 4.1.1\n",
      "    Uninstalling Pillow-4.1.1:\n",
      "      Successfully uninstalled Pillow-4.1.1\n",
      "Successfully installed Pillow-4.0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "PIL"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/csc421/a3\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Setup python environment and change the current working directory\n",
    "######################################################################\n",
    "!pip install torch torchvision\n",
    "!pip install Pillow==4.0.0\n",
    "%mkdir -p /content/csc421/a3/\n",
    "%cd /content/csc421/a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9DaTdRNuUra7"
   },
   "source": [
    "# Helper code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4BIpGwANoQOg"
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-UJHBYZkh7f"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tarfile\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_file(fname,\n",
    "             origin,\n",
    "             untar=False,\n",
    "             extract=False,\n",
    "             archive_format='auto',\n",
    "             cache_dir='data'):\n",
    "    datadir = os.path.join(cache_dir)\n",
    "    if not os.path.exists(datadir):\n",
    "        os.makedirs(datadir)\n",
    "\n",
    "    if untar:\n",
    "        untar_fpath = os.path.join(datadir, fname)\n",
    "        fpath = untar_fpath + '.tar.gz'\n",
    "    else:\n",
    "        fpath = os.path.join(datadir, fname)\n",
    "    \n",
    "    print(fpath)\n",
    "    if not os.path.exists(fpath):\n",
    "        print('Downloading data from', origin)\n",
    "\n",
    "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
    "        try:\n",
    "            try:\n",
    "                urlretrieve(origin, fpath)\n",
    "            except URLError as e:\n",
    "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
    "            except HTTPError as e:\n",
    "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
    "        except (Exception, KeyboardInterrupt) as e:\n",
    "            if os.path.exists(fpath):\n",
    "                os.remove(fpath)\n",
    "            raise\n",
    "\n",
    "    if untar:\n",
    "        if not os.path.exists(untar_fpath):\n",
    "            print('Extracting file.')\n",
    "            with tarfile.open(fpath) as archive:\n",
    "                archive.extractall(datadir)\n",
    "        return untar_fpath\n",
    "\n",
    "    if extract:\n",
    "        _extract_archive(fpath, datadir, archive_format)\n",
    "\n",
    "    return fpath\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        \n",
    "def to_var(tensor, cuda):\n",
    "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
    "\n",
    "        Arguments:\n",
    "            tensor: A Tensor object.\n",
    "            cuda: A boolean flag indicating whether to use the GPU.\n",
    "\n",
    "        Returns:\n",
    "            A Variable object, on the GPU if cuda==True.\n",
    "    \"\"\"\n",
    "    if cuda:\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "\n",
    "def create_dir_if_not_exists(directory):\n",
    "    \"\"\"Creates a directory if it doesn't already exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def save_loss_plot(train_losses, val_losses, opts):\n",
    "    \"\"\"Saves a plot of the training and validation loss curves.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(train_losses)), train_losses)\n",
    "    plt.plot(range(len(val_losses)), val_losses)\n",
    "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
    "    plt.xlabel('Epochs', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def checkpoint(encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
    "    contains the char_to_index and index_to_char mappings, and the start_token\n",
    "    and end_token values.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
    "        torch.save(encoder, f)\n",
    "\n",
    "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
    "        torch.save(decoder, f)\n",
    "\n",
    "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
    "        pkl.dump(idx_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbvpn4MaV0I1"
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVT4TNTOV3Eg"
   },
   "outputs": [],
   "source": [
    "def read_lines(filename):\n",
    "    \"\"\"Read a file and split it into lines.\n",
    "    \"\"\"\n",
    "    lines = open(filename).read().strip().lower().split('\\n')\n",
    "    return lines\n",
    "\n",
    "\n",
    "def read_pairs(filename):\n",
    "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
    "\n",
    "    Returns:\n",
    "        source_words: A list of the first word in each line of the file.\n",
    "        target_words: A list of the second word in each line of the file.\n",
    "    \"\"\"\n",
    "    lines = read_lines(filename)\n",
    "    source_words, target_words = [], []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            source, target = line.split()\n",
    "            source_words.append(source)\n",
    "            target_words.append(target)\n",
    "    return source_words, target_words\n",
    "\n",
    "\n",
    "def all_alpha_or_dash(s):\n",
    "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
    "    \"\"\"\n",
    "    return all(c.isalpha() or c == '-' for c in s)\n",
    "\n",
    "\n",
    "def filter_lines(lines):\n",
    "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
    "    \"\"\"\n",
    "    return [line for line in lines if all_alpha_or_dash(line)]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
    "    \"\"\"\n",
    "\n",
    "    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n",
    "\n",
    "    # Filter lines\n",
    "    source_lines = filter_lines(source_lines)\n",
    "    target_lines = filter_lines(target_lines)\n",
    "\n",
    "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
    "\n",
    "    # Create a dictionary mapping each character to a unique index\n",
    "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
    "\n",
    "    # Add start and end tokens to the dictionary\n",
    "    start_token = len(char_to_index)\n",
    "    end_token = len(char_to_index) + 1\n",
    "    char_to_index['SOS'] = start_token\n",
    "    char_to_index['EOS'] = end_token\n",
    "\n",
    "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
    "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
    "\n",
    "    # Store the final size of the vocabulary\n",
    "    vocab_size = len(char_to_index)\n",
    "\n",
    "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
    "\n",
    "    idx_dict = { 'char_to_index': char_to_index,\n",
    "                 'index_to_char': index_to_char,\n",
    "                 'start_token': start_token,\n",
    "                 'end_token': end_token }\n",
    "\n",
    "    return line_pairs, vocab_size, idx_dict\n",
    "\n",
    "\n",
    "def create_dict(pairs):\n",
    "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
    "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
    "    all source indexes and the other containing all corresponding target indexes.\n",
    "    Within a batch, all the source words are the same length, and all the target words are\n",
    "    the same length.\n",
    "    \"\"\"\n",
    "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
    "\n",
    "    d = defaultdict(list)\n",
    "    for (s,t) in unique_pairs:\n",
    "        d[(len(s), len(t))].append((s,t))\n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bRWfRdmVVjUl"
   },
   "source": [
    "## Training and evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wa5-onJhoSeM"
   },
   "outputs": [],
   "source": [
    "def string_to_index_list(s, char_to_index, end_token):\n",
    "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
    "    \"\"\"\n",
    "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
    "\n",
    "\n",
    "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
    "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
    "    word independently, and then stitching the words back together with spaces between them.\n",
    "    \"\"\"\n",
    "    if idx_dict is None:\n",
    "      line_pairs, vocab_size, idx_dict = load_data()\n",
    "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
    "\n",
    "\n",
    "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Translates a given string from English to Pig-Latin.\n",
    "    \"\"\"\n",
    "\n",
    "    char_to_index = idx_dict['char_to_index']\n",
    "    index_to_char = idx_dict['index_to_char']\n",
    "    start_token = idx_dict['start_token']\n",
    "    end_token = idx_dict['end_token']\n",
    "\n",
    "    max_generated_chars = 20\n",
    "    gen_string = ''\n",
    "\n",
    "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
    "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
    "\n",
    "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
    "\n",
    "    decoder_hidden = encoder_last_hidden\n",
    "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
    "    decoder_inputs = decoder_input\n",
    "\n",
    "    for i in range(max_generated_chars):\n",
    "      ## slow decoding, recompute everything at each time\n",
    "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
    "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
    "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
    "      ni = ni[-1] #latest output token\n",
    "\n",
    "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
    "      \n",
    "      if ni == end_token:\n",
    "          break\n",
    "      else:\n",
    "          gen_string = \"\".join(\n",
    "              [index_to_char[int(item)] \n",
    "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
    "\n",
    "    return gen_string\n",
    "\n",
    "\n",
    "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
    "    \"\"\"\n",
    "    if idx_dict is None:\n",
    "      line_pairs, vocab_size, idx_dict = load_data()\n",
    "    char_to_index = idx_dict['char_to_index']\n",
    "    index_to_char = idx_dict['index_to_char']\n",
    "    start_token = idx_dict['start_token']\n",
    "    end_token = idx_dict['end_token']\n",
    "\n",
    "    max_generated_chars = 20\n",
    "    gen_string = ''\n",
    "\n",
    "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
    "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
    "\n",
    "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
    "    decoder_inputs = decoder_input\n",
    "\n",
    "    produced_end_token = False\n",
    "\n",
    "    for i in range(max_generated_chars):\n",
    "      ## slow decoding, recompute everything at each time\n",
    "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
    "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
    "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
    "      ni = ni[-1] #latest output token\n",
    "      \n",
    "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
    "      \n",
    "      if ni == end_token:\n",
    "          break\n",
    "      else:\n",
    "          gen_string = \"\".join(\n",
    "              [index_to_char[int(item)] \n",
    "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
    "    \n",
    "    if isinstance(attention_weights, tuple):\n",
    "      ## transformer's attention mweights\n",
    "      attention_weights, self_attention_weights = attention_weights\n",
    "    \n",
    "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
    "    \n",
    "    for i in range(len(all_attention_weights)):\n",
    "      attention_weights_matrix = all_attention_weights[i].squeeze()\n",
    "      fig = plt.figure()\n",
    "      ax = fig.add_subplot(111)\n",
    "      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
    "      fig.colorbar(cax)\n",
    "\n",
    "      # Set up axes\n",
    "      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
    "      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
    "\n",
    "      # Show label at every tick\n",
    "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "      # Add title\n",
    "      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
    "      plt.tight_layout()\n",
    "      plt.grid('off')\n",
    "      plt.show()\n",
    "      #plt.savefig(save)\n",
    "\n",
    "      #plt.close(fig)\n",
    "\n",
    "    return gen_string\n",
    "\n",
    "\n",
    "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
    "    \"\"\"Train/Evaluate the model on a dataset.\n",
    "\n",
    "    Arguments:\n",
    "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
    "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
    "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
    "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
    "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
    "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
    "        opts: The command-line arguments.\n",
    "\n",
    "    Returns:\n",
    "        mean_loss: The average loss over all batches from data_dict.\n",
    "    \"\"\"\n",
    "    start_token = idx_dict['start_token']\n",
    "    end_token = idx_dict['end_token']\n",
    "    char_to_index = idx_dict['char_to_index']\n",
    "\n",
    "    losses = []\n",
    "    for key in data_dict:\n",
    "        input_strings, target_strings = zip(*data_dict[key])\n",
    "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
    "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
    "\n",
    "        num_tensors = len(input_tensors)\n",
    "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
    "\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            start = i * opts.batch_size\n",
    "            end = start + opts.batch_size\n",
    "\n",
    "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
    "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
    "\n",
    "            # The batch size may be different in each epoch\n",
    "            BS = inputs.size(0)\n",
    "\n",
    "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
    "\n",
    "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
    "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
    "\n",
    "            loss = 0.0\n",
    "\n",
    "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
    "\n",
    "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
    "            \n",
    "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
    "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
    "            targets_flatten = targets.view(-1)\n",
    "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            ## training if an optimizer is provided\n",
    "            if optimizer:\n",
    "              # Zero gradients\n",
    "              optimizer.zero_grad()\n",
    "              # Compute gradients\n",
    "              loss.backward()\n",
    "              # Update the parameters of the encoder and decoder\n",
    "              optimizer.step()\n",
    "              \n",
    "    mean_loss = np.mean(losses)\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
    "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
    "        * Prints training and val loss each epoch.\n",
    "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
    "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
    "\n",
    "    Arguments:\n",
    "        train_dict: The training word pairs, organized by source and target lengths.\n",
    "        val_dict: The validation word pairs, organized by source and target lengths.\n",
    "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
    "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
    "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
    "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
    "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
    "        opts: The command-line arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    start_token = idx_dict['start_token']\n",
    "    end_token = idx_dict['end_token']\n",
    "    char_to_index = idx_dict['char_to_index']\n",
    "\n",
    "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
    "\n",
    "    best_val_loss = 1e6\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(opts.nepochs):\n",
    "\n",
    "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
    "        \n",
    "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
    "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            checkpoint(encoder, decoder, idx_dict, opts)\n",
    "\n",
    "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
    "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
    "\n",
    "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
    "        loss_log.flush()\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        save_loss_plot(train_losses, val_losses, opts)\n",
    "\n",
    "\n",
    "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
    "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
    "    \"\"\"\n",
    "    print('=' * 80)\n",
    "    print('Data Stats'.center(80))\n",
    "    print('-' * 80)\n",
    "    for pair in line_pairs[:5]:\n",
    "        print(pair)\n",
    "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
    "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
    "    print('Vocab size: {}'.format(vocab_size))\n",
    "    print('=' * 80)\n",
    "\n",
    "\n",
    "def train(opts):\n",
    "    line_pairs, vocab_size, idx_dict = load_data()\n",
    "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
    "\n",
    "    # Split the line pairs into an 80% train and 20% val split\n",
    "    num_lines = len(line_pairs)\n",
    "    num_train = int(0.8 * num_lines)\n",
    "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
    "\n",
    "    # Group the data by the lengths of the source and target words, to form batches\n",
    "    train_dict = create_dict(train_pairs)\n",
    "    val_dict = create_dict(val_pairs)\n",
    "\n",
    "    ##########################################################################\n",
    "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
    "    ##########################################################################\n",
    "    encoder = GRUEncoder(vocab_size=vocab_size, \n",
    "                         hidden_size=opts.hidden_size, \n",
    "                         opts=opts)\n",
    "\n",
    "    if opts.decoder_type == 'rnn':\n",
    "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
    "                             hidden_size=opts.hidden_size)\n",
    "    elif opts.decoder_type == 'rnn_attention':\n",
    "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
    "                                      hidden_size=opts.hidden_size, \n",
    "                                      attention_type=opts.attention_type)\n",
    "    elif opts.decoder_type == 'transformer':\n",
    "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
    "                                     hidden_size=opts.hidden_size, \n",
    "                                     num_layers=opts.num_transformer_layers)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    #### setup checkpoint path\n",
    "    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n",
    "                                      opts.batch_size, \n",
    "                                      opts.decoder_type)\n",
    "    opts.checkpoint_path = model_name\n",
    "    create_dir_if_not_exists(opts.checkpoint_path)\n",
    "    ####\n",
    "\n",
    "    if opts.cuda:\n",
    "        encoder.cuda()\n",
    "        decoder.cuda()\n",
    "        print(\"Moved models to GPU!\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
    "\n",
    "    try:\n",
    "        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n",
    "    except KeyboardInterrupt:\n",
    "        print('Exiting early from training.')\n",
    "        return encoder, decoder\n",
    "    \n",
    "    return encoder, decoder\n",
    "\n",
    "\n",
    "def print_opts(opts):\n",
    "    \"\"\"Prints the values of all command-line arguments.\n",
    "    \"\"\"\n",
    "    print('=' * 80)\n",
    "    print('Opts'.center(80))\n",
    "    print('-' * 80)\n",
    "    for key in opts.__dict__:\n",
    "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
    "    print('=' * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_BAfi_8yWB3y"
   },
   "source": [
    "## GRU cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ztmyA5Ro67o"
   },
   "outputs": [],
   "source": [
    "class MyGRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MyGRUCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        ## Input linear layers\n",
    "        self.Wiz = nn.Linear(input_size, hidden_size)\n",
    "        self.Wir = nn.Linear(input_size, hidden_size)\n",
    "        self.Wih = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        ## Hidden linear layers\n",
    "        self.Whz = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Whr = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"Forward pass of the GRU computation for one time step.\n",
    "        Arguments\n",
    "            x: batch_size x input_size\n",
    "            h_prev: batch_size x hidden_size\n",
    "        Returns:\n",
    "            h_new: batch_size x hidden_size\n",
    "        \"\"\"\n",
    "        z = F.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
    "        r = F.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
    "        g = F.tanh(self.Wih(x) + r * self.Whh(h_prev))\n",
    "        h_new = (1 - z) * g + z * h_prev\n",
    "        return h_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JBVFLEZWNC1"
   },
   "source": [
    "### GRU encoder / decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xaDt7XDmWRzC"
   },
   "outputs": [],
   "source": [
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, opts):\n",
    "        super(GRUEncoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.opts = opts\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.gru = nn.GRUCell(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass of the encoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
    "\n",
    "        Returns:\n",
    "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "        annotations = []\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
    "            hidden = self.gru(x, hidden)\n",
    "            annotations.append(hidden)\n",
    "\n",
    "        annotations = torch.stack(annotations, dim=1)\n",
    "        return annotations, hidden\n",
    "\n",
    "    def init_hidden(self, bs):\n",
    "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
    "        of a batch of sequences.\n",
    "\n",
    "        Arguments:\n",
    "            bs: The batch size for the initial hidden state.\n",
    "\n",
    "        Returns:\n",
    "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
    "        \"\"\"\n",
    "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)\n",
    "\n",
    "\n",
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        #### Test MyGRUCell\n",
    "        # self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        self.rnn = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, annotations, hidden_init):\n",
    "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch. (batch_size x decoder_seq_len)\n",
    "            annotations: This is not used here. It just maintains consistency with the\n",
    "                    interface used by the AttentionDecoder class.\n",
    "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            None\n",
    "        \"\"\"\n",
    "        batch_size, decoder_seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x decoder_seq_len x hidden_size        \n",
    "\n",
    "        hiddens = []\n",
    "        h_prev = hidden_init\n",
    "        for i in range(decoder_seq_len):\n",
    "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
    "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
    "            hiddens.append(h_prev)\n",
    "\n",
    "        hiddens = torch.stack(hiddens, dim=1) # batch_size x decoder_seq_len x hidden_size\n",
    "        \n",
    "        output = self.out(hiddens)  # batch_size x decoder_seq_len x vocab_size\n",
    "        return output, None      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tWe0RO5FWajD"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GUK5A7CWhV8"
   },
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # A two layer fully-connected network\n",
    "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
    "        self.attention_network = nn.Sequential(\n",
    "                                    nn.Linear(hidden_size*2, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, 1)\n",
    "                                 )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the additive attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
    "\n",
    "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, hidden_size = keys.size()\n",
    "        expanded_queries = queries.unsqueeze(1).expand_as(keys)\n",
    "        concat_inputs = torch.cat((expanded_queries, keys), 2)\n",
    "        \n",
    "        unnormalized_attention = self.attention_network(concat_inputs)\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(torch.transpose(attention_weights, 1, 2), values)\n",
    "        return context, attention_weights\n",
    "        \n",
    "\n",
    "class ScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
    "\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, hidden_size = keys.size()\n",
    "        if queries.dim()<3:\n",
    "          queries = queries.unsqueeze(1)\n",
    "        q = torch.transpose(self.Q(queries),1,2)\n",
    "        k = self.K(keys)\n",
    "        v = self.V(values)\n",
    "        unnormalized_attention = torch.bmm(k, q) / self.scaling_factor # batch_size x seq_len x k\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(torch.transpose(attention_weights, 1, 2), v)\n",
    "        return context, attention_weights\n",
    "        \n",
    "\n",
    "class CausalScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CausalScaledDotAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.neg_inf = torch.tensor(-1e7)\n",
    "        \n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
    "\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, hidden_size = keys.size()\n",
    "        if queries.dim()!=3:\n",
    "          queries = queries.unsqueeze(1)\n",
    "        \n",
    "        q = self.Q(queries) # batch_size x (k) x hidden_size\n",
    "        k = self.K(keys)  # batch_size x seq_len x hidden_size\n",
    "        v = self.V(values)  # batch_size x seq_len x hidden_size\n",
    "        unnormalized_attention = torch.bmm(q,k.transpose(1,2)) / self.scaling_factor  # batch_size x k x seq_len\n",
    "        \n",
    "        mask = torch.tril(torch.ones((unnormalized_attention.size(1),unnormalized_attention.size(2)),dtype=torch.uint8)).unsqueeze(0).expand_as(unnormalized_attention)\n",
    "        unnormalized_attention[~mask] = self.neg_inf\n",
    "        \n",
    "        attention_weights = self.softmax(torch.transpose(unnormalized_attention,1,2))\n",
    "        context = torch.bmm(torch.transpose(attention_weights, 1, 2), v)\n",
    "        \n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pemjZo2XWtRt"
   },
   "source": [
    "### Attention decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfjF0Z-PWwPv"
   },
   "outputs": [],
   "source": [
    "class RNNAttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
    "        super(RNNAttentionDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
    "        if attention_type == 'additive':\n",
    "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
    "        elif attention_type == 'scaled_dot':\n",
    "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs, annotations, hidden_init):\n",
    "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
    "            annotations: The encoder hidden states for each step of the input.\n",
    "                         sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x seq_len x decoder_seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, decoder_seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x decoder_seq_len x hidden_size        \n",
    "\n",
    "        hiddens = []\n",
    "        attentions = []\n",
    "        h_prev = hidden_init\n",
    "        for i in range(decoder_seq_len):\n",
    "            embed_current = embed[:,i,:]\n",
    "            context, attention_weights = self.attention(h_prev,annotations,annotations)\n",
    "            # scalerattention: context(batch_size,k,hidden_size) attention_weights(batch_size,seq_len,k)\n",
    "            embed_and_context = torch.cat((torch.squeeze(context,1), embed_current),1)\n",
    "            h_prev = self.rnn(embed_and_context,h_prev)\n",
    "            \n",
    "            hiddens.append(h_prev)\n",
    "            attentions.append(attention_weights)\n",
    "\n",
    "        hiddens = torch.stack(hiddens, dim=1) # batch_size x decoder_seq_len x hidden_size\n",
    "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x decoder_seq_len\n",
    "        \n",
    "        output = self.out(hiddens)  # batch_size x decoder_seq_len x vocab_size\n",
    "        return output, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8JpcwTRW5cw"
   },
   "source": [
    "### Transformer decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5vJPku1W7sz"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
    "                                    hidden_size=hidden_size, \n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
    "                                    hidden_size=hidden_size, \n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
    "                                    nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs, annotations, hidden_init):\n",
    "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
    "        \n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
    "            annotations: The encoder hidden states for each step of the input.\n",
    "                         sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden_init: Not used in the transformer decoder\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x decoder_seq_len x hidden_size        \n",
    "        \n",
    "        encoder_attention_weights_list = []\n",
    "        self_attention_weights_list = []\n",
    "        contexts = embed\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "          new_contexts, self_attention_weights = self.self_attentions[i](contexts,annotations,annotations)\n",
    "          residual_contexts = contexts + new_contexts\n",
    "          new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts,annotations,annotations)\n",
    "          residual_contexts = residual_contexts + new_contexts\n",
    "          new_contexts = self.attention_mlps[i](residual_contexts)\n",
    "          contexts = new_contexts + residual_contexts\n",
    "\n",
    "          encoder_attention_weights_list.append(encoder_attention_weights)\n",
    "          self_attention_weights_list.append(self_attention_weights)\n",
    "          \n",
    "        output = self.out(contexts)\n",
    "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
    "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
    "        \n",
    "        return output, (encoder_attention_weights, self_attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XuNFd6LNo0-o"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kiUwiOITHTW4"
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "xwcFjsEpHRbI",
    "outputId": "363d23b6-8a18-49b5-af60-b5b4867b0ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pig_latin_data.txt\n",
      "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_data.txt\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Download Translation datasets\n",
    "######################################################################\n",
    "data_fpath = get_file(fname='pig_latin_data.txt', \n",
    "                      origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n",
    "                      untar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hmQmyJDSRFKR"
   },
   "source": [
    "## RNN decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2358
    },
    "colab_type": "code",
    "id": "0LKaRF1jwhH7",
    "outputId": "73ea0679-7859-4cfc-a170-5f8d94387d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 100                                    \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.005                                  \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 20                                     \n",
      "                           decoder_type: rnn                                    \n",
      "                         attention_type:                                        \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('person', 'ersonpay')\n",
      "('paces', 'acespay')\n",
      "('retreated', 'etreatedray')\n",
      "('shutting', 'uttingshay')\n",
      "('placed', 'acedplay')\n",
      "Num unique word pairs: 6387\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNNDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 | Train loss: 2.336 | Val loss: 2.003 | Gen: ay ay onstay-ay ay ontintay\n",
      "Epoch:   1 | Train loss: 1.882 | Val loss: 1.872 | Gen: ay ay onsay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay onway\n",
      "Epoch:   2 | Train loss: 1.734 | Val loss: 1.770 | Gen: ay-away away onsionsay-ingway illay-ay-ay-ay-ay-en ontay-ay-ay-ay-entay\n",
      "Epoch:   3 | Train loss: 1.616 | Val loss: 1.709 | Gen: away away-away-ay-away-ay onsingsay-insay-ingw ationway onsay-ingway\n",
      "Epoch:   4 | Train loss: 1.535 | Val loss: 1.653 | Gen: etay-away away-away-ay-away-ay onsay-inssay-ingway issay-ay-ay-ay-ay-ay oodday\n",
      "Epoch:   5 | Train loss: 1.479 | Val loss: 1.642 | Gen: estay away-ay-ay-ay-ay-ay- onstionsay-ingstay away-ay-ay-ay-ay-ay- oodpay-ay-ay-ay-ay-a\n",
      "Epoch:   6 | Train loss: 1.434 | Val loss: 1.573 | Gen: estay away-away-away onsay-instay-onsay-i issay oorsay-onsay-away-aw\n",
      "Epoch:   7 | Train loss: 1.379 | Val loss: 1.564 | Gen: esay away-ay-ay-ay-ay-ay- onsay-inday-ay-ay-ay isay-ay-ay-ay-ay-ay ongay-ay-ay-ay-ay-ay\n",
      "Epoch:   8 | Train loss: 1.333 | Val loss: 1.510 | Gen: estay away-away-ayday onsay-indationsay isay-away oullway-ayday\n",
      "Epoch:   9 | Train loss: 1.292 | Val loss: 1.507 | Gen: etay away-away-away ongray-onsay-oday-aw isay-away ooulgsay-ayday\n",
      "Epoch:  10 | Train loss: 1.257 | Val loss: 1.455 | Gen: eway away-ayday onstiontionday-away isay-away ouligay-isay-away\n",
      "Epoch:  11 | Train loss: 1.230 | Val loss: 1.468 | Gen: esay away-away-ayday ongrationday isay-away oulgday-ayday\n",
      "Epoch:  12 | Train loss: 1.208 | Val loss: 1.417 | Gen: estay away-away-away-ayday ongrationday-ayday isay-away-ayday oodgray-atay-ayday\n",
      "Epoch:  13 | Train loss: 1.178 | Val loss: 1.456 | Gen: etay away-away-ayday onglingday isay-ayday ouliglay-away-ayday\n",
      "Epoch:  14 | Train loss: 1.166 | Val loss: 1.425 | Gen: etay away-away-away-ayday onglingday-ingstay isay-away-ayday oorday-ingstay\n",
      "Epoch:  15 | Train loss: 1.143 | Val loss: 1.351 | Gen: eway ilway onglingday-ay-away isay-ayday oondionday\n",
      "Epoch:  16 | Train loss: 1.111 | Val loss: 1.359 | Gen: ethay away-isay-away-yeay onglingday isay-ayday oodgray-oway-ayday\n",
      "Epoch:  17 | Train loss: 1.088 | Val loss: 1.299 | Gen: etay away-atestay onglitiongray isay-ayday ouliglingway\n",
      "Epoch:  18 | Train loss: 1.067 | Val loss: 1.277 | Gen: way away-awlay ongitionday isay-ybay ouliglay-ay-awlay\n",
      "Epoch:  19 | Train loss: 1.045 | Val loss: 1.262 | Gen: etay ilaway ongitiongray isay-awlay ouliglingway\n",
      "Epoch:  20 | Train loss: 1.028 | Val loss: 1.292 | Gen: etay ailway-away-awlay otiongutionday isay-awlay oulighay-ay-away-awa\n",
      "Epoch:  21 | Train loss: 1.022 | Val loss: 1.261 | Gen: ehay ilway ontioculiglyway isay ouliglingway\n",
      "Epoch:  22 | Train loss: 1.006 | Val loss: 1.248 | Gen: etay ilaway ongitiongray isay-awlay ouliglingway\n",
      "Epoch:  23 | Train loss: 1.002 | Val loss: 1.243 | Gen: etay allway-away-awlay ongitiongday isay ouliglyway\n",
      "Epoch:  24 | Train loss: 1.022 | Val loss: 1.281 | Gen: ethay illway-ay-ayssay oncoutionglay-ayday isay-aysay ouliglingway\n",
      "Epoch:  25 | Train loss: 0.982 | Val loss: 1.214 | Gen: ehay illway ongitionglyway isay ouliglingway\n",
      "Epoch:  26 | Train loss: 0.959 | Val loss: 1.180 | Gen: ehay illway ongingdinway isay ouliglyway\n",
      "Epoch:  27 | Train loss: 0.936 | Val loss: 1.159 | Gen: etay illaway ongitioncycay isay ouliglingway\n",
      "Epoch:  28 | Train loss: 0.919 | Val loss: 1.171 | Gen: ehay allway-ay-aysay ongitiongray isay ouliglyway\n",
      "Epoch:  29 | Train loss: 0.929 | Val loss: 1.377 | Gen: ehtay allay-ay-ay-ayeray ongingingday isay ouliglay-ay-ay-ay-ay\n",
      "Epoch:  30 | Train loss: 0.968 | Val loss: 1.196 | Gen: etay ilaway ongitionglay isay ouliglyway\n",
      "Epoch:  31 | Train loss: 0.924 | Val loss: 1.156 | Gen: ethay illway ongingingway issway oulgingway\n",
      "Epoch:  32 | Train loss: 0.883 | Val loss: 1.195 | Gen: ethay illway ongingingway issway oulightway\n",
      "Epoch:  33 | Train loss: 0.879 | Val loss: 1.175 | Gen: etay ilaray ongitingday isay oulgingway\n",
      "Epoch:  34 | Train loss: 0.861 | Val loss: 1.151 | Gen: ehay illaway ongingitionway isay ouliglyway\n",
      "Epoch:  35 | Train loss: 0.853 | Val loss: 1.094 | Gen: ehay illway oncouglintionday issway oulightinway\n",
      "Epoch:  36 | Train loss: 0.842 | Val loss: 1.195 | Gen: ehay illaway ongitingdray isay oulgingway\n",
      "Epoch:  37 | Train loss: 0.846 | Val loss: 1.083 | Gen: etay illaway ongintiongway isay ouliglyway\n",
      "Epoch:  38 | Train loss: 0.845 | Val loss: 1.139 | Gen: ehtay illaway ongitiondingway isay ounglyway\n",
      "Epoch:  39 | Train loss: 0.822 | Val loss: 1.091 | Gen: ethay illway onculigitionway issway ouligntway\n",
      "Epoch:  40 | Train loss: 0.809 | Val loss: 1.120 | Gen: ehay illaway ongitiondionway issway oulgingway\n",
      "Epoch:  41 | Train loss: 0.792 | Val loss: 1.071 | Gen: ehtay illaway onginitingchay issway ondulglyway\n",
      "Epoch:  42 | Train loss: 0.795 | Val loss: 1.126 | Gen: ehtay illaway onchingdray issway ondullyway-away-year\n",
      "Epoch:  43 | Train loss: 0.805 | Val loss: 1.083 | Gen: ehtay illaway ongitioningday issway ondullyway-iway-ayda\n",
      "Epoch:  44 | Train loss: 0.783 | Val loss: 1.069 | Gen: ehtay illaway oncoucingitiodway issway ondullyway\n",
      "Epoch:  45 | Train loss: 0.766 | Val loss: 1.155 | Gen: ethay illway onculigitioncay issway oulgityway\n",
      "Epoch:  46 | Train loss: 0.782 | Val loss: 1.179 | Gen: ehay illamay onculignitiongway isay ondultyway\n",
      "Epoch:  47 | Train loss: 0.771 | Val loss: 1.058 | Gen: ethay ailway oncoulightingway issway ondingway\n",
      "Epoch:  48 | Train loss: 0.757 | Val loss: 1.053 | Gen: ehtay ailfay oncidinglypray issway ondingway\n",
      "Epoch:  49 | Train loss: 0.756 | Val loss: 1.117 | Gen: ehtay ailfay ongitionityday issway ondlyblyway\n",
      "Epoch:  50 | Train loss: 0.791 | Val loss: 1.330 | Gen: ehtay ailfway ongioctulngway issway ounglybay\n",
      "Epoch:  51 | Train loss: 0.781 | Val loss: 1.032 | Gen: ehtay illamay oncigutioncyway issway ondingway\n",
      "Epoch:  52 | Train loss: 0.729 | Val loss: 1.060 | Gen: ethay ilmay onccuingitingway iseway ondingthay\n",
      "Epoch:  53 | Train loss: 0.707 | Val loss: 0.984 | Gen: ethay illaway oncidingitionway issway ondintyday\n",
      "Epoch:  54 | Train loss: 0.694 | Val loss: 0.986 | Gen: ehtay illaway oncidingitionway issay ondindway-oway\n",
      "Epoch:  55 | Train loss: 0.696 | Val loss: 1.063 | Gen: ehtay illaway onculigitingway issway ondindbray\n",
      "Epoch:  56 | Train loss: 0.713 | Val loss: 1.056 | Gen: ehtay ailoway oninginingdray issay ondingwray\n",
      "Epoch:  57 | Train loss: 0.725 | Val loss: 1.098 | Gen: ehtay ilarway ondicitingtay isay oulglynay-awlay\n",
      "Epoch:  58 | Train loss: 0.703 | Val loss: 1.040 | Gen: ethay ilarway ondingitingcray iseway ondinglyway\n",
      "Epoch:  59 | Train loss: 0.688 | Val loss: 1.105 | Gen: ehtay illaway oncidinicugitionday issway ondingway\n",
      "Epoch:  60 | Train loss: 0.680 | Val loss: 0.992 | Gen: ehtay ailmay oncigutioncionday issway ondingwray\n",
      "Epoch:  61 | Train loss: 0.668 | Val loss: 0.976 | Gen: ehtay illamay ondingitingnay issay ondintybay\n",
      "Epoch:  62 | Train loss: 0.667 | Val loss: 1.018 | Gen: ehtay ailway ondicitingcray issay ordglynay\n",
      "Epoch:  63 | Train loss: 0.667 | Val loss: 1.009 | Gen: ehtay ailway ondingicitycay issay ondingwray\n",
      "Epoch:  64 | Train loss: 0.655 | Val loss: 1.019 | Gen: ehtay illay ondinitingccyway iseway ordnglyway\n",
      "Epoch:  65 | Train loss: 0.684 | Val loss: 1.059 | Gen: ehtay ailway ondingitingcray issway-ay-ay-yeay ordingtay\n",
      "Epoch:  66 | Train loss: 0.673 | Val loss: 1.228 | Gen: ehay ilmay oncidingccay issay ondiondway\n",
      "Epoch:  67 | Train loss: 0.682 | Val loss: 0.966 | Gen: ehtay ailfuay ondingitingvay issay ordntiodway\n",
      "Epoch:  68 | Train loss: 0.649 | Val loss: 1.036 | Gen: ethay irarway ondingicincycay iseway ordnglyway\n",
      "Epoch:  69 | Train loss: 0.639 | Val loss: 0.953 | Gen: ehtay ailmay oncidingitionway issay ondiontway\n",
      "Epoch:  70 | Train loss: 0.625 | Val loss: 1.002 | Gen: ehtay ailway ondictiondionday issay ordgryway\n",
      "Epoch:  71 | Train loss: 0.630 | Val loss: 1.028 | Gen: ehtay ailmay ondingicincycay issay ordngray\n",
      "Epoch:  72 | Train loss: 0.622 | Val loss: 1.024 | Gen: ehtay irarway ondictionglay issway ordngray\n",
      "Epoch:  73 | Train loss: 0.630 | Val loss: 0.989 | Gen: ehtay ailway ondingitioncyway issay ordngray\n",
      "Epoch:  74 | Train loss: 0.616 | Val loss: 0.978 | Gen: ehteday ailway ondicignioncay issay ordngray\n",
      "Epoch:  75 | Train loss: 0.610 | Val loss: 1.005 | Gen: ehtay irarway ondiniticgndway iseway ordnglyway\n",
      "Epoch:  76 | Train loss: 0.612 | Val loss: 1.051 | Gen: ehtway ailway ondingitincay issay ordingway\n",
      "Epoch:  77 | Train loss: 0.638 | Val loss: 1.078 | Gen: ehtay irarway incidinginkway issay ordingway\n",
      "Epoch:  78 | Train loss: 0.626 | Val loss: 1.010 | Gen: ehtay ailway ondinitingcycay issay ordingway\n",
      "Epoch:  79 | Train loss: 0.609 | Val loss: 1.067 | Gen: ehtay ailmay ondincigucionway issay orkyingway\n",
      "Epoch:  80 | Train loss: 0.619 | Val loss: 1.071 | Gen: ehtay ailoway ondiniticgncyway issway ordntionday\n",
      "Epoch:  81 | Train loss: 0.606 | Val loss: 1.060 | Gen: ehtway ailoway onindingccionday iseway ordntionday\n",
      "Epoch:  82 | Train loss: 0.597 | Val loss: 0.962 | Gen: ehtway ailmay oncidingitionday issay ordntionday\n",
      "Epoch:  83 | Train loss: 0.581 | Val loss: 0.974 | Gen: ehtay ailmay ondincigutionway issay orkyingway\n",
      "Epoch:  84 | Train loss: 0.572 | Val loss: 0.941 | Gen: ehtay ailhay ondicitingday issay ordngray\n",
      "Epoch:  85 | Train loss: 0.574 | Val loss: 0.938 | Gen: ehtay ailmay ondincigityway issay ordngray\n",
      "Epoch:  86 | Train loss: 0.571 | Val loss: 0.968 | Gen: ehteway ailhay ondicitingshay issay ordingway\n",
      "Epoch:  87 | Train loss: 0.627 | Val loss: 1.095 | Gen: ethay ailfuay inckingingray issway ordngway\n",
      "Epoch:  88 | Train loss: 0.672 | Val loss: 0.944 | Gen: ehtway ailway ondingitingpay issway ordingway\n",
      "Epoch:  89 | Train loss: 0.591 | Val loss: 0.979 | Gen: ehtway ailmay ondincigutionway issay ordingway\n",
      "Epoch:  90 | Train loss: 0.569 | Val loss: 0.911 | Gen: ehtay ailuray onidingctionway issay ondiontway\n",
      "Epoch:  91 | Train loss: 0.553 | Val loss: 0.915 | Gen: ehtway ailhay ondiniticgday issay ordingway\n",
      "Epoch:  92 | Train loss: 0.544 | Val loss: 0.954 | Gen: ehtway ailuray ondinictiongway issay ordngray\n",
      "Epoch:  93 | Train loss: 0.546 | Val loss: 0.956 | Gen: ehtway ailhay ondiniticgnay issay ordingway\n",
      "Epoch:  94 | Train loss: 0.580 | Val loss: 1.165 | Gen: ehway ailquay ondincingincypay issay ordingway\n",
      "Epoch:  95 | Train loss: 0.621 | Val loss: 0.971 | Gen: ehtway ailuray ondinctiongway iseway orkyingway\n",
      "Epoch:  96 | Train loss: 0.563 | Val loss: 0.925 | Gen: ehtway ailfuay ondincingcray-awlay issay ordingway\n",
      "Epoch:  97 | Train loss: 0.546 | Val loss: 0.895 | Gen: ehtway ailouway ondinictiongway issay ordnay-imay\n",
      "Epoch:  98 | Train loss: 0.556 | Val loss: 0.941 | Gen: ehtway ailuray ondinictiongway issay ordngray\n",
      "Epoch:  99 | Train loss: 0.560 | Val loss: 1.019 | Gen: ehtay ailuray ondiniticgway issay ordngray\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tehtay ailuray ondiniticgway issay ordngray\n",
      "Time cost: 227.21425199508667\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "              'cuda':True, \n",
    "              'nepochs':100, \n",
    "              'checkpoint_dir':\"checkpoints\", \n",
    "              'learning_rate':0.005, \n",
    "              'lr_decay':0.99,\n",
    "              'batch_size':64, \n",
    "              'hidden_size':20, \n",
    "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
    "              'attention_type': '',  # options: additive / scaled_dot\n",
    "}\n",
    "args.update(args_dict)\n",
    "\n",
    "print_opts(args)\n",
    "start = time.time()\n",
    "rnn_encoder, rnn_decoder = train(args)\n",
    "end = time.time()\n",
    "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "print(\"Time cost:\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "p2kPGj5DFv7a",
    "outputId": "da135b06-21e6-4db7-e9b2-f6c469769a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t\tthing \n",
      "translated:\tingbay\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'thing'\n",
    "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cP7nl5NRJbu"
   },
   "source": [
    "## RNN attention decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2498
    },
    "colab_type": "code",
    "id": "nKlyfbuPDXDR",
    "outputId": "8f498ee3-ec48-48ef-d969-24cc2b3d3974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 100                                    \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.005                                  \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 20                                     \n",
      "                           decoder_type: rnn_attention                          \n",
      "                         attention_type: additive                               \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('person', 'ersonpay')\n",
      "('paces', 'acespay')\n",
      "('retreated', 'etreatedray')\n",
      "('shutting', 'uttingshay')\n",
      "('placed', 'acedplay')\n",
      "Num unique word pairs: 6387\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNNAttentionDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type MyGRUCell. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type AdditiveAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 | Train loss: 2.277 | Val loss: 1.973 | Gen: ay ay-ay onntintintay-indway osay ontintay-indway\n",
      "Epoch:   1 | Train loss: 1.809 | Val loss: 1.775 | Gen: ay-ay-ay-ay-ay-ay-ay atinway onconconcay-inconcay instay oncondway\n",
      "Epoch:   2 | Train loss: 1.609 | Val loss: 1.629 | Gen: ay-ay ay-inesway onconconconcay-incon issway oncondway\n",
      "Epoch:   3 | Train loss: 1.442 | Val loss: 1.593 | Gen: essay ay-eway onconcingingway insway onclingway\n",
      "Epoch:   4 | Train loss: 1.298 | Val loss: 1.432 | Gen: esay-esesay arinway oncincingingway issay oringway\n",
      "Epoch:   5 | Train loss: 1.142 | Val loss: 1.339 | Gen: esshay arinay ondndingingingway issay orrtingingway\n",
      "Epoch:   6 | Train loss: 1.003 | Val loss: 1.275 | Gen: ehay aray indininingingway issway oncinghay\n",
      "Epoch:   7 | Train loss: 0.920 | Val loss: 1.321 | Gen: eyway ainway ondnitiongay issway oroninghay\n",
      "Epoch:   8 | Train loss: 0.804 | Val loss: 1.213 | Gen: ehay airway onditiininingingay issay orkingigay\n",
      "Epoch:   9 | Train loss: 0.734 | Val loss: 1.171 | Gen: teshay airray-ay-ay-ay-ay-a onditiondingingway issway orkinginghay\n",
      "Epoch:  10 | Train loss: 0.679 | Val loss: 1.016 | Gen: ethay airway onditingingay issway oriingway\n",
      "Epoch:  11 | Train loss: 0.623 | Val loss: 1.030 | Gen: eestay ay-iway ondtiongday-ingday issay ortingday\n",
      "Epoch:  12 | Train loss: 0.594 | Val loss: 1.061 | Gen: ehay ainway oditingday issway orkingway\n",
      "Epoch:  13 | Train loss: 0.554 | Val loss: 0.955 | Gen: ehay ainway ondiitiongay issay oringay\n",
      "Epoch:  14 | Train loss: 0.491 | Val loss: 1.045 | Gen: ehay ainway onitiondgay issay oringsay\n",
      "Epoch:  15 | Train loss: 0.498 | Val loss: 0.752 | Gen: etay ainway onditioningsay issay oringsay\n",
      "Epoch:  16 | Train loss: 0.452 | Val loss: 0.826 | Gen: etay aincay onditioningday isway orkingsay\n",
      "Epoch:  17 | Train loss: 0.382 | Val loss: 0.676 | Gen: ehay ainway onditioningday isway orkingway\n",
      "Epoch:  18 | Train loss: 0.360 | Val loss: 0.842 | Gen: eetay airway onditioningway issay orkingway\n",
      "Epoch:  19 | Train loss: 0.411 | Val loss: 0.910 | Gen: ehay ainway onditioningingway isway orkingway\n",
      "Epoch:  20 | Train loss: 0.424 | Val loss: 0.899 | Gen: ehay ainway onditioningday isway orkinngsay\n",
      "Epoch:  21 | Train loss: 0.353 | Val loss: 0.706 | Gen: etay ainway ondititiongway isway orkingsay\n",
      "Epoch:  22 | Train loss: 0.355 | Val loss: 1.237 | Gen: etay ainway onditionnndngngdlay isway orkingngay\n",
      "Epoch:  23 | Train loss: 0.467 | Val loss: 0.636 | Gen: etay aincay onditioningway isway orkingay\n",
      "Epoch:  24 | Train loss: 0.308 | Val loss: 0.525 | Gen: etay ainway onditioningcay isway orkingmay\n",
      "Epoch:  25 | Train loss: 0.249 | Val loss: 0.482 | Gen: etay aincay onditioningnay isway orkingnay\n",
      "Epoch:  26 | Train loss: 0.223 | Val loss: 0.403 | Gen: etay airray onditioningnay isway orkingway\n",
      "Epoch:  27 | Train loss: 0.228 | Val loss: 0.430 | Gen: ehay airway onditioningway isway orkingway\n",
      "Epoch:  28 | Train loss: 0.214 | Val loss: 0.500 | Gen: etay airway onditioningcay isway oringway\n",
      "Epoch:  29 | Train loss: 0.234 | Val loss: 0.925 | Gen: eehay iray onitiondway isway oringway\n",
      "Epoch:  30 | Train loss: 0.447 | Val loss: 3.270 | Gen: ehtay aivivay oconcdiikitiioiorion isisay ororkiiningday\n",
      "Epoch:  31 | Train loss: 0.651 | Val loss: 0.489 | Gen: etay airway onditiondnay isway orkngmay\n",
      "Epoch:  32 | Train loss: 0.232 | Val loss: 0.429 | Gen: etay airway onditioningway isway orkingway\n",
      "Epoch:  33 | Train loss: 0.186 | Val loss: 0.373 | Gen: ethay airray onditioningway isway orkingway\n",
      "Epoch:  34 | Train loss: 0.166 | Val loss: 0.356 | Gen: etay airway onditioningway isway orkingway\n",
      "Epoch:  35 | Train loss: 0.166 | Val loss: 0.442 | Gen: ethay airray onditiionindway isway orkingway\n",
      "Epoch:  36 | Train loss: 0.158 | Val loss: 0.321 | Gen: ethay airway onditioningway isway orkingway\n",
      "Epoch:  37 | Train loss: 0.138 | Val loss: 0.305 | Gen: etay airway onditioningway isway orkingway\n",
      "Epoch:  38 | Train loss: 0.131 | Val loss: 0.594 | Gen: etay airway onitionnway isway orkingay\n",
      "Epoch:  39 | Train loss: 0.147 | Val loss: 0.245 | Gen: ettay airway onditioningway isway orkingway\n",
      "Epoch:  40 | Train loss: 0.116 | Val loss: 0.455 | Gen: esay airway onditiorngway isway oringway\n",
      "Epoch:  41 | Train loss: 0.282 | Val loss: 1.092 | Gen: ehay airway onditiongwray isway orkngway\n",
      "Epoch:  42 | Train loss: 0.343 | Val loss: 0.517 | Gen: etay airway onditioningchay isway orkingway\n",
      "Epoch:  43 | Train loss: 0.153 | Val loss: 0.363 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  44 | Train loss: 0.121 | Val loss: 0.288 | Gen: ettay airway onditioningray isway orkingway\n",
      "Epoch:  45 | Train loss: 0.105 | Val loss: 0.271 | Gen: ettay airway onditioningcay isway orkingway\n",
      "Epoch:  46 | Train loss: 0.105 | Val loss: 0.253 | Gen: ettay airway onditioningway isway orkingway\n",
      "Epoch:  47 | Train loss: 0.105 | Val loss: 0.313 | Gen: ettay airway onditionignway isway orkingway\n",
      "Epoch:  48 | Train loss: 0.097 | Val loss: 0.534 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  49 | Train loss: 0.081 | Val loss: 0.199 | Gen: ettay airway onditioningcay isway orkingway\n",
      "Epoch:  50 | Train loss: 0.130 | Val loss: 0.674 | Gen: ehay arway onditiongway isay orkngway\n",
      "Epoch:  51 | Train loss: 0.170 | Val loss: 0.335 | Gen: ethay airway onditioningray isway orkingway\n",
      "Epoch:  52 | Train loss: 0.086 | Val loss: 0.199 | Gen: ethay airway onditioningway isway orkingway\n",
      "Epoch:  53 | Train loss: 0.065 | Val loss: 0.182 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  54 | Train loss: 0.055 | Val loss: 0.169 | Gen: ethay airway onditioningray isway orkingway\n",
      "Epoch:  55 | Train loss: 0.048 | Val loss: 0.151 | Gen: ethay airway onditioningray isway orkingway\n",
      "Epoch:  56 | Train loss: 0.043 | Val loss: 0.155 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  57 | Train loss: 0.040 | Val loss: 0.147 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  58 | Train loss: 0.037 | Val loss: 0.162 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  59 | Train loss: 0.036 | Val loss: 0.144 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  60 | Train loss: 0.032 | Val loss: 0.143 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  61 | Train loss: 0.032 | Val loss: 0.208 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  62 | Train loss: 0.038 | Val loss: 0.226 | Gen: ethay airway onditionignway isway orkingway\n",
      "Epoch:  63 | Train loss: 0.043 | Val loss: 0.237 | Gen: ethay airway onditioniglway isway orkingway\n",
      "Epoch:  64 | Train loss: 0.116 | Val loss: 0.682 | Gen: ethay airway onditionningcay isway orkingway\n",
      "Epoch:  65 | Train loss: 0.125 | Val loss: 0.412 | Gen: tay airway onditioningcay isway orkingway\n",
      "Epoch:  66 | Train loss: 0.146 | Val loss: 0.414 | Gen: ethay airway onditionningchay isway orkingway\n",
      "Epoch:  67 | Train loss: 0.114 | Val loss: 0.222 | Gen: ethay airway onditioningchay isway orkinggray\n",
      "Epoch:  68 | Train loss: 0.056 | Val loss: 0.158 | Gen: ethay airway onditioningray isway orkingway\n",
      "Epoch:  69 | Train loss: 0.037 | Val loss: 0.133 | Gen: ethay airway onditioningray isway orkingway\n",
      "Epoch:  70 | Train loss: 0.029 | Val loss: 0.128 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  71 | Train loss: 0.025 | Val loss: 0.126 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  72 | Train loss: 0.022 | Val loss: 0.124 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  73 | Train loss: 0.021 | Val loss: 0.125 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  74 | Train loss: 0.019 | Val loss: 0.125 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  75 | Train loss: 0.018 | Val loss: 0.130 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  76 | Train loss: 0.016 | Val loss: 0.129 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  77 | Train loss: 0.015 | Val loss: 0.131 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  78 | Train loss: 0.014 | Val loss: 0.132 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  79 | Train loss: 0.014 | Val loss: 0.133 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  80 | Train loss: 0.013 | Val loss: 0.127 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  81 | Train loss: 0.014 | Val loss: 0.137 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  82 | Train loss: 0.011 | Val loss: 0.142 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  83 | Train loss: 0.010 | Val loss: 0.138 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  84 | Train loss: 0.139 | Val loss: 1.714 | Gen: eepay airway onditionfningway isway orfingway\n",
      "Epoch:  85 | Train loss: 0.273 | Val loss: 0.376 | Gen: ehay airway onditiongray isway orkingway\n",
      "Epoch:  86 | Train loss: 0.068 | Val loss: 0.219 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  87 | Train loss: 0.034 | Val loss: 0.164 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  88 | Train loss: 0.025 | Val loss: 0.146 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  89 | Train loss: 0.019 | Val loss: 0.142 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  90 | Train loss: 0.016 | Val loss: 0.137 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  91 | Train loss: 0.014 | Val loss: 0.130 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  92 | Train loss: 0.012 | Val loss: 0.128 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  93 | Train loss: 0.011 | Val loss: 0.126 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  94 | Train loss: 0.011 | Val loss: 0.126 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  95 | Train loss: 0.010 | Val loss: 0.127 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  96 | Train loss: 0.009 | Val loss: 0.126 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  97 | Train loss: 0.009 | Val loss: 0.126 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  98 | Train loss: 0.008 | Val loss: 0.124 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  99 | Train loss: 0.008 | Val loss: 0.125 | Gen: ethay airway onditioningcay isway orkingway\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay airway onditioningcay isway orkingway\n",
      "Time cost: 691.3675038814545\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "              'cuda':True, \n",
    "              'nepochs':100, \n",
    "              'checkpoint_dir':\"checkpoints\", \n",
    "              'learning_rate':0.005, \n",
    "              'lr_decay':0.99,\n",
    "              'batch_size':64, \n",
    "              'hidden_size':20, \n",
    "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
    "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
    "}\n",
    "args.update(args_dict)\n",
    "\n",
    "print_opts(args)\n",
    "start = time.time()\n",
    "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
    "end = time.time()\n",
    "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "print(\"Time cost:\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "e7tFrZ9TBZnx",
    "outputId": "deb87f24-d0e2-455a-e874-2d0fed5a1c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t\tsearch \n",
      "translated:\tearchsay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'search'\n",
    "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2498
    },
    "colab_type": "code",
    "id": "BqLZYCZTAFHQ",
    "outputId": "fd91c0b1-b6a1-46a0-e74e-d17c2ec81eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 100                                    \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.005                                  \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 20                                     \n",
      "                           decoder_type: rnn_attention                          \n",
      "                         attention_type: scaled_dot                             \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('person', 'ersonpay')\n",
      "('paces', 'acespay')\n",
      "('retreated', 'etreatedray')\n",
      "('shutting', 'uttingshay')\n",
      "('placed', 'acedplay')\n",
      "Num unique word pairs: 6387\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNNAttentionDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type MyGRUCell. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type ScaledDotAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 | Train loss: 2.242 | Val loss: 1.984 | Gen: ay ay ongay-ingay insay-ingay-ingay-in onsay-ingay-ingay-in\n",
      "Epoch:   1 | Train loss: 1.818 | Val loss: 1.828 | Gen: edway away onstingsay insay-ontay onssay-\n",
      "Epoch:   2 | Train loss: 1.669 | Val loss: 1.687 | Gen: elway-edway away oongsay ingsay ontingggway\n",
      "Epoch:   3 | Train loss: 1.555 | Val loss: 1.663 | Gen: entay away oongsay-iongway insay ontingsay\n",
      "Epoch:   4 | Train loss: 1.483 | Val loss: 1.551 | Gen: edway away oonsingway insway ongtway\n",
      "Epoch:   5 | Train loss: 1.411 | Val loss: 1.524 | Gen: eway away oongsay-iongay insay otingsay\n",
      "Epoch:   6 | Train loss: 1.351 | Val loss: 1.597 | Gen: eway away ongsay-ingway onsay erway-ingway\n",
      "Epoch:   7 | Train loss: 1.362 | Val loss: 1.464 | Gen: eway-eway away oongtingway iseway ompringsay\n",
      "Epoch:   8 | Train loss: 1.299 | Val loss: 1.482 | Gen: eway-eway away-ayway oongtingeway isway ompray-iongay\n",
      "Epoch:   9 | Train loss: 1.263 | Val loss: 1.400 | Gen: eway-ay away ongtingsay isay ompray-ingway\n",
      "Epoch:  10 | Train loss: 1.224 | Val loss: 1.572 | Gen: eay-eday away ongay-ongay isay ornay-ingay\n",
      "Epoch:  11 | Train loss: 1.248 | Val loss: 1.455 | Gen: eway away ongongongoningway isay oringway\n",
      "Epoch:  12 | Train loss: 1.163 | Val loss: 1.377 | Gen: eway away oningongway isay ompray-ingway\n",
      "Epoch:  13 | Train loss: 1.181 | Val loss: 1.495 | Gen: ensay away ongpray-inglay issay olrray\n",
      "Epoch:  14 | Train loss: 1.203 | Val loss: 1.419 | Gen: eaday away oningingway issay oringway\n",
      "Epoch:  15 | Train loss: 1.092 | Val loss: 1.320 | Gen: etway away ontingsingway issay ortingway\n",
      "Epoch:  16 | Train loss: 1.093 | Val loss: 1.262 | Gen: eway-ayday away ontingay-ingway issay orringway\n",
      "Epoch:  17 | Train loss: 1.013 | Val loss: 1.367 | Gen: eway away ontingray-ingway isay orringray-ingway\n",
      "Epoch:  18 | Train loss: 1.004 | Val loss: 1.294 | Gen: eay-ayday away-away ontiongsway isay-aypay oringlway\n",
      "Epoch:  19 | Train loss: 0.960 | Val loss: 1.245 | Gen: etay-ayway away onititiongway issay oringlay\n",
      "Epoch:  20 | Train loss: 0.937 | Val loss: 1.216 | Gen: eday away ongingway isay orrintingway\n",
      "Epoch:  21 | Train loss: 0.901 | Val loss: 1.326 | Gen: edway away ondititingway isway orpray-ingway\n",
      "Epoch:  22 | Train loss: 0.897 | Val loss: 1.205 | Gen: edway-ayday away-awlay onitiongway isay orringway\n",
      "Epoch:  23 | Train loss: 0.865 | Val loss: 1.198 | Gen: edhay array ontincintingway isway orringway\n",
      "Epoch:  24 | Train loss: 0.812 | Val loss: 1.196 | Gen: eay-ay airway-awlay ondgray-ingway isway orpray-ingway\n",
      "Epoch:  25 | Train loss: 0.752 | Val loss: 1.094 | Gen: ethay-ay airway ongtay-ingway isway orpray-ingway\n",
      "Epoch:  26 | Train loss: 0.730 | Val loss: 1.122 | Gen: eway airway ondincaningray isway orkningway\n",
      "Epoch:  27 | Train loss: 0.696 | Val loss: 1.071 | Gen: eay-ay ailway ondindingway isway orkningway\n",
      "Epoch:  28 | Train loss: 0.739 | Val loss: 1.282 | Gen: etway-ay awray oodincingway isway oorningway\n",
      "Epoch:  29 | Train loss: 0.780 | Val loss: 0.981 | Gen: eway airway ondincingway isway orkningway\n",
      "Epoch:  30 | Train loss: 0.644 | Val loss: 0.895 | Gen: edway airway ondtioncingway isway orkningway\n",
      "Epoch:  31 | Train loss: 0.595 | Val loss: 1.043 | Gen: eway airway onditiongway isway oruningway\n",
      "Epoch:  32 | Train loss: 0.585 | Val loss: 0.881 | Gen: eway airway ondtincingway isway orkingway\n",
      "Epoch:  33 | Train loss: 0.571 | Val loss: 1.353 | Gen: eway arway ondiongray isway orknay\n",
      "Epoch:  34 | Train loss: 0.662 | Val loss: 1.088 | Gen: eway airway-awlay ondittiongray isway orkingway\n",
      "Epoch:  35 | Train loss: 0.557 | Val loss: 0.830 | Gen: eway airway ondtitingray isway orkingway\n",
      "Epoch:  36 | Train loss: 0.472 | Val loss: 0.723 | Gen: ehay airway onditimongway isway orkingway\n",
      "Epoch:  37 | Train loss: 0.440 | Val loss: 0.743 | Gen: ehay airway ondtitiongmay isway orkinggway\n",
      "Epoch:  38 | Train loss: 0.410 | Val loss: 0.809 | Gen: ay-ay airway ondgincingway isway orkingway\n",
      "Epoch:  39 | Train loss: 0.462 | Val loss: 0.809 | Gen: ehay airway ondtitiongray isway orkingway\n",
      "Epoch:  40 | Train loss: 0.425 | Val loss: 0.991 | Gen: eway awray-awray ondgtitingway isway orkngngway\n",
      "Epoch:  41 | Train loss: 0.410 | Val loss: 0.763 | Gen: ehay airway ondtitiongway isway orkingway\n",
      "Epoch:  42 | Train loss: 0.371 | Val loss: 0.907 | Gen: etway airway ondimongingway isway orkingway\n",
      "Epoch:  43 | Train loss: 0.378 | Val loss: 0.777 | Gen: ehay airway ondticongingway isway orkngngay\n",
      "Epoch:  44 | Train loss: 0.360 | Val loss: 0.744 | Gen: ay-ay airway onditiongray isway orkingway\n",
      "Epoch:  45 | Train loss: 0.322 | Val loss: 0.703 | Gen: ehway airway ondtiongingray isway orkingway\n",
      "Epoch:  46 | Train loss: 0.299 | Val loss: 0.705 | Gen: way-ay airway onditiongmay isway orkingway\n",
      "Epoch:  47 | Train loss: 0.373 | Val loss: 0.818 | Gen: eway airway onditotingcay isway orkingway\n",
      "Epoch:  48 | Train loss: 0.305 | Val loss: 0.756 | Gen: ehay airway ondiongray isway orkingway\n",
      "Epoch:  49 | Train loss: 0.267 | Val loss: 0.704 | Gen: ehway airway onditiongmay isway orkingway\n",
      "Epoch:  50 | Train loss: 0.265 | Val loss: 0.934 | Gen: ehay airway ondiongmay isway orkingway\n",
      "Epoch:  51 | Train loss: 0.258 | Val loss: 0.736 | Gen: ehway airway onitiongray isway orkingway\n",
      "Epoch:  52 | Train loss: 0.247 | Val loss: 0.694 | Gen: ehay airway onditiongnay isway orkingway\n",
      "Epoch:  53 | Train loss: 0.237 | Val loss: 0.660 | Gen: ehay airway onditiongray isway orkingway\n",
      "Epoch:  54 | Train loss: 0.259 | Val loss: 0.783 | Gen: ehway airway onditiongimingingmay isway orkingway\n",
      "Epoch:  55 | Train loss: 0.251 | Val loss: 0.571 | Gen: ehay airway onditiongingray isway orkingway\n",
      "Epoch:  56 | Train loss: 0.234 | Val loss: 1.033 | Gen: way airway onditionguningnay isway orhingngway\n",
      "Epoch:  57 | Train loss: 0.274 | Val loss: 0.629 | Gen: ewsay airway onditioningray isway orkingway\n",
      "Epoch:  58 | Train loss: 0.230 | Val loss: 0.698 | Gen: ehay airway ondtioncingway isway orkingway\n",
      "Epoch:  59 | Train loss: 0.202 | Val loss: 0.487 | Gen: ehay airway onditiongray isway orkingway\n",
      "Epoch:  60 | Train loss: 0.215 | Val loss: 0.575 | Gen: ehay airway onditiongray isway orkingway\n",
      "Epoch:  61 | Train loss: 0.272 | Val loss: 0.854 | Gen: ewsay airway onditiongray isway owkingtay\n",
      "Epoch:  62 | Train loss: 0.280 | Val loss: 0.718 | Gen: eway airway onditicongingingming isway orkingway\n",
      "Epoch:  63 | Train loss: 0.206 | Val loss: 0.577 | Gen: etway airway onditionglay isway orkingway\n",
      "Epoch:  64 | Train loss: 0.222 | Val loss: 0.718 | Gen: ehay airway onditionglay isway orkingway\n",
      "Epoch:  65 | Train loss: 0.191 | Val loss: 0.458 | Gen: ehay airway onditionglay isway orkingway\n",
      "Epoch:  66 | Train loss: 0.157 | Val loss: 0.494 | Gen: ehay airway onditiongcay isway orkingway\n",
      "Epoch:  67 | Train loss: 0.136 | Val loss: 0.436 | Gen: ehay airway onditiongmay isway orkingway\n",
      "Epoch:  68 | Train loss: 0.131 | Val loss: 0.582 | Gen: ehay airway onditimngngnay isway orkingway\n",
      "Epoch:  69 | Train loss: 0.131 | Val loss: 0.400 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  70 | Train loss: 0.116 | Val loss: 0.416 | Gen: ehtay airway onditioningcay isway orkingway\n",
      "Epoch:  71 | Train loss: 0.119 | Val loss: 0.565 | Gen: ehtay airway onditiongcay isway orkingway\n",
      "Epoch:  72 | Train loss: 0.135 | Val loss: 0.641 | Gen: ehay airway ondttoongcay isway orkingway\n",
      "Epoch:  73 | Train loss: 0.194 | Val loss: 0.678 | Gen: ehay airway onditioningray isway orkingway\n",
      "Epoch:  74 | Train loss: 0.164 | Val loss: 0.610 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  75 | Train loss: 0.161 | Val loss: 0.490 | Gen: ehay airway onditioningray isway orkingway\n",
      "Epoch:  76 | Train loss: 0.113 | Val loss: 0.384 | Gen: ethay airway onditiongray isway orkingway\n",
      "Epoch:  77 | Train loss: 0.107 | Val loss: 0.605 | Gen: esway airway onditiongcay isway orkingway\n",
      "Epoch:  78 | Train loss: 0.140 | Val loss: 0.492 | Gen: ehtay airway onditioningcay isway orkingway\n",
      "Epoch:  79 | Train loss: 0.119 | Val loss: 1.084 | Gen: ehay airway onditionnaningday isway orkingway\n",
      "Epoch:  80 | Train loss: 0.153 | Val loss: 0.757 | Gen: ehtay airway onditiongsay isway orkingway\n",
      "Epoch:  81 | Train loss: 0.266 | Val loss: 0.888 | Gen: ehtay airway onditionciongcay iisway orkingway\n",
      "Epoch:  82 | Train loss: 0.201 | Val loss: 0.490 | Gen: ehay airway onditiongcay isway orkingway\n",
      "Epoch:  83 | Train loss: 0.116 | Val loss: 0.443 | Gen: ehay airway onditiongcay isway orkingway\n",
      "Epoch:  84 | Train loss: 0.089 | Val loss: 0.354 | Gen: ehay airway onditiongingcay isway orkingway\n",
      "Epoch:  85 | Train loss: 0.076 | Val loss: 0.343 | Gen: ehtay airway onditiongingcay isway orkingway\n",
      "Epoch:  86 | Train loss: 0.071 | Val loss: 0.341 | Gen: ehtay airway onditiongingcay isway orkingway\n",
      "Epoch:  87 | Train loss: 0.067 | Val loss: 0.338 | Gen: ehtay airway onditiongcay isway orkingway\n",
      "Epoch:  88 | Train loss: 0.064 | Val loss: 0.332 | Gen: ehtay airway onditiongcay isway orkingway\n",
      "Epoch:  89 | Train loss: 0.066 | Val loss: 0.463 | Gen: ehtay airway onditioningmay isway orkingway\n",
      "Epoch:  90 | Train loss: 0.087 | Val loss: 0.428 | Gen: ehay airway onditiongcay isway orkingway\n",
      "Epoch:  91 | Train loss: 0.072 | Val loss: 0.368 | Gen: ehtay airway onditioningcay isway orkingway\n",
      "Epoch:  92 | Train loss: 0.068 | Val loss: 0.409 | Gen: ehtay airway onditioningcay isway orkingway\n",
      "Epoch:  93 | Train loss: 0.069 | Val loss: 0.386 | Gen: esway airway onditioningcay isway orkingway\n",
      "Epoch:  94 | Train loss: 0.080 | Val loss: 0.897 | Gen: ehay airway onditioningcay isway orkingway\n",
      "Epoch:  95 | Train loss: 0.177 | Val loss: 0.616 | Gen: ehay airway onditiongcay isway orkingway\n",
      "Epoch:  96 | Train loss: 0.108 | Val loss: 0.397 | Gen: ehay airway onditiongcay isway orkingway\n",
      "Epoch:  97 | Train loss: 0.071 | Val loss: 0.360 | Gen: ehay airway onditioningcay isway orkingway\n",
      "Epoch:  98 | Train loss: 0.061 | Val loss: 0.344 | Gen: ehay airway onditioningcay isway orkingway\n",
      "Epoch:  99 | Train loss: 0.055 | Val loss: 0.316 | Gen: ethay airway onditiongmay isway orkingway\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay airway onditiongmay isway orkingway\n",
      "Time cost: 723.786470413208\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "              'cuda':True, \n",
    "              'nepochs':100, \n",
    "              'checkpoint_dir':\"checkpoints\", \n",
    "              'learning_rate':0.005,\n",
    "              'lr_decay':0.99,\n",
    "              'batch_size':64, \n",
    "              'hidden_size':20, \n",
    "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
    "              'attention_type': 'scaled_dot',  # options: additive / scaled_dot\n",
    "}\n",
    "args.update(args_dict)\n",
    "\n",
    "print_opts(args)\n",
    "start = time.time()\n",
    "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
    "end = time.time()\n",
    "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "print(\"Time cost:\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "vE-hKCxhF3iR",
    "outputId": "726ee00d-7fd8-43b6-b607-8604a4b953e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay airway onditiongmay isway orkingway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8FaZZUWRpY9"
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2428
    },
    "colab_type": "code",
    "id": "Ik5rx9qw9KCg",
    "outputId": "93202b98-b423-481b-dae4-b8bf85a285ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 100                                    \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.005                                  \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 20                                     \n",
      "                           decoder_type: transformer                            \n",
      "                 num_transformer_layers: 3                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('person', 'ersonpay')\n",
      "('paces', 'acespay')\n",
      "('retreated', 'etreatedray')\n",
      "('shutting', 'uttingshay')\n",
      "('placed', 'acedplay')\n",
      "Num unique word pairs: 6387\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type TransformerDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type CausalScaledDotAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type ScaledDotAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 | Train loss: 1.942 | Val loss: 1.871 | Gen: thehay ay ontinnnnnnnnay insay oray-ray\n",
      "Epoch:   1 | Train loss: 1.449 | Val loss: 1.688 | Gen: eheway alay onconnnnnnnnway isay ortrtrtay\n",
      "Epoch:   2 | Train loss: 1.306 | Val loss: 1.619 | Gen: othay alay inconay-inay isay ininkinay\n",
      "Epoch:   3 | Train loss: 1.198 | Val loss: 1.467 | Gen: ethay ayiray onditititiondiondion isay ay-ibray\n",
      "Epoch:   4 | Train loss: 1.100 | Val loss: 1.365 | Gen: iehay ayiray onay-onitinionay isay ay-ay-ay-ay-ay-ay-ay\n",
      "Epoch:   5 | Train loss: 1.034 | Val loss: 1.439 | Gen: thethethethethetheth aribribribribribribr onconcongncongngngng isay ngringway\n",
      "Epoch:   6 | Train loss: 1.012 | Val loss: 1.424 | Gen: etheway ayiray onayionayioay iseay-iseay-iseay-is ingwayinwararararara\n",
      "Epoch:   7 | Train loss: 1.038 | Val loss: 1.406 | Gen: eway ay-ay onininininninday isay inghblway\n",
      "Epoch:   8 | Train loss: 0.974 | Val loss: 1.352 | Gen: eway aray ondinnnnnnnndway isay orwangngay\n",
      "Epoch:   9 | Train loss: 0.912 | Val loss: 1.267 | Gen: eheway airay onicionconconcway isssssssssssssssssss orfrrrrwrwrwrwrwrwrw\n",
      "Epoch:  10 | Train loss: 0.908 | Val loss: 1.306 | Gen: aycay airay ondingdingdindway isay ingway-inway\n",
      "Epoch:  11 | Train loss: 1.023 | Val loss: 1.338 | Gen: ethay airay ondndndndndnway-ay-a isay way-ingway-ay-ay-ay-\n",
      "Epoch:  12 | Train loss: 1.016 | Val loss: 1.181 | Gen: ethetway airay ondninindninay isay orway-inay\n",
      "Epoch:  13 | Train loss: 0.867 | Val loss: 1.146 | Gen: ethetway airay ooonionionioncay isay orway-iway-iway-iway\n",
      "Epoch:  14 | Train loss: 0.813 | Val loss: 1.143 | Gen: ontway airay onsdcayonsdinway ishay ingringay-inay-inay-\n",
      "Epoch:  15 | Train loss: 0.847 | Val loss: 1.186 | Gen: othethethethethethet agray ondcititititidndndnd ishay ingingray\n",
      "Epoch:  16 | Train loss: 0.854 | Val loss: 1.152 | Gen: ethetttttttttttttttt artay ondindindindway-ay-a isssssssssssssssssss ewringray-ay-ay-ay-a\n",
      "Epoch:  17 | Train loss: 0.872 | Val loss: 1.135 | Gen: ethay aray ondisndsndcondndndnd isisisisisisisisisis ingrsngrngrngrngrngr\n",
      "Epoch:  18 | Train loss: 0.848 | Val loss: 1.137 | Gen: ethay aisway onayististionay isay orwingngway\n",
      "Epoch:  19 | Train loss: 0.874 | Val loss: 1.122 | Gen: ehtay airway ondconscondcay-ay-ay isssssssssssssssssss orngorfay\n",
      "Epoch:  20 | Train loss: 0.820 | Val loss: 1.218 | Gen: ehethethethethetheth arlay onansconsconaydayday isssssssssssssssssss orararay\n",
      "Epoch:  21 | Train loss: 0.863 | Val loss: 1.113 | Gen: ehthethethethethethe aybay ondititindcondway isay orayblngway\n",
      "Epoch:  22 | Train loss: 0.814 | Val loss: 1.089 | Gen: etheay airay ondiongcongcay isay orprwrway\n",
      "Epoch:  23 | Train loss: 0.802 | Val loss: 0.997 | Gen: eheheheheheheheheheh airay ondiongongongway ishay oraray-oray\n",
      "Epoch:  24 | Train loss: 0.726 | Val loss: 0.954 | Gen: ehecay airay ondiongcongcay ishay oringwray\n",
      "Epoch:  25 | Train loss: 0.703 | Val loss: 0.973 | Gen: ehecay ay-ay onininininiongway-in ishay oringringay\n",
      "Epoch:  26 | Train loss: 0.694 | Val loss: 0.943 | Gen: ehecay airday ondcongcongcay isway orkway-orway\n",
      "Epoch:  27 | Train loss: 0.665 | Val loss: 0.945 | Gen: ehetway airway oninininininay isway iningbrway\n",
      "Epoch:  28 | Train loss: 0.689 | Val loss: 1.028 | Gen: ethay airway onininginingway isway orkingway\n",
      "Epoch:  29 | Train loss: 0.705 | Val loss: 0.949 | Gen: ehtway airway oninininininsway isway orkinghway\n",
      "Epoch:  30 | Train loss: 0.676 | Val loss: 0.928 | Gen: ehethay ay-ay onionininginay isway orkngngway\n",
      "Epoch:  31 | Train loss: 0.646 | Val loss: 0.890 | Gen: ethay ay-ay onccondcondcay isway oray-ingway\n",
      "Epoch:  32 | Train loss: 0.674 | Val loss: 1.164 | Gen: ehechechechechechech ainwaynwaynwaynwaynw onninininnngsdnsdnsd isway orngway\n",
      "Epoch:  33 | Train loss: 0.731 | Val loss: 0.987 | Gen: ehepay airaywaywaywaywayway ondingcrqudrnay isway orknghray\n",
      "Epoch:  34 | Train loss: 0.809 | Val loss: 1.102 | Gen: epecay airway ondiongcaypray isway orkrkwray\n",
      "Epoch:  35 | Train loss: 0.859 | Val loss: 1.085 | Gen: ehecay airay ondionscondinay ishay oringwray\n",
      "Epoch:  36 | Train loss: 0.810 | Val loss: 1.018 | Gen: ehecay airway ondindindindway isay oraingway\n",
      "Epoch:  37 | Train loss: 0.793 | Val loss: 0.982 | Gen: ehecay airay oniongciongonay isay owaway\n",
      "Epoch:  38 | Train loss: 0.804 | Val loss: 1.000 | Gen: eheway airway ondindingcrcay isay ingwlwlay\n",
      "Epoch:  39 | Train loss: 0.789 | Val loss: 0.930 | Gen: ehecay airwaywaywaywaywaywa ondionscrcrcay isway orwwwwway\n",
      "Epoch:  40 | Train loss: 0.741 | Val loss: 0.940 | Gen: ehtay airay ontintinononay isay orwaybrway\n",
      "Epoch:  41 | Train loss: 0.706 | Val loss: 0.852 | Gen: ehtay airway oninnanionionay isway orkingrway\n",
      "Epoch:  42 | Train loss: 0.668 | Val loss: 0.996 | Gen: ehtay ay onindindindinway isay orwkingway\n",
      "Epoch:  43 | Train loss: 0.705 | Val loss: 0.895 | Gen: eheway airay ondiongciongway isway orway-inway\n",
      "Epoch:  44 | Train loss: 0.714 | Val loss: 0.927 | Gen: ehtway airaywaywaywaywayway ondiondindinway isway orwayingway\n",
      "Epoch:  45 | Train loss: 0.687 | Val loss: 0.921 | Gen: ethay airay ondiondiondinway isway orway-iay\n",
      "Epoch:  46 | Train loss: 0.701 | Val loss: 1.055 | Gen: ethay airwaywaywaywaywaywa onionionioniway isway orcorway\n",
      "Epoch:  47 | Train loss: 0.721 | Val loss: 0.878 | Gen: ehtay airaywaywaywaywayway ondionionininay isway orwawaway\n",
      "Epoch:  48 | Train loss: 0.632 | Val loss: 0.798 | Gen: eheway airaywaywaywaywayway onioniongcinway isway orkingway\n",
      "Epoch:  49 | Train loss: 0.585 | Val loss: 0.795 | Gen: eheay airwaywaywaywaywaywa oninininininway isway orwrwrway\n",
      "Epoch:  50 | Train loss: 0.605 | Val loss: 0.879 | Gen: ehthechechechecheche airway onnioninioniway isway orwrwrway\n",
      "Epoch:  51 | Train loss: 0.641 | Val loss: 0.850 | Gen: ehtway airway onionininininway isway orwringway\n",
      "Epoch:  52 | Train loss: 0.732 | Val loss: 0.897 | Gen: ehtay airway onndionionionway isway orwingngway\n",
      "Epoch:  53 | Train loss: 0.662 | Val loss: 0.931 | Gen: ehtay airway onionioniongway isway owingwrway\n",
      "Epoch:  54 | Train loss: 0.710 | Val loss: 1.081 | Gen: ehtay aywwwwwwwwwwwwwwwwww ondingcindindindindi isay owrngwway\n",
      "Epoch:  55 | Train loss: 0.805 | Val loss: 1.150 | Gen: ehehay ay-ay onininayingedway iray oringwway\n",
      "Epoch:  56 | Train loss: 0.808 | Val loss: 0.941 | Gen: ehtay ay-ay oninayinayingway isay oringwlway\n",
      "Epoch:  57 | Train loss: 0.716 | Val loss: 0.919 | Gen: ehecay airway onaninayccccay isway ingwingway\n",
      "Epoch:  58 | Train loss: 0.698 | Val loss: 0.861 | Gen: ehecay airway oningciningcay isway orngwrway\n",
      "Epoch:  59 | Train loss: 0.661 | Val loss: 0.836 | Gen: ehehay airway oniningcedcedway isway orngwrway\n",
      "Epoch:  60 | Train loss: 0.630 | Val loss: 0.826 | Gen: ehehay airway ondcininaycray isway orkingway\n",
      "Epoch:  61 | Train loss: 0.589 | Val loss: 0.779 | Gen: ehtay ayirway oninininingcay isway orwrwrway\n",
      "Epoch:  62 | Train loss: 0.592 | Val loss: 0.801 | Gen: ehecay ayiray onioniniongingway isway orwingway\n",
      "Epoch:  63 | Train loss: 0.568 | Val loss: 0.776 | Gen: ehecay airway onioningcongway isway orwrwrway\n",
      "Epoch:  64 | Train loss: 0.550 | Val loss: 0.851 | Gen: ehetay aynway onidiongciondway isway orkwingway\n",
      "Epoch:  65 | Train loss: 0.571 | Val loss: 0.748 | Gen: ehtay airway ondccciongceway isway orkingway\n",
      "Epoch:  66 | Train loss: 0.540 | Val loss: 0.798 | Gen: ehtay airway onditingcingway isway orkwrwrway\n",
      "Epoch:  67 | Train loss: 0.547 | Val loss: 0.815 | Gen: ehtay airway ontitongcingway isway orkway-way\n",
      "Epoch:  68 | Train loss: 0.593 | Val loss: 0.844 | Gen: ohtay airway oninininincinay isway orkingway\n",
      "Epoch:  69 | Train loss: 0.658 | Val loss: 0.879 | Gen: ohtay airway oninininingcay isway orway-iway\n",
      "Epoch:  70 | Train loss: 0.597 | Val loss: 0.833 | Gen: ehtay airway ontintingcponway isway orkingway\n",
      "Epoch:  71 | Train loss: 0.571 | Val loss: 0.782 | Gen: ehtay airway ondingcpongcay isway orkingway\n",
      "Epoch:  72 | Train loss: 0.551 | Val loss: 0.821 | Gen: ehtay airway oninininingcay isway orkingway\n",
      "Epoch:  73 | Train loss: 0.578 | Val loss: 0.949 | Gen: ohtay airway ondcongctingway isway owawingway\n",
      "Epoch:  74 | Train loss: 0.710 | Val loss: 0.918 | Gen: oheway airway onditiningsinay isway wrwringway\n",
      "Epoch:  75 | Train loss: 0.648 | Val loss: 0.892 | Gen: ohetay airway onditititidcay isway orwaywrway\n",
      "Epoch:  76 | Train loss: 0.618 | Val loss: 0.867 | Gen: ehtay airwaywaywaywaywaywa onditionioniony isway orwringway\n",
      "Epoch:  77 | Train loss: 0.594 | Val loss: 0.898 | Gen: oheway airwaywaywaywaywaywa ondsqunionionay isway orwrwrinway\n",
      "Epoch:  78 | Train loss: 0.626 | Val loss: 0.810 | Gen: ehetay airway onditcecececay isway owrwrwway\n",
      "Epoch:  79 | Train loss: 0.589 | Val loss: 0.739 | Gen: ehetay airway onidccccccccay isway orwwwwway\n",
      "Epoch:  80 | Train loss: 0.538 | Val loss: 0.702 | Gen: eheway airway ondititionionay isway orwwingway\n",
      "Epoch:  81 | Train loss: 0.524 | Val loss: 0.782 | Gen: ehetay airway ontintiontioncay isway orwrwwrway\n",
      "Epoch:  82 | Train loss: 0.558 | Val loss: 0.780 | Gen: eheway airway onidiongceceway isway orkingway\n",
      "Epoch:  83 | Train loss: 0.555 | Val loss: 0.773 | Gen: ethay airwwwwwwwwwwwwwwwww oniontiongcecay isway orkingway\n",
      "Epoch:  84 | Train loss: 0.526 | Val loss: 0.721 | Gen: ehetway airwwwwwwwwwwwwwwwww oniongcronenecay isway orkingway\n",
      "Epoch:  85 | Train loss: 0.481 | Val loss: 0.690 | Gen: ehetway airwwwwwwwwwwwwwwwww onioniongcecay isway orkingway\n",
      "Epoch:  86 | Train loss: 0.468 | Val loss: 0.713 | Gen: ehtway airway ondingciongcay isway orkingway\n",
      "Epoch:  87 | Train loss: 0.485 | Val loss: 0.721 | Gen: ehecay airway ondcrongcongay isway orkingway\n",
      "Epoch:  88 | Train loss: 0.489 | Val loss: 0.723 | Gen: ehthay airwwwwwwwwwwwwwwwww ondititiongcay isway orkngwray\n",
      "Epoch:  89 | Train loss: 0.476 | Val loss: 0.724 | Gen: ehecay airwwwwwwwwwwwwwwwww ondionionionay isway orkingway\n",
      "Epoch:  90 | Train loss: 0.472 | Val loss: 0.709 | Gen: ehecay airwwwwwwwwwwwwwwwww ondindionioncay isway orngwrnway\n",
      "Epoch:  91 | Train loss: 0.448 | Val loss: 0.718 | Gen: ehthay airwwwwwwwwwwwwwwwww ondcccccccccay isway orkngwrway\n",
      "Epoch:  92 | Train loss: 0.431 | Val loss: 0.724 | Gen: ehthay airay ondcccccccccay isway orkingrway\n",
      "Epoch:  93 | Train loss: 0.440 | Val loss: 0.781 | Gen: ehthay airwwwwwwwwwwwwwwwww ondcccrcrcrcay isway orkingrway\n",
      "Epoch:  94 | Train loss: 0.495 | Val loss: 0.738 | Gen: ehetay airwwwwwwwwwwwwwwwww ondititiongcay isway orkngwrway\n",
      "Epoch:  95 | Train loss: 0.529 | Val loss: 0.752 | Gen: ehethay airway ondititiongcay isway orkwrwrway\n",
      "Epoch:  96 | Train loss: 0.483 | Val loss: 0.688 | Gen: ehthay airway onditingcrcy isway orkngwrway\n",
      "Epoch:  97 | Train loss: 0.461 | Val loss: 0.712 | Gen: ehthay airwwwwwwwwwwwwwwwww onditingcinincay isway orkingway\n",
      "Epoch:  98 | Train loss: 0.464 | Val loss: 0.695 | Gen: ehthay airwwwwwwwwwwwwwwwww ondincingcedway isway orkingway\n",
      "Epoch:  99 | Train loss: 0.436 | Val loss: 0.685 | Gen: ehthay airwwwwwwwwwwwwwwwww ondincingcincay isway orkingway\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tehthay airwwwwwwwwwwwwwwwww ondincingcincay isway orkingway\n",
      "Time cost: 376.3806400299072\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "              'cuda':True,\n",
    "              'nepochs':100,\n",
    "              'checkpoint_dir':\"checkpoints\", \n",
    "              'learning_rate':0.005,\n",
    "              'lr_decay':0.99,\n",
    "              'batch_size':64, \n",
    "              'hidden_size':20, \n",
    "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
    "              'num_transformer_layers': 3,\n",
    "}\n",
    "args.update(args_dict)\n",
    "print_opts(args)\n",
    "start = time.time()\n",
    "transformer_encoder, transformer_decoder = train(args)\n",
    "end = time.time()\n",
    "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "print(\"Time cost:\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ULCMHm5ZF7vx",
    "outputId": "5a92e9d0-5393-46bb-a7fe-e1604ebe0619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t\tthe air conditioning is working \n",
      "translated:\tehthay airwwwwwwwwwwwwwwwww ondincingcincay isway orkingway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QyicXiUEVCpv"
   },
   "outputs": [],
   "source": [
    "## Modified transformerdecoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
    "                                    hidden_size=hidden_size, \n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
    "                                    hidden_size=hidden_size, \n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
    "                                    nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs, annotations, hidden_init):\n",
    "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
    "        \n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
    "            annotations: The encoder hidden states for each step of the input.\n",
    "                         sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden_init: Not used in the transformer decoder\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x decoder_seq_len x hidden_size        \n",
    "        \n",
    "        encoder_attention_weights_list = []\n",
    "        self_attention_weights_list = []\n",
    "        contexts = embed\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "          new_contexts, self_attention_weights = self.self_attentions[i](contexts,annotations,annotations)\n",
    "          residual_contexts = contexts + new_contexts\n",
    "          new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts,annotations,annotations)\n",
    "          residual_contexts = residual_contexts + new_contexts\n",
    "          new_contexts = self.attention_mlps[i](residual_contexts)\n",
    "          contexts = new_contexts + residual_contexts\n",
    "\n",
    "          encoder_attention_weights_list.append(encoder_attention_weights)\n",
    "          self_attention_weights_list.append(self_attention_weights)\n",
    "          \n",
    "        output = self.out(contexts)\n",
    "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
    "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
    "        \n",
    "        return output, (encoder_attention_weights, self_attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2393
    },
    "colab_type": "code",
    "id": "dyIadq96VCs2",
    "outputId": "e0e08a9c-c48e-4ba8-8436-654e8ce5364f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 100                                    \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.005                                  \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 20                                     \n",
      "                           decoder_type: transformer                            \n",
      "                 num_transformer_layers: 3                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('person', 'ersonpay')\n",
      "('paces', 'acespay')\n",
      "('retreated', 'etreatedray')\n",
      "('shutting', 'uttingshay')\n",
      "('placed', 'acedplay')\n",
      "Num unique word pairs: 6387\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type TransformerDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type ScaledDotAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 | Train loss: 2.008 | Val loss: 1.961 | Gen: et iray ongngngngngngngngngn isisisisisisisisisis ingingingingingingin\n",
      "Epoch:   1 | Train loss: 1.590 | Val loss: 1.686 | Gen: etay iray iondghiondghiondghio isisisisisisisisisis ingingingingingingin\n",
      "Epoch:   2 | Train loss: 1.415 | Val loss: 1.760 | Gen: ay awawawawawawawawawaw otititititititititit isisisisisisisisisis ongay-ingay-ingay-in\n",
      "Epoch:   3 | Train loss: 1.351 | Val loss: 1.538 | Gen: ethay awawawawawawawawawaw ongngngngngngngngngn isisisisisisisisisis oway-oway-oway-oway-\n",
      "Epoch:   4 | Train loss: 1.212 | Val loss: 1.269 | Gen: ethethethethethethet ay ongrtitititititititi isisisisisisisisisis orkorkorkorkorkorkor\n",
      "Epoch:   5 | Train loss: 1.069 | Val loss: 1.311 | Gen: ethay airairairairairairai iongway isisisisisisisisisis orkorkorkorkorkorkor\n",
      "Epoch:   6 | Train loss: 1.025 | Val loss: 1.354 | Gen: eway airairairairairairai ondcay isisisisisisisisisis oray\n",
      "Epoch:   7 | Train loss: 1.010 | Val loss: 1.275 | Gen: ethay airairairairairairai ongmiongmiongmiongmi isway ofay\n",
      "Epoch:   8 | Train loss: 0.972 | Val loss: 1.223 | Gen: eway iray ongway isay orkeway\n",
      "Epoch:   9 | Train loss: 0.907 | Val loss: 1.328 | Gen: ethethethethethethet awawawawawawawawawaw ongititititititititi isisisisisisisisisis ay\n",
      "Epoch:  10 | Train loss: 0.951 | Val loss: 1.276 | Gen: ethethethethethethet irairairairairairair ongway isisisisisisisisisis orkingway\n",
      "Epoch:  11 | Train loss: 0.901 | Val loss: 1.159 | Gen: eway ay ongiongiongiongiongi isisisisisisisisisis orkengway\n",
      "Epoch:  12 | Train loss: 0.836 | Val loss: 1.131 | Gen: ehay ay ondiondiondiondiondi isay oingway\n",
      "Epoch:  13 | Train loss: 0.813 | Val loss: 1.057 | Gen: ehay airwairwairwairwairw onionionionionionion isway orkegway\n",
      "Epoch:  14 | Train loss: 0.829 | Val loss: 1.059 | Gen: ethay airwairwairwairwairw ongingingingingingin isway oingway\n",
      "Epoch:  15 | Train loss: 0.759 | Val loss: 1.015 | Gen: ewayhewayhewayhewayh ay ongcay isway orkway\n",
      "Epoch:  16 | Train loss: 0.746 | Val loss: 1.144 | Gen: ay ay ondititititititititi isway orkingway\n",
      "Epoch:  17 | Train loss: 0.804 | Val loss: 1.038 | Gen: ethay ay ongcay isway orkingwkingwkingwkin\n",
      "Epoch:  18 | Train loss: 0.756 | Val loss: 1.000 | Gen: ethethethethethethet airwairwairwairwairw ondindindindindindin isway orkngway\n",
      "Epoch:  19 | Train loss: 0.686 | Val loss: 0.856 | Gen: ethay airwairwairwairwairw onininininininininin isway orkngway\n",
      "Epoch:  20 | Train loss: 0.669 | Val loss: 0.847 | Gen: ethay airwairwairwairwairw onininininininininin isway orkinway\n",
      "Epoch:  21 | Train loss: 0.627 | Val loss: 0.799 | Gen: ethay ay ongcay isway orkingway\n",
      "Epoch:  22 | Train loss: 0.641 | Val loss: 0.974 | Gen: ethay ay oncay isway orkingway\n",
      "Epoch:  23 | Train loss: 0.889 | Val loss: 1.186 | Gen: ethay ay ongway isay orkengway\n",
      "Epoch:  24 | Train loss: 0.924 | Val loss: 1.368 | Gen: ethay iray indcay isssssssssssssssssss ingway\n",
      "Epoch:  25 | Train loss: 1.015 | Val loss: 1.130 | Gen: ethay ay indcay isay ingay\n",
      "Epoch:  26 | Train loss: 0.873 | Val loss: 1.214 | Gen: ethay airwairwairwairwairw ontntntntntntntntntn isway orkay\n",
      "Epoch:  27 | Train loss: 1.052 | Val loss: 1.218 | Gen: eway airairairairairairai ondindindindindindin isway orkngway\n",
      "Epoch:  28 | Train loss: 1.011 | Val loss: 1.226 | Gen: ethththththththththt ay oindindindindindindi isway oy\n",
      "Epoch:  29 | Train loss: 0.968 | Val loss: 1.055 | Gen: ehay airwairwairwairwairw ondindindindindindin isway orkengway\n",
      "Epoch:  30 | Train loss: 0.876 | Val loss: 0.962 | Gen: ethay airwairwairwairwairw ongcay isway orkngway\n",
      "Epoch:  31 | Train loss: 0.821 | Val loss: 1.038 | Gen: ethethethethethethet ay ongcay isway orkingway\n",
      "Epoch:  32 | Train loss: 0.811 | Val loss: 1.025 | Gen: ethay ay ongway isway oingway\n",
      "Epoch:  33 | Train loss: 0.769 | Val loss: 0.894 | Gen: ethay airwairwairwairwairw ondiondiondiondiondi isway orkengway\n",
      "Epoch:  34 | Train loss: 0.731 | Val loss: 0.916 | Gen: ethay airwairwairwairwairw ondiondiondiondiondi isway orkingway\n",
      "Epoch:  35 | Train loss: 0.699 | Val loss: 0.884 | Gen: ethethethethethethet airwairwairwairwairw ongcay isway ongway\n",
      "Epoch:  36 | Train loss: 0.679 | Val loss: 0.894 | Gen: ethay ay onionionionionionion isway oicicicicicicicicici\n",
      "Epoch:  37 | Train loss: 0.710 | Val loss: 0.914 | Gen: ethay ay ondititititititititi isway oingway\n",
      "Epoch:  38 | Train loss: 0.747 | Val loss: 1.029 | Gen: thay airwairwairwairwairw onay isway orkingwagwagwagwagwa\n",
      "Epoch:  39 | Train loss: 0.757 | Val loss: 0.943 | Gen: etay airairairairairairai ondititititititititi isway orgway\n",
      "Epoch:  40 | Train loss: 0.752 | Val loss: 1.120 | Gen: ay airairairairairairai ondcay isway orkay\n",
      "Epoch:  41 | Train loss: 0.799 | Val loss: 1.396 | Gen: ethay airwairwairwairwairw ongcay isway ongwrbingwrbingwrbin\n",
      "Epoch:  42 | Train loss: 1.024 | Val loss: 1.074 | Gen: ethay-ethay-ethay-et ay ondindindindindindin isway orkingway\n",
      "Epoch:  43 | Train loss: 0.855 | Val loss: 1.002 | Gen: ethay airwairwairwairwairw ondindindindindindin isway orbingway\n",
      "Epoch:  44 | Train loss: 0.813 | Val loss: 1.056 | Gen: ethay ailwailwailwailwailw ionionionionionionio isway ongway\n",
      "Epoch:  45 | Train loss: 0.834 | Val loss: 0.944 | Gen: ethay airwairwairwairwairw onininininininininin isway orkway\n",
      "Epoch:  46 | Train loss: 0.753 | Val loss: 0.955 | Gen: ethay airwairwairwairwairw onininininininininin isway orkway\n",
      "Epoch:  47 | Train loss: 0.754 | Val loss: 0.908 | Gen: ethay airwairwairwairwairw onininininininininin isway orkay\n",
      "Epoch:  48 | Train loss: 0.727 | Val loss: 0.954 | Gen: ethay airwairwairwairwairw onininininininininin isway orkway\n",
      "Epoch:  49 | Train loss: 0.781 | Val loss: 0.999 | Gen: ethay airwairwairwairwairw onininininininininin isway orkingway\n",
      "Epoch:  50 | Train loss: 0.765 | Val loss: 0.967 | Gen: ethay ay ondindindindindindin isway orkingway\n",
      "Epoch:  51 | Train loss: 0.754 | Val loss: 0.876 | Gen: ethay ay onionionionionionion isway orkingway\n",
      "Epoch:  52 | Train loss: 0.727 | Val loss: 0.884 | Gen: ethay airwairwairwairwairw onititititititititit isway orkingway\n",
      "Epoch:  53 | Train loss: 0.688 | Val loss: 0.875 | Gen: ethay airwairwairwairwairw onininininininininin isway orkingway\n",
      "Epoch:  54 | Train loss: 0.665 | Val loss: 0.859 | Gen: ethay airwairwairwairwairw ondindindindindindin isway orkingway\n",
      "Epoch:  55 | Train loss: 0.681 | Val loss: 0.837 | Gen: ethay ay ondititititititititi isway orkway\n",
      "Epoch:  56 | Train loss: 0.664 | Val loss: 0.831 | Gen: ethethethethethethet ay ondiondiondiondiondi isway orkingway\n",
      "Epoch:  57 | Train loss: 0.646 | Val loss: 0.784 | Gen: ethay airwairwairwairwairw onay isway orkingway\n",
      "Epoch:  58 | Train loss: 0.621 | Val loss: 0.798 | Gen: ethay airwairwairwairwairw ondiondiondiondiondi isway orkingway\n",
      "Epoch:  59 | Train loss: 0.631 | Val loss: 0.786 | Gen: ethay airwairwairwairwairw ondititititititititi isway orkingway\n",
      "Epoch:  60 | Train loss: 0.597 | Val loss: 0.747 | Gen: ethay ay ondititititititititi isway orkingway\n",
      "Epoch:  61 | Train loss: 0.608 | Val loss: 0.776 | Gen: ethay ay onititititititititit isway orkingway\n",
      "Epoch:  62 | Train loss: 0.591 | Val loss: 0.797 | Gen: ethay ay ondititititititititi isway orkengway\n",
      "Epoch:  63 | Train loss: 0.599 | Val loss: 0.755 | Gen: ethay ay ondititititititititi isway orkingway\n",
      "Epoch:  64 | Train loss: 0.612 | Val loss: 0.958 | Gen: ethay airwairwairwairwairw ondititititititititi isway orkorkorkorkorkorkor\n",
      "Epoch:  65 | Train loss: 0.735 | Val loss: 0.855 | Gen: ethay airwairwairwairwairw ondititititititititi isway orkorkorkorkorkorkor\n",
      "Epoch:  66 | Train loss: 0.703 | Val loss: 0.863 | Gen: ethay airwairwairwairwairw onionionionionionion isway oray\n",
      "Epoch:  67 | Train loss: 0.678 | Val loss: 0.806 | Gen: ethay ay ondindindindindindin isway orkay\n",
      "Epoch:  68 | Train loss: 0.668 | Val loss: 0.843 | Gen: ethethethethethethet airwairwairwairwairw onititititititititit isway oray\n",
      "Epoch:  69 | Train loss: 0.665 | Val loss: 0.829 | Gen: ethay ay onionionionionionion isway orkingeway\n",
      "Epoch:  70 | Train loss: 0.648 | Val loss: 0.805 | Gen: ethay airwairwairwairwairw ondiondiondiondiondi isway orkingway\n",
      "Epoch:  71 | Train loss: 0.643 | Val loss: 0.813 | Gen: ethay airwairwairwairwairw ondiondiondiondiondi isway orkingway\n",
      "Epoch:  72 | Train loss: 0.619 | Val loss: 0.821 | Gen: ethay ay ondiondiondiondiondi isway orkingway\n",
      "Epoch:  73 | Train loss: 0.604 | Val loss: 0.776 | Gen: ethay ay ongcay isway orkingngngngngngngng\n",
      "Epoch:  74 | Train loss: 0.586 | Val loss: 0.757 | Gen: ethay airwairwairwairwairw onionionionionionion isway orkingway\n",
      "Epoch:  75 | Train loss: 0.578 | Val loss: 0.759 | Gen: ethay ay onititititititititit isssssssssssssssssss orkorkorkorkorkorkor\n",
      "Epoch:  76 | Train loss: 0.564 | Val loss: 0.744 | Gen: ethay ay onionionionionionion isway orkingway\n",
      "Epoch:  77 | Train loss: 0.543 | Val loss: 0.725 | Gen: ethay airwairwairwairwairw onionionionionionion isway orkingway\n",
      "Epoch:  78 | Train loss: 0.542 | Val loss: 0.758 | Gen: ethay airwairwairwairwairw ondindindindindindin isway orkingway\n",
      "Epoch:  79 | Train loss: 0.537 | Val loss: 0.724 | Gen: ethay airwairwairwairwairw ondindindindindindin isway orkingway\n",
      "Epoch:  80 | Train loss: 0.523 | Val loss: 0.718 | Gen: ethay ay ondindindindindindin isway orkingway\n",
      "Epoch:  81 | Train loss: 0.517 | Val loss: 0.728 | Gen: ethay airwairwairwairwairw ondindindindindindin isway orkingway\n",
      "Epoch:  82 | Train loss: 0.512 | Val loss: 0.776 | Gen: ethay ay ondiondiondiondiondi isway orkingingingingingin\n",
      "Epoch:  83 | Train loss: 0.539 | Val loss: 0.749 | Gen: ethay ay ondiondiondiondiondi isway orkingway\n",
      "Epoch:  84 | Train loss: 0.535 | Val loss: 0.825 | Gen: ethay ay ondindindindindindin isway orkingway\n",
      "Epoch:  85 | Train loss: 0.595 | Val loss: 0.753 | Gen: ethay airwairwairwairwairw ondititititititititi isway orkingway\n",
      "Epoch:  86 | Train loss: 0.540 | Val loss: 0.703 | Gen: ethay airwairwairwairwairw ondititititititititi isway orkingway\n",
      "Epoch:  87 | Train loss: 0.533 | Val loss: 0.749 | Gen: ethay airwairwairwairwairw onititititititititit isway orkingway\n",
      "Epoch:  88 | Train loss: 0.529 | Val loss: 0.685 | Gen: ethay ay ondindindindindindin isway orkingway\n",
      "Epoch:  89 | Train loss: 0.522 | Val loss: 0.715 | Gen: ethay ay ondindindindindindin isway orkingway\n",
      "Epoch:  90 | Train loss: 0.547 | Val loss: 0.721 | Gen: ethay airwairwairwairwairw ondititititititititi isway orkingway\n",
      "Epoch:  91 | Train loss: 0.563 | Val loss: 0.709 | Gen: ethay airwairwairwairwairw onionionionionionion isway orkingway\n",
      "Epoch:  92 | Train loss: 0.552 | Val loss: 0.704 | Gen: ethay airwairwairwairwairw ondiondiondiondiondi isway orkingway\n",
      "Epoch:  93 | Train loss: 0.528 | Val loss: 0.678 | Gen: ethay airwairwairwairwairw onititititititititit isway orkingway\n",
      "Epoch:  94 | Train loss: 0.510 | Val loss: 0.693 | Gen: ethay ay onionionionionionion isway orkingway\n",
      "Epoch:  95 | Train loss: 0.519 | Val loss: 0.688 | Gen: ethay ay ongcay isway orkingway\n",
      "Epoch:  96 | Train loss: 0.532 | Val loss: 0.746 | Gen: ethay ay onionionionionionion isway orkingway\n",
      "Epoch:  97 | Train loss: 0.541 | Val loss: 0.693 | Gen: ethay ay onininininininininin isway orkingway\n",
      "Epoch:  98 | Train loss: 0.649 | Val loss: 0.895 | Gen: othay airwairwairwairwairw ongcay isway orkingway\n",
      "Epoch:  99 | Train loss: 0.657 | Val loss: 0.834 | Gen: ethay airwairwairwairwairw ongcay isway orkingway\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay airwairwairwairwairw ongcay isway orkingway\n",
      "Time cost: 337.60201597213745\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "              'cuda':True,\n",
    "              'nepochs':100,\n",
    "              'checkpoint_dir':\"checkpoints\", \n",
    "              'learning_rate':0.005,\n",
    "              'lr_decay':0.99,\n",
    "              'batch_size':64, \n",
    "              'hidden_size':20, \n",
    "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
    "              'num_transformer_layers': 3,\n",
    "}\n",
    "args.update(args_dict)\n",
    "print_opts(args)\n",
    "start = time.time()\n",
    "transformer_encoder, transformer_decoder = train(args)\n",
    "end = time.time()\n",
    "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "print(\"Time cost:\",end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbfZCByITOI6"
   },
   "source": [
    "# Attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itCGMv3FdXsn"
   },
   "outputs": [],
   "source": [
    "TEST_WORD_ATTN = 'visualize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xBv4QQuBiU-V"
   },
   "source": [
    "## Visualize RNN attention map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "colab_type": "code",
    "id": "aXvqoQYONMTA",
    "outputId": "6fade232-4bd5-4b62-c0d1-7a5553cd50f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
      "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAGBCAYAAACKBaN2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMCBSPAAADjQ/wNtZyMR5ouLTgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x396 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'isualizevay'"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xuOvxfA1NMz3"
   },
   "source": [
    "## Visualize transformer attention maps from all the transformer layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1238
    },
    "colab_type": "code",
    "id": "HSSB4wd8-M7g",
    "outputId": "75789294-a392-4493-b5c4-6750b54bacfd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
      "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATAAAAGACAYAAAAqMT6GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<Figure size 576x396 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATAAAAGACAYAAAAqMT6GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKev/Adr81xrvrrW8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x396 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATAAAAGACAYAAAAqMT6GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 576x396 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'isuay'"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
