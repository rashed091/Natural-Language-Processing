{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "adgAwRaDI5p3"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch==0.4.1\n",
    "!pip -qq install torchtext==0.3.1\n",
    "!pip -qq install gensim==3.6.0\n",
    "!pip -qq install pyldavis==2.1.2\n",
    "!pip -qq install attrs==18.2.0\n",
    "!wget -qq --no-check-certificate 'https://drive.google.com/uc?export=download&id=1OIU9ICMebvZXJ0Grc2SLlMep3x9EkZtz' -O perashki.txt\n",
    "!wget -qq --no-check-certificate 'https://drive.google.com/uc?export=download&id=1v66uAEKL3KunyylYitNKggdl2gCeYgZZ' -O poroshki.txt\n",
    "!git clone https://github.com/UniversalDependencies/UD_Russian-SynTagRus.git\n",
    "!wget -qq https://raw.githubusercontent.com/DanAnastasyev/neuromorphy/master/neuromorphy/train/corpus_iterator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "uhvfH55PUJ8K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txWqIO_74A4s"
   },
   "source": [
    "# Word-Level Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOD_3I7d4oDV"
   },
   "source": [
    "Today we are mainly engaged in the fact that we generate * cakes * and * powders *.\n",
    "\n",
    "* (Data without demand downloaded from the site http://poetory.ru) *\n",
    "\n",
    "The pies are here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2vMrlrRQpuJ"
   },
   "outputs": [],
   "source": [
    "!head perashki.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Lm0-PeG5Dh9"
   },
   "source": [
    "Порошки вот:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-Jf88bxVTGj"
   },
   "outputs": [],
   "source": [
    "!head poroshki.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgYh4FNP5FyX"
   },
   "source": [
    "Do not confuse!\n",
    "\n",
    "In general, a pie is a quatrain, written by iambic tetrameter under the scheme 9-8-9-8. In powder scheme 9-8-9-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSBpLFRgGaXS"
   },
   "outputs": [],
   "source": [
    "vowels = 'ёуеыаоэяию'\n",
    "\n",
    "odd_pattern = '-+-+-+-+-'\n",
    "even_pattern = '-+-+-+-+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hl9BFoug519c"
   },
   "source": [
    "Считываем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3aFzzOQKLlD"
   },
   "outputs": [],
   "source": [
    "def read_poem(path):\n",
    "    poem = []\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if len(line) == 0:\n",
    "                yield poem\n",
    "                poem = []\n",
    "                continue\n",
    "            \n",
    "            poem.extend(line.split() + ['\\\\n'])\n",
    "            \n",
    "perashki = list(read_poem('perashki.txt'))\n",
    "poroshki = list(read_poem('poroshki.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xiRq1vbf55qN"
   },
   "source": [
    "Построим датасет для порошков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOBgLAgVTrk1"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, Example, Dataset, BucketIterator\n",
    "\n",
    "text_field = Field(init_token='<s>', eos_token='</s>')\n",
    "        \n",
    "fields = [('text', text_field)]\n",
    "examples = [Example.fromlist([poem], fields) for poem in poroshki]\n",
    "dataset = Dataset(examples, fields)\n",
    "\n",
    "text_field.build_vocab(dataset, min_freq=7)\n",
    "\n",
    "print('Vocab size =', len(text_field.vocab))\n",
    "train_dataset, test_dataset = dataset.split(split_ratio=0.9)\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n",
    "                                              shuffle=True, device=DEVICE, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FYJe2CA8GcY"
   },
   "source": [
    "**Задание** Напишите класс языковой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8ndCRZLl4ZZ"
   },
   "outputs": [],
   "source": [
    "class LMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, lstm_hidden_dim=256, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim)\n",
    "        \n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self, init_range=0.1):\n",
    "        self._emb.weight.data.uniform_(-init_range, init_range)\n",
    "        self._out_layer.bias.data.zero_()\n",
    "        self._out_layer.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        <apply layers>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySJ4tUAqvFvB"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_qVuSL8QJg4"
   },
   "outputs": [],
   "source": [
    "model = LMModel(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
    "\n",
    "model(batch.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rsh3_eR08PqQ"
   },
   "source": [
    "**Задание** Добавьте подсчет потерей с маскингом паддингов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_E2JxfRuphch"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.get_lock().locks = []\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = len(data_iter)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):                \n",
    "                logits, _ = model(batch.text)\n",
    "\n",
    "                <calc loss>\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
    "                                                                                         math.exp(loss.item())))\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
    "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
    "            )\n",
    "            progress_bar.refresh()\n",
    "\n",
    "    return epoch_loss / batches_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n",
    "    best_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n",
    "            \n",
    "            if best_val_loss and val_loss > best_val_loss:\n",
    "                optimizer.param_groups[0]['lr'] /= 4.\n",
    "                print('Optimizer lr = {:g}'.format(optimizer.param_groups[0]['lr']))\n",
    "            else:\n",
    "                best_val_loss = val_loss\n",
    "        print()\n",
    "        generate(model)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufpoSwQ-8bcN"
   },
   "source": [
    "**Задание** Напишите функцию-генератор для модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BYoHY1se2bhB"
   },
   "outputs": [],
   "source": [
    "def sample(probs, temp):\n",
    "    probs = F.log_softmax(probs.squeeze(), dim=0)\n",
    "    probs = (probs / temp).exp()\n",
    "    probs /= probs.sum()\n",
    "    probs = probs.cpu().numpy()\n",
    "\n",
    "    return np.random.choice(np.arange(len(probs)), p=probs)\n",
    "\n",
    "\n",
    "def generate(model, temp=0.6):\n",
    "    model.eval()\n",
    "    with torch.no_grad():        \n",
    "        prev_token = train_iter.dataset.fields['text'].vocab.stoi['<s>']\n",
    "        end_token = train_iter.dataset.fields['text'].vocab.stoi['</s>']\n",
    "        \n",
    "        hidden = None\n",
    "        for _ in range(150):\n",
    "            <generate text>\n",
    "                \n",
    "generate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5X2kYDU_rCjP"
   },
   "outputs": [],
   "source": [
    "model = LMModel(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
    "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
    "criterion = nn.CrossEntropyLoss(...).to(DEVICE)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=300, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_YtM4ms8v--"
   },
   "source": [
    "**Задание** Добавьте маскинг `<unk>` токенов при тренировке модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LzGwmgVf9Dkg"
   },
   "source": [
    "## Improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BHneb8br9WXh"
   },
   "source": [
    "### Tying input and output embeddings\n",
    "\n",
    "There are two embeddings in the model - input and output. A beautiful and useful idea in life is to learn only one matrix shared between them: [Using the Output Mode for Improving Language Models] (http://www.aclweb.org/anthology/E17-2025)\n",
    "\n",
    "From the idea there are some pluses: it turns out that there are much less trained parameters and at the same time a noticeably higher quality.\n",
    "\n",
    "** Assignment ** Implement it. It is enough to write something like this in the constructor:\n",
    "\n",
    "`self._out_layer.weight = self._emb.weight`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8I3QC4a_a8q"
   },
   "source": [
    "### Add information to the sample\n",
    "\n",
    "Now we have every word represented by one index. Models are very difficult to know how many syllables there are - which means it is difficult to generate a correct poem.\n",
    "\n",
    "In fact, each word can be attributed to a piece of the metric pattern:\n",
    "\n",
    "<img src=\"https://hsto.org/web/59a/b39/bd0/59ab39bd020c49a78a12cbab62c80181.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "**Task** Update the function `read_poem`, let it generate two lists - a list of words and a list of pieces of the template. Add an input to the model - template sequences, concatenate their embeddings with words.\n",
    "An additional idea is to make the model guess which pattern should go next (about half will be suitable, the rest will not). Add additional loss from guessing the pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MBX4NjzZ-0Hc"
   },
   "source": [
    "### We increase the selection\n",
    "\n",
    "We have a sample for pies, which is much larger.\n",
    "\n",
    "** Task ** Learn from it.\n",
    "\n",
    "### Transfer learning\n",
    "\n",
    "A simple and pleasant way to improve the model is to make the transfer trained on a large case of the model for a smaller amount of datasets.\n",
    "\n",
    "This method is more popular in computer vision: [Transfer learning, cs231n] (http://cs231n.github.io/transfer-learning/) - there is a huge ImageNet on which the model is trained to freeze the lower layers and replace the weekend. As a result, the model uses universal data representations, learned on a large package, but to predict very different labels - and the quality grows very well.\n",
    "\n",
    "We still do not need such perversions (although the keywords will come in handy later: ULMFiT, ELMo and company). Just take a model trained in a larger case and teach it on a smaller case. She just needs to learn a new matrix pattern of the last row.\n",
    "\n",
    "**Assignment** Model trained in the last paragraph to train for powders.\n",
    "\n",
    "### Conditional language model\n",
    "\n",
    "Even better, just learn from both buildings at once. Combine the pies and powders, for each store the index 0/1 - whether it was a pie or powder. Add an entry — this index and concatenate it either to each embedding of the words or to each output from the LSTM.\n",
    "\n",
    "**Assignment** Teach a single model from which you can ask to generate a pie or powder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnP743CM-bY6"
   },
   "source": [
    "### Variational & word dropout\n",
    "\n",
    "** Assignment ** In the last lesson, examples of dropout adapters more suitable for RNNs were given. Add them.\n",
    "\n",
    "** Task ** In addition, try increasing the size of the model or the number of layers in it to improve the quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ejqx6BC0JcG2"
   },
   "source": [
    "## Multi-task learning\n",
    "\n",
    "Another important way to improve the model is multi-task learning. This is when one model learns to make predictions for several tasks at once.\n",
    "\n",
    "In our case, this can be a prediction of the lemma of the word separately and its grammatical meaning separately:\n",
    "\n",
    "<img src = \"https://hsto.org/web/e97/8a8/6e8/e978a86e8a874d8d946bb15e6a49a713.png\" width = \"50%\">\n",
    "\n",
    "As a result, the model learns both the language model according to the lemmas and the POS tagging model. At the same time!\n",
    "\n",
    "Take the case from universal dependencies - it is already marked up as needed.\n",
    "\n",
    "We read it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YT-kzC2_KuLX"
   },
   "outputs": [],
   "source": [
    "from corpus_iterator import Token, CorpusIterator\n",
    "\n",
    "fields = [('word', Field()), ('lemma', Field()), ('gram_val', Field())]\n",
    "examples = []\n",
    "\n",
    "with CorpusIterator('UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu') as corpus_iter:\n",
    "    for sent in corpus_iter:\n",
    "        words = ['<s>'] + [tok.token.lower() for tok in sent] + ['</s>']\n",
    "        lemmas = ['<s>'] + [tok.lemma.lower() for tok in sent] + ['</s>']\n",
    "        gr_vals = ['<s>'] + [tok.grammar_value for tok in sent] + ['</s>']\n",
    "        examples.append(Example.fromlist([words, lemmas, gr_vals], fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_3xaD-2KwNW"
   },
   "outputs": [],
   "source": [
    "print('Words:', examples[1].word)\n",
    "print('Lemmas:', examples[1].lemma)\n",
    "print('Grammar vals:', examples[1].gram_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcGm5fPsLESH"
   },
   "source": [
    "Таким образом, размер словаря может быть существенно сокращен - лемм меньше, чем слов, а предсказание грамматики вынуждает модель быть более осведомленной о согласовании слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZe5HimdLb9i"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(examples, fields)\n",
    "\n",
    "dataset.fields['word'].build_vocab(dataset, min_freq=3)\n",
    "print('Word vocab size =', len(dataset.fields['word'].vocab))\n",
    "dataset.fields['lemma'].build_vocab(dataset, min_freq=3)\n",
    "print('Lemma vocab size =', len(dataset.fields['lemma'].vocab))\n",
    "dataset.fields['gram_val'].build_vocab(dataset)\n",
    "print('Grammar val vocab size =', len(dataset.fields['gram_val'].vocab))\n",
    "\n",
    "train_dataset, test_dataset = dataset.split(split_ratio=0.75)\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n",
    "                                              shuffle=True, device=DEVICE, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y7xlr15lLm78"
   },
   "source": [
    "Построим маппинг из пары (лемма, грамматическое значение) в слово - если бы у нас под рукой был морфологический словарь, маппинг можно было бы пополнить, добавить слова для лемм из корпуса, которые не встретились в обучении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_AvT2MgeLmP8"
   },
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    (lemma, gr_val): word\n",
    "    for example in train_iter.dataset.examples \n",
    "    for word, lemma, gr_val in zip(example.word, example.lemma, example.gram_val)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VaP8Krx1LeJl"
   },
   "source": [
    "**Задание**  Обновите генератор - например, можно сэмплировать лемму и находить самое вероятное грамматическое значение, которое встречается  в паре с этой леммой в `dictionary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PeBH0WYjMQ5h"
   },
   "outputs": [],
   "source": [
    "def generate(model, temp=0.7):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3GzOZ8dMVMJ"
   },
   "source": [
    "** Task ** Update the model and learning function.\n",
    "\n",
    "The model should take the pairs `lemma, gr_val`, concatenate their embeddings and predict the following` lemma, gr_val` on leaving LSTM.\n",
    "\n",
    "Function `do_epoch` should summarize loss prediction Lemma (for making Muskingum` <unk> `and` <pad> `) + losses on the prediction of grammatical meaning (according to the Muskingum` <pad> `)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vL2xPe-BNRhu"
   },
   "source": [
    "## Controlled generation\n",
    "\n",
    "I want to make the generation more controlled - ideally, to set the topic.\n",
    "\n",
    "A simple way is to do thematic modeling and find some topics in the texts - and then transfer the vector of themes along with embedding to the model so that the model learns to generate thematically agreed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "exopH1jlN4fc"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "docs = [[word for word in poem if word != '\\\\n'] for poem in perashki]\n",
    "\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "dictionary.filter_n_most_frequent(100)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "lda_model = models.LdaModel(bow_corpus, num_topics=5, id2word=dictionary, passes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLPO9U1-Pakp"
   },
   "source": [
    "Посмотреть, что выучилось, можно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hzEg-8SZs8t7"
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHx1GJrWPkM8"
   },
   "source": [
    "Предсказывает распределение модель как-то так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rTD0CGMdPsF5"
   },
   "outputs": [],
   "source": [
    "for word in perashki[10]:\n",
    "    if word == '\\\\n':\n",
    "        print()\n",
    "    else:\n",
    "        print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0m0b6i2MPlKD"
   },
   "outputs": [],
   "source": [
    "lda_model.get_document_topics(bow_corpus[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-imKaGGUQM5K"
   },
   "source": [
    "** Task ** Count for all the texts of the vector of themes, pass them along with the words (concatenating to embeddings). See what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w8V0KAz_CNf0"
   },
   "source": [
    "# Referrence\n",
    "\n",
    "Regularizing and Optimizing LSTM Language Models, 2017 [[arxiv]](https://arxiv.org/abs/1708.02182), [[github]](https://github.com/salesforce/awd-lstm-lm) - одна из самых полезных статей про языковые модели + репозиторий, в котором реализовано много полезного, стоит заглянуть\n",
    "\n",
    "Exploring the Limits of Language Modeling, 2016 [[arxiv]](https://arxiv.org/abs/1602.02410)\n",
    "\n",
    "Using the Output Embedding to Improve Language Models, 2017 [[pdf]](http://www.aclweb.org/anthology/E17-2025)\n",
    "\n",
    "[Transfer learning, cs231n](http://cs231n.github.io/transfer-learning/)  \n",
    "[Transfer learning, Ruder](http://ruder.io/transfer-learning/) - очень подробная статья от чувака из NLP\n",
    "[An Overview of Multi-Task Learning in Deep Neural Networks, Ruder](http://ruder.io/multi-task/)  \n",
    "[Multi-Task Learning Objectives for Natural Language Processing, Ruder](http://ruder.io/multi-task-learning-nlp/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 08 - Language Models (Part 2).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
