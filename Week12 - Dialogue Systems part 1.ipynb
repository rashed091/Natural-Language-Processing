{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4WlMyJVRkzQ"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch==0.4.1\n",
    "!pip -qq install torchtext==0.3.1\n",
    "!git clone https://github.com/MiuLab/SlotGated-SLU.git\n",
    "!wget -qq https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/week08_multitask/conlleval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvJKy3mtVOpw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4QR5dTAfVhLD"
   },
   "source": [
    "# Interactive systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fox5ub_GKSLL"
   },
   "source": [
    "\n",
    "Interactive systems are divided into two types - * goal-orientied * and * general conversation *.\n",
    "\n",
    "** General conversation ** is a chat talk on a free topic:\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/bFwwGpc/alice.jpg\" width=\"10%\">\n",
    "</center>\n",
    "    \n",
    "Today we will speak not about them, but about ** goal-orientied ** systems:\n",
    "<center>\n",
    "<img src=\"https://hsto.org/webt/gj/3y/xl/gj3yxlqbr7ujuqr9r2akacxmkee.jpeg\" width=\"20%\">  \n",
    "</center>\n",
    "*From [Как устроена Алиса](https://habr.com/company/yandex/blog/349372/)*\n",
    "\n",
    "The user says something, it recognizes something. By recognized it is determined - what, where and when he wanted. Then the dialog engine decides whether the user really knows what he wanted to ask. There is a trip to the sources - to find out the information that (it seems) requested by the user. Based on all this, some response is generated:\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/8XcdpJ7/goal-orientied.png\" width=\"20%\">\n",
    "</center>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/task_oriented_dialog_systems.gif\" width=\"20%\">\n",
    "</center>\n",
    "We will study the part that is in the middle - the classifier and the tagger. All the rest is usually heuristic and zahardkozhennye answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIJt4hPLPYtO"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUZ8xjG_PT7C"
   },
   "source": [
    "There is a conventionally standard dataset - atis, which is indecently small, in fact.\n",
    "\n",
    "To him you can take more dataset snips - it is bigger and more diverse.\n",
    "\n",
    "We will take both datasets from the repository of the article [Slot-Gated Modeling for Joint Slot Filling and Intent Prediction] (http://aclweb.org/anthology/N18-2118).\n",
    "\n",
    "Let's start with atis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yw_FnOVOVgdX"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def read_dataset(path):\n",
    "    with open(os.path.join(path, 'seq.in')) as f_words, \\\n",
    "            open(os.path.join(path, 'seq.out')) as f_tags, \\\n",
    "            open(os.path.join(path, 'label')) as f_intents:\n",
    "        \n",
    "        return [\n",
    "            (words.strip().split(), tags.strip().split(), intent.strip()) \n",
    "            for words, tags, intent in zip(f_words, f_tags, f_intents)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JrAgjAFVWnh9"
   },
   "outputs": [],
   "source": [
    "train_data = read_dataset('SlotGated-SLU/data/atis/train/')\n",
    "val_data = read_dataset('SlotGated-SLU/data/atis/valid/')\n",
    "test_data = read_dataset('SlotGated-SLU/data/atis/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3zvT5BsWv0p"
   },
   "outputs": [],
   "source": [
    "intent_to_example = {example[2]: example for example in train_data}\n",
    "for example in intent_to_example.values():\n",
    "    print('Intent:\\t', example[2])\n",
    "    print('Text:\\t', '\\t'.join(example[0]))\n",
    "    print('Tags:\\t', '\\t'.join(example[1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4EoT_us7Y23P"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, LabelField, Example, Dataset, BucketIterator\n",
    "\n",
    "tokens_field = Field()\n",
    "tags_field = Field(unk_token=None)\n",
    "intent_field = LabelField()\n",
    "\n",
    "fields = [('tokens', tokens_field), ('tags', tags_field), ('intent', intent_field)]\n",
    "\n",
    "train_dataset = Dataset([Example.fromlist(example, fields) for example in train_data], fields)\n",
    "val_dataset = Dataset([Example.fromlist(example, fields) for example in val_data], fields)\n",
    "test_dataset = Dataset([Example.fromlist(example, fields) for example in test_data], fields)\n",
    "\n",
    "tokens_field.build_vocab(train_dataset)\n",
    "tags_field.build_vocab(train_dataset)\n",
    "intent_field.build_vocab(train_dataset)\n",
    "\n",
    "print('Vocab size =', len(tokens_field.vocab))\n",
    "print('Tags count =', len(tags_field.vocab))\n",
    "print('Intents count =', len(intent_field.vocab))\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "    datasets=(train_dataset, val_dataset, test_dataset), batch_sizes=(32, 128, 128), \n",
    "    shuffle=True, device=DEVICE, sort=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rx8tY7_xQIpi"
   },
   "source": [
    "## Classifier intents\n",
    "\n",
    "Let's start with a classifier: to which intent this request belongs.\n",
    "\n",
    "** Assignment ** Nothing clever - take rnn'ku and learn how to predict mark-intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u4pZR9IRckK-"
   },
   "outputs": [],
   "source": [
    "class IntentClassifierModel(nn.Module):\n",
    "    def __init__(self, vocab_size, intents_count, emb_dim=64, lstm_hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        <init layers>\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        <apply layers>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ppZMvviI0iXf"
   },
   "source": [
    "**Задание** `ModelTrainer` для подсчета лосса и accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MfKFMfeg_RLV"
   },
   "outputs": [],
   "source": [
    "class ModelTrainer():\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def on_epoch_begin(self, is_train, name, batches_count):\n",
    "        \"\"\"\n",
    "        Initializes metrics\n",
    "        \"\"\"\n",
    "        self.epoch_loss = 0\n",
    "        self.correct_count, self.total_count = 0, 0\n",
    "        self.is_train = is_train\n",
    "        self.name = name\n",
    "        self.batches_count = batches_count\n",
    "        \n",
    "        self.model.train(is_train)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Outputs final metrics\n",
    "        \"\"\"\n",
    "        return '{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "            self.name, self.epoch_loss / self.batches_count, self.correct_count / self.total_count\n",
    "        )\n",
    "        \n",
    "    def on_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Performs forward and (if is_train) backward pass with optimization, updates metrics\n",
    "        \"\"\"\n",
    "        <As usual: perform the forward pass, then call backward and apply optimizer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IqCvQEByddtj"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.get_lock().locks = []\n",
    "\n",
    "\n",
    "def do_epoch(trainer, data_iter, is_train, name=None):\n",
    "    trainer.on_epoch_begin(is_train, name, batches_count=len(data_iter))\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=trainer.batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                batch_progress = trainer.on_batch(batch)\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description(batch_progress)\n",
    "                \n",
    "            epoch_progress = trainer.on_epoch_end()\n",
    "            progress_bar.set_description(epoch_progress)\n",
    "            progress_bar.refresh()\n",
    "\n",
    "            \n",
    "def fit(trainer, train_iter, epochs_count=1, val_iter=None):\n",
    "    best_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        do_epoch(trainer, train_iter, is_train=True, name=name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            do_epoch(trainer, val_iter, is_train=False, name=name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQBsP8SHhjqm"
   },
   "outputs": [],
   "source": [
    "model = IntentClassifierModel(vocab_size=len(tokens_field.vocab), intents_count=len(intent_field.vocab)).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "trainer = ModelTrainer(model, criterion, optimizer)\n",
    "\n",
    "fit(trainer, train_iter, epochs_count=30, val_iter=val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9YhthG30xp2"
   },
   "source": [
    "**Задание** Подсчитайте итоговое качество на тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JxHshnyZjMuX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5zsAJJCEQ8Ti"
   },
   "source": [
    "## Tegger\n",
    "\n",
    "<center>\n",
    "<img src=\"https://commons.bmstu.wiki/images/0/00/NER1.png\" width=\"20%\">\n",
    "</center>\n",
    "    \n",
    "*From [NER](https://ru.bmstu.wiki/NER_(Named-Entity_Recognition)*\n",
    "\n",
    "** Assignment ** Still nothing clever - simple tagger like POS, only NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwphVxmdkChy"
   },
   "outputs": [],
   "source": [
    "class TokenTaggerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, tags_count, emb_dim=64, lstm_hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        <init layers again>\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        <apply 'em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6mzyxM0502wy"
   },
   "source": [
    "** Task ** Update `ModelTrainer`: you need to consider all the same loss and accuracy, only now it is a little different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cMRwby_NnyvJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3QXaapt3nuF_"
   },
   "outputs": [],
   "source": [
    "<fit the model>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UrkNHhuTRMQv"
   },
   "source": [
    "NERs are usually rated for F1 guessing slots. For this, everyone is dragging the conlleval script from each other :)\n",
    "\n",
    "** Task ** Write a function to evaluate tegger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKeXWjs7pE35"
   },
   "outputs": [],
   "source": [
    "from conlleval import evaluate\n",
    "\n",
    "def eval_tagger(model, test_iter):\n",
    "    true_seqs, pred_seqs = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            <calc true_seqs and pred_seqs for the batch>\n",
    "    print('Precision = {:.2f}%, Recall = {:.2f}%, F1 = {:.2f}%'.format(*evaluate(true_seqs, pred_seqs, verbose=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APcntGZ0ReXl"
   },
   "source": [
    "## Multi-task learning\n",
    "\n",
    "We have already discussed that multi-task learning is cool, fashionable and youthful. Let's ~~ let's like it ~~ we implement a model that can immediately predict tags and intents. The idea is that there is general information in all of this, which should help both one and the other: knowing the intent, you can understand which slots can be, and knowing the slots, you can guess the intent.\n",
    "\n",
    "** Task ** Implement the combined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goLcDk-Tu0uM"
   },
   "outputs": [],
   "source": [
    "class SharedModel(nn.Module):\n",
    "    def __init__(self, vocab_size, intents_count, tags_count, emb_dim=64, lstm_hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        <init layers>\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        <apply layers>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5semraSfv56f"
   },
   "outputs": [],
   "source": [
    "<update ModelTrainer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BP4b-4zxU0v"
   },
   "outputs": [],
   "source": [
    "<fit the model>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DlTbVSOezMD7"
   },
   "outputs": [],
   "source": [
    "<calc intent accuracy>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-6XO9lsyhJc"
   },
   "outputs": [],
   "source": [
    "<calc tags F1-score>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rAqVyqZ_SXxc"
   },
   "source": [
    " ## Asynchronous learning\n",
    " \n",
    " In general, everything was started precisely because of this - asynchronous learning multi-task model.\n",
    " \n",
    "The idea is described in [A Bi-model-based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling] (http://aclweb.org/anthology/N18-2050)\n",
    "\n",
    "Let's start with this model:\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/N2T1X2f/2018-11-27-2-11-01.png\" width=\"20%\">\n",
    "</center>\n",
    "\n",
    "The main difference from what has already been implemented is in what order everything is optimized. Instead of combined learning of all layers, the networks for the tagger and for the classifier are trained separately.\n",
    "\n",
    "At each learning step, sequences of hidden states $ h ^ 1 $ and $ h ^ 2 $ are generated - for the classifier and for the tagger.\n",
    "\n",
    "Next, losses from the prediction of the intensity are considered first and the optimizer step is taken, and then the losses from the prediction of the tags - and again the optimizer step.\n",
    "\n",
    "** Assignment ** Implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLlVOYfG-dRY"
   },
   "outputs": [],
   "source": [
    "class AsyncSharedModel(nn.Module):\n",
    "    def __init__(self, vocab_size, intents_count, tags_count, emb_dim=64, lstm_hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        <init layers>\n",
    "        \n",
    "    <do smth>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vEYVn6IXBRiR"
   },
   "outputs": [],
   "source": [
    "<update ModelTrainer somehow>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_z6u-dE3s8J"
   },
   "source": [
    "You need to create separate optimizers for each part of the model.\n",
    "\n",
    "Separate parameters can be obtained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9ZkkA8q372v"
   },
   "outputs": [],
   "source": [
    "model = AsyncSharedModel(\n",
    "    vocab_size=len(tokens_field.vocab),\n",
    "    intents_count=len(intent_field.vocab),\n",
    "    tags_count=len(tags_field.vocab)\n",
    ").to(DEVICE)\n",
    "\n",
    "tags_parameters = [param for name, param in model.named_parameters() if not name.startswith('_intent')]\n",
    "intent_parameters = [param for name, param in model.named_parameters() if not name.startswith('_tags')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aMNscGmZ4APl"
   },
   "source": [
    "Then they need to be transferred to separate optimizers and taught separately.\n",
    "\n",
    "* Also, perhaps the reward_graph parameter of the backward () method is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmMt811LBUXb"
   },
   "outputs": [],
   "source": [
    "<fit the model>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KnMw5L4iG5Io"
   },
   "outputs": [],
   "source": [
    "<calc intent accuracy and tags F1-score>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NE-5oyMUU40L"
   },
   "source": [
    "## Improvements\n",
    "\n",
    "** Task ** Look at the parameters in the article and try to achieve a similar quality.\n",
    "\n",
    "** Task ** Try replacing the case you are working with.\n",
    "\n",
    "### Encoder-decoder\n",
    "\n",
    "A good idea is to use not just independent tag predictions, but a decoder above them:\n",
    "\n",
    "<center>\n",
    "<img src = \"https://i.ibb.co/qrgVSqF/2018-11-27-2-11-17.png\" width = \"20%\">\n",
    "</center>\n",
    "\n",
    "In fact, there is just another RNN layer added here, this time unidirectional. In this case, in the case of tag prediction, its input is the previous tag, the previous hidden state, and the hidden states from the tag and integer encoders. For intent - simple RNN.\n",
    "\n",
    "** Task ** Implement such a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NujuoWDU195f"
   },
   "source": [
    "# Async Multi-task Learning for POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9Q0ip3p2a5v"
   },
   "source": [
    "These were toy datasets and not very good articles (albeit from NAACL-2018).\n",
    "\n",
    "I prefer this one: [Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings] (https://arxiv.org/pdf/1805.08237.pdf). Much more.\n",
    "\n",
    "The architecture there is this:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/0nSX6CC/2018-11-27-9-26-15.png\" width=\"20%\">\n",
    "</center>\n",
    "\n",
    "A multi-task task is to train individual classifiers of a lower level (above characters and words) to predict tags by individual optimizers.\n",
    "\n",
    "** Task ** Try to implement what is written in the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2x9-j4oz08p"
   },
   "source": [
    "# Referrence\n",
    "A Bi-model based RNN Semantic Frame Parsing Model for Intent Detection and Slot Filling, 2018 [[pdf]](http://aclweb.org/anthology/N18-2050)  \n",
    "Slot-Gated Modeling for Joint Slot Filling and Intent Prediction, 2018 [[pdf]](http://aclweb.org/anthology/N18-2118)  \n",
    "Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings, 2018 [[arxiv]](https://arxiv.org/pdf/1805.08237.pdf)\n",
    "\n",
    "[Как устроена Алиса](https://habr.com/company/yandex/blog/349372/)  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 12 - Dialogue Systems (Part 1).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
